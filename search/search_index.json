{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BMIS-2542: Data Programming Essentials with Python","text":"<ul> <li> <p> Course Information</p> <p> Course Description</p> <p> Schedule</p> <p> Course Material</p> </li> <li> <p> Instructor's Office &amp; Classroom</p> <p> Instructor Info Page via Canvas</p> <p> Virtual Office</p> <p> Weekly Meeting Links via Canvas</p> </li> <li> <p> Course Modules</p> <p> Getting Started</p> <p> Exploratory Data Analysis</p> <p> Introduction to Modeling </p> <p> Advanced Data Types</p> </li> <li> <p> Grading &amp; Evaluation</p> <p> Grading Information</p> <p> Project Guide</p> <p> Canvas Assignment Page</p> </li> <li> <p> Labs</p> <p> Starter Files</p> <p> Code Samples</p> </li> <li> <p> Resources and References</p> <p> Course Materials</p> <p> List of Data Sources</p> <p> Polars Docs</p> <p> Altair Docs</p> <p> scikit-learn Docs</p> </li> </ul>"},{"location":"adv-data-types/","title":"Advanced Data Types","text":"<ul> <li> <p> Time Series Data Analysis</p> <p> Foundations of Time Series Analysis</p> </li> <li> <p> Text Analysis</p> <p> Getting started</p> </li> </ul>"},{"location":"adv-data-types/text-analysis/","title":"Text Analysis","text":"<ul> <li> Starter Colab Notebook</li> <li> Getting Started </li> <li> Text Data Preparation </li> <li> Vectorization </li> <li> Sentiment Analysis </li> <li> Topic Modeling </li> <li> Named Entity Recognition</li> <li> Conclusion &amp; Next Steps</li> </ul>"},{"location":"adv-data-types/text-analysis/getting-started/","title":"Getting Started with Textual Data","text":"<p>In the previous lessons, you learned about extracting insights from structured data\u2014the clean, predictable world of rows and columns found in spreadsheets and databases. But what about the other 80% of the world's data? The messy, context-rich, and deeply human data found in emails, reviews, reports, and social media posts. This section serves as your entry point into this world, providing the foundational concepts needed to turn raw text into a strategic business asset.</p>"},{"location":"adv-data-types/text-analysis/getting-started/#11-why-text-is-a-unique-data-source","title":"1.1 Why Text is a Unique Data Source","text":"<p>Imagine you are a manager at a growing e-commerce company, \"GlobalCart.\" You have a dashboard showing sales figures, inventory levels, and return rates\u2014all clear, quantifiable metrics. This is structured data. Each piece of information has a defined place and format, making it easy for computer systems to process. You can easily query it, aggregate it, and create visualizations.</p> <p>However, this data tells you what is happening, but rarely why. Why did return rates for the \"Pro-Grade Blender\" spike last month? Why are sales of the \"Comfort-Fit Running Shoes\" declining despite positive initial forecasts? The answers are likely buried in the text customers have generated.</p> <p>Consider this small sample of recent customer feedback for GlobalCart:</p> Customer ID Feedback Text Rating CUST-001 The Pro-Grade Blender is a beast! It's powerful and quiet. Delivery was also incredibly fast, arrived in one day. 5 CUST-002 My package with the Comfort-Fit Running Shoes arrived damaged, and the box was completely crushed. Disappointed. 2 CUST-003 I had to return the blender. It was much larger than expected and didn't fit on my counter. The return was easy. 3 CUST-004 Are the Comfort-Fit shoes waterproof? Need to know before I buy. Your support chat is offline. 3 CUST-005 Fast shipping is great, but the Pro-Grade Blender's lid doesn't seem to seal properly. Seems like a defect. 2 <p>This is unstructured data. It lacks a predefined model or organization. It's free-form text, rich with nuances, sentiment, and specific details. Programmatically analyzing this type of data is the core challenge and opportunity of text analysis. </p> <p>To begin working with this data, we need a new vocabulary that maps to the structures we already know from working with DataFrames.</p> <p>The Hierarchy of Text: Corpus, Document, and Token</p> <p>In text analysis, we use a simple hierarchy to organize our data. This hierarchy has a direct and helpful analogy to the DataFrames you are used to.</p> <ul> <li>Corpus: The entire collection of texts we are analyzing. In our example, the five feedback entries together form our corpus. This is analogous to an entire <code>DataFrame</code>.</li> <li>Document: A single piece of text within the corpus. Each customer feedback entry is one document. This is analogous to a single <code>row</code> in a DataFrame, representing one observation.</li> <li>Token: An individual unit within a document, such as a word or a punctuation mark. For example, the words \"package,\" \"arrived,\" and \"damaged\" are all tokens from the second document.</li> </ul> <p>The most important difference from structured data is this: In a DataFrame, the columns (features) are already defined for you. In text analysis, we must create our features from the text itself. The tokens within each document are the raw materials we will use to build these features.</p>"},{"location":"adv-data-types/text-analysis/getting-started/#12-the-text-analysis-workflow-vs-traditional-ml","title":"1.2 The Text Analysis Workflow vs. Traditional ML","text":"<p>You are already familiar with the standard machine learning workflow from your work with libraries like <code>scikit-learn</code>. The process is logical and methodical. However, when working with text, the workflow requires a new, specialized set of initial steps.</p> <p>The primary difference is that machine learning models operate on numbers, not words. Therefore, the most critical task in text analysis is to devise a meaningful strategy to convert text into a numerical representation. This involves a much more intensive pre-processing and feature engineering phase than you might be used to with structured data.</p> <p>A Tale of Two Workflows</p> <p>Here is a high-level comparison of the traditional machine learning workflow you know and the specialized workflow required for textual data.</p> Traditional ML Workflow (Structured Data) Text Analysis Workflow (Unstructured Data) 1. Load Data: Read a CSV or database table. 1. Load Data: Read text files, emails, reviews, etc. into a corpus. 2. Pre-process Data: Handle missing values, scale numerical features. 2. Pre-process Text: A multi-step, intensive process including tokenization, lemmatization, stop-word removal, and more. 3. Feature Engineering: Select the most relevant columns from your table. 3. Vectorize Text: Convert the clean text into numerical features using techniques like Bag-of-Words or TF-IDF. (Covered in Section 3) 4. Model Training: Fit a model (e.g., <code>LogisticRegression</code>) to the data. 4. Model Training: Fit a model to the newly created numerical representation of the text. 5. Evaluation: Assess model performance. 5. Evaluation &amp; Interpretation: Assess performance and interpret results in the context of the original text. <p>The key takeaway is the addition of two major phases: intensive pre-processing to clean and standardize the text, and vectorization to convert that text into features for your model. The next two sections are dedicated entirely to these foundational steps.</p>"},{"location":"adv-data-types/text-analysis/getting-started/#13-a-map-of-text-analysis-techniques","title":"1.3 A Map of Text Analysis Techniques","text":"<p>Before we dive into the \"how,\" let's create a high-level map of the \"what.\" What kind of business problems can we solve with text analysis? These techniques are the ultimate destination, and the pre-processing and vectorization steps are the necessary journey to get there.</p>"},{"location":"adv-data-types/text-analysis/getting-started/#text-classification-and-sentiment-analysis","title":"Text Classification and Sentiment Analysis","text":"<p>The most direct application of your existing supervised learning knowledge is Text Classification. The goal is to assign a predefined category or label to a piece of text. A common and commercially valuable form of text classification is Sentiment Analysis, where the labels are related to opinion, such as \"positive,\" \"negative,\" or \"neutral.\"</p> <ul> <li>Business Question: \"Is this customer review positive or negative?\" or \"Should this support email be routed to the 'Urgent' or 'General' queue?\"</li> <li>ML Analogy: This is a classic supervised classification problem. You will use a labeled dataset to train a model to make predictions on new, unseen text. </li> </ul>"},{"location":"adv-data-types/text-analysis/getting-started/#topic-modeling","title":"Topic Modeling","text":"<p>Sometimes, you don't know the categories in advance. You might have thousands of customer reviews and want to discover the main themes or topics that people are discussing. This is where Topic Modeling comes in. It's an automated way to scan a set of documents and infer the underlying topics.</p> <ul> <li>Business Question: \"What are the most common themes in our customer feedback? Are people talking more about 'shipping delays,' 'product quality,' or 'price'?\"</li> <li>ML Analogy: This is an unsupervised learning problem, conceptually similar to the clustering you performed with K-Means. The algorithm groups words and documents together to create abstract topics. </li> </ul>"},{"location":"adv-data-types/text-analysis/getting-started/#named-entity-recognition-ner","title":"Named Entity Recognition (NER)","text":"<p>Your text data is often full of proper nouns\u2014people, products, companies, locations, and dates. Named Entity Recognition (NER) is the technique used to locate and classify these entities within text.</p> <ul> <li>Business Question: \"Which of our products are mentioned most often on social media?\" or \"Are customers in specific geographic locations complaining about shipping?\"</li> <li>ML Analogy: This is a more advanced technique, but it can be thought of as a form of granular classification at the word level.</li> </ul> <p>Linking Problems to Techniques</p> If you want to... The technique is... The ML analogy is... Assign text to known categories (e.g., spam/not spam) Text Classification Supervised Classification Gauge the opinion within text (e.g., positive/negative) Sentiment Analysis A type of Classification Discover hidden themes in documents Topic Modeling Unsupervised Clustering Extract proper nouns like products or places Named Entity Recognition (More advanced classification) <p>With this foundational map in place, you are ready to begin the journey. The next section will equip you with the essential tools for cleaning and preparing text for analysis.</p>"},{"location":"adv-data-types/text-analysis/named-entity-recognition/","title":"Information Extraction with Named Entity Recognition (NER)","text":"<p>Our text analysis workflows have so far focused on understanding documents at a high level. With sentiment analysis, we assigned a single label (like \"positive\") to a whole document. With topic modeling, we discovered the abstract themes a document belongs to.</p> <p>Now, we will shift our focus to a different type of task: information extraction. Instead of asking \"What is this document about?\", we will ask, \"What specific, structured information can I pull out of this unstructured text?\"</p> <p>The primary technique for this is Named Entity Recognition (NER). NER is the process of locating and categorizing predefined entities\u2014real-world objects\u2014within a text. These entities can include product names, companies, locations, dates, people's names, and more.</p> <p>For a business, this is an incredibly powerful capability. It allows you to automatically parse reviews, reports, and emails to answer questions like:</p> <ul> <li>Which of our products are mentioned most often alongside the word \"damaged\"?</li> <li>Are customers in specific cities or countries complaining about shipping times?</li> <li>Are our competitors being mentioned in our customer support tickets?</li> </ul>"},{"location":"adv-data-types/text-analysis/named-entity-recognition/#our-tool-for-the-job-spacy","title":"Our Tool for the Job: <code>spaCy</code>","text":"<p>While <code>NLTK</code> has NER capabilities, this is a perfect opportunity to introduce another industry-standard Python library: <code>spaCy</code>. <code>spaCy</code> is renowned for its speed, ease of use, and production-readiness. It provides access to highly optimized, pre-trained models that make tasks like NER incredibly straightforward.</p> <p>Setting up <code>spaCy</code></p> <p>To get started, you will need to install the library and download its small,  pre-trained English language model. In Google Colab environment, this is  already done for you. Depending on the language of the text data, you may  need to install a specific language model.</p> <pre><code># Install or update python setup tools\npip install -U pip setuptools wheel\n# Install or update Specy\npip install -U spacy\n# Next, download the English model package\npython -m spacy download en_core_web_sm\n</code></pre>"},{"location":"adv-data-types/text-analysis/named-entity-recognition/#performing-ner-in-practice","title":"Performing NER in Practice","text":"<p>Using <code>spaCy</code> to perform NER is a simple, three-step process: load a model, process your text, and then iterate through the identified entities.</p> <p>Let's use a new example from our \"GlobalCart\" dataset that contains a few different entities.</p> <pre><code>import spacy\nfrom spacy import displacy\n\n# Why: We load the pre-trained English model we just downloaded. \n# This 'nlp' object now contains all the components of the processing pipeline, including the NER model.\nnlp = spacy.load(\"en_core_web_sm\")\n\n# A new, more complex example document\ntext = \"On July 23, a customer from Berlin, Sarah Miller, reported that her Pro-Grade Blender from GlobalCart arrived with a defect.\"\n\n# Why: We process the text with the 'nlp' object. This creates a 'doc' object that\n# has been tokenized, tagged, and analyzed. The named entities are stored in the `doc.ents` attribute.\ndoc = nlp(text)\n\n# Why: We can now iterate through the identified entities and print their text\n# and their corresponding label.\nprint(\"Identified Entities:\")\nfor ent in doc.ents:\n    print(f\"- Text: {ent.text}, Label: {ent.label_}\")\n# The following line assumes a notebook environment.\n# In others, use displacy.serve()\n# displacy output not shown.\ndisplacy.render(doc, style=\"ent\")\n</code></pre> <p>Output:</p> <pre><code>Identified Entities:\n- Text: July 23, Label: DATE\n- Text: Berlin, Label: GPE\n- Text: Sarah Miller, Label: PERSON\n- Text: Pro-Grade Blender, Label: PRODUCT\n- Text: GlobalCart, Label: ORG\n</code></pre> <p>In just a few lines of code, <code>spaCy</code> has transformed an unstructured sentence into structured, categorized data.</p> <p>Rendered Table of Entities:</p> Entity Text Label Description July 23 <code>DATE</code> An absolute or relative date. Berlin <code>GPE</code> Geopolitical Entity (countries, cities, states). Sarah Miller <code>PERSON</code> A person's name. Pro-Grade Blender <code>PRODUCT</code> An object, vehicle, or other product. GlobalCart <code>ORG</code> An organization, company, or agency. <p>What's Happening Under the Hood?</p> <p>You might wonder how <code>spaCy</code> accomplishes this. The pre-trained <code>en_core_web_sm</code> model contains a sophisticated deep learning neural network that has been trained on millions of text examples. It learned to recognize the patterns and contexts that typically surround named entities. For this course, it's not necessary to understand the complex architecture of the model itself. The key skill is knowing how to effectively use this powerful, pre-trained tool to extract valuable, structured information from your text data.</p> <p>Connecting the Dots: Using NER to Enhance Vectorization</p> <p>Now that you can identify named entities, you can use that information as an advanced step in your data preparation to create more powerful and meaningful features for your models.</p> <p>The Problem: Our standard pre-processing workflow would take a phrase like \"Pro-Grade Blender\" and, after cleaning, might result in two separate tokens: <code>['pro-grade', 'blender']</code>. When this goes into the <code>TfidfVectorizer</code>, the link between these two words is lost. The model sees them as two independent features.</p> <p>The Solution: A more advanced workflow involves running NER before vectorization to \"weld\" the words of an entity into a single, protected token.</p> <p>The process would look like this:</p> <ol> <li>Process your raw text with <code>spaCy</code> to identify the entities.</li> <li>Create a new version of the text where the spaces within each entity are replaced with underscores.</li> <li>Feed this modified text into your <code>TfidfVectorizer</code>.</li> </ol> <p>Conceptual Example:</p> <ul> <li>Original Text: <code>\"her Pro-Grade Blender from GlobalCart arrived\"</code></li> <li>Modified Text (after NER): <code>\"her Pro-Grade_Blender from GlobalCart arrived\"</code></li> <li>Resulting Feature: The vectorizer will now learn a single feature for <code>pro-grade_blender</code>, preserving the full identity of the product.</li> </ul> <p>Why is this useful? This technique protects the integrity of important multi-word concepts, such as product names, company names, and locations, leading to a richer and more accurate numerical representation of your text. It's a powerful way to inject domain knowledge directly into your feature set.</p> <p>By extracting entities, you can now aggregate and analyze this information. For example, you could count how many times each product is mentioned or create a map showing the locations of customers who are providing feedback. This ability to create structured data from unstructured text is a cornerstone of modern data analysis.</p>"},{"location":"adv-data-types/text-analysis/next-steps/","title":"Conclusion and Your Next Steps","text":"<p>Congratulations on completing this lesson on textual data analysis. You have learned analytical workflows that transform raw and unstructured text into structured and powerful business insights. You've learned that text is not just a collection of words, but a rich source of data waiting to be explored.</p> <p>The core lesson has been one of transformation: you learned how to systematically translate ambiguous human language into a clean, numerical format that machine learning algorithms can understand. With this foundation, you are now equipped to go beyond asking what happened and begin to uncover why it happened, using the very words of your customers, clients, and colleagues.</p>"},{"location":"adv-data-types/text-analysis/next-steps/#tying-it-all-together","title":"Tying It All Together","text":"<p>The techniques you've learned in this module form a cohesive and repeatable workflow. While we explored each step individually, it's crucial to see them as part of an integrated process that flows from raw data to actionable insight.</p> <p>The Text Analysis Workflow: A Summary</p> <p>Think of your text analysis projects as following this strategic pipeline.</p> <ol> <li> <p>Problem Definition &amp; Data Collection</p> <ul> <li>Start with a business question. What are you trying to understand?</li> <li>Gather your corpus: the collection of emails, reviews, or reports you will analyze.</li> </ul> </li> <li> <p>Pre-processing with NLTK</p> <ul> <li>This is the cleaning phase. The goal is to reduce noise and standardize the text.</li> <li>Key Steps: Tokenization, lowercasing, stop-word removal, and lemmatization.</li> </ul> </li> <li> <p>Vectorization with scikit-learn</p> <ul> <li>This is the translation phase, turning clean tokens into numerical vectors.</li> <li>Key Choices: Bag-of-Words for simple counts or TF-IDF for weighted importance scores.</li> </ul> </li> <li> <p>Modeling with scikit-learn</p> <ul> <li>This is the analysis phase where you apply a machine learning model to your vectorized data.</li> <li>Supervised Approach: If you have labeled data, use a classifier for tasks like Sentiment Analysis.</li> <li>Unsupervised Approach: If you don't have labels, use an algorithm like NMF for discovery-oriented tasks like Topic Modeling.</li> </ul> </li> <li> <p>Interpretation and Action</p> <ul> <li>This is the final, human-centric step. You interpret the model's output and translate it back into a business context to make data-driven decisions.</li> </ul> </li> </ol>"},{"location":"adv-data-types/text-analysis/next-steps/#ethical-considerations-in-text-analysis","title":"Ethical Considerations in Text Analysis","text":"<p>As you begin to apply these powerful techniques, it is imperative to consider the ethical implications. Machine learning models are powerful pattern-matchers, but they have no understanding of fairness or societal context. They learn directly from the data they are given, which means if there is bias in the data, there will be bias in the model's output.</p> <p>The guiding principle is: Bias In, Bias Out.</p> <p>The Risk of Amplifying Bias</p> <p>Be mindful of how biases hidden in text data can lead to harmful outcomes in a business context.</p> <ul> <li>Hiring and Promotion: A model trained on historical job descriptions and resumes might learn to associate male-coded words with seniority, creating a biased tool that disadvantages female candidates.</li> <li>Customer Service: A sentiment model that hasn't been trained on diverse dialects might misinterpret certain language patterns as negative, leading to unfairly low customer satisfaction scores for specific demographic groups.</li> <li>Content Moderation: An automated system for flagging \"toxic\" comments could disproportionately censor marginalized communities if it's trained on data where their manner of speaking is underrepresented or mislabeled.</li> </ul> <p>As a data professional, it is your responsibility to critically examine your data sources, question the outputs of your models, and ensure they are being used to make fair and equitable decisions.</p>"},{"location":"adv-data-types/text-analysis/next-steps/#what-to-explore-next","title":"What to Explore Next","text":"<p>The foundations you've built in this module open the door to a vast and exciting field. Here are some paths you can explore to continue your learning.</p>"},{"location":"adv-data-types/text-analysis/next-steps/#try-a-project","title":"Try a Project","text":"<p>The best way to solidify your skills is to apply them. Find a dataset that interests you and try to run it through the entire workflow.</p> <ul> <li>Analyze your own emails to find common themes.</li> <li>Use a public API (like Twitter's) to pull tweets about your company and perform sentiment analysis.</li> <li>Find a dataset of product reviews on a site like Kaggle and discover the primary topics of praise or complaint.</li> </ul>"},{"location":"adv-data-types/text-analysis/next-steps/#explore-advanced-techniques","title":"Explore Advanced Techniques","text":"<p>The methods we used are foundational. The state-of-the-art in Natural Language Processing (NLP) has advanced rapidly.</p> <ul> <li>Word Embeddings (Word2Vec, GloVe): We used TF-IDF to represent words as vectors based on their importance. The next step is to use word embeddings, which represent words as dense vectors based on their meaning and context. These models can learn that \"king\" and \"queen\" are semantically similar in a way TF-IDF cannot. The <code>gensim</code> library is a great place to start with this.</li> <li>Transformers (BERT, GPT): These are the models that power modern tools like ChatGPT. They are deeply context-aware and represent the current state-of-the-art for nearly all NLP tasks. The Hugging Face library provides an accessible way to start using these powerful pre-trained models.</li> </ul> <p>You now possess a highly valuable skill set. The ability to unlock insights from unstructured text will enable you to ask better questions and make more nuanced, data-informed decisions in your professional life. Happy analyzing!</p>"},{"location":"adv-data-types/text-analysis/outline/","title":"Outline","text":""},{"location":"adv-data-types/text-analysis/outline/#content-outline-for-static-site-textual-data-analysis","title":"Content Outline for Static Site: Textual Data Analysis","text":""},{"location":"adv-data-types/text-analysis/outline/#section-1-getting-started-with-textual-data","title":"Section 1: Getting Started with Textual Data","text":"<ul> <li>1.1. Introduction: Why Text is a Unique Data Source<ul> <li>Compare structured vs. unstructured data with business examples.</li> <li>Discuss the potential value locked in text: customer feedback, reports, social media, etc.</li> </ul> </li> <li>1.2. The Text Analysis Workflow vs. Traditional ML<ul> <li>A side-by-side comparison (using diagrams/flowcharts) of the steps involved.</li> <li>Emphasize the new, intensive pre-processing stage in the text workflow.</li> </ul> </li> <li>1.3. A Map of Text Analysis Techniques \ud83d\uddfa\ufe0f<ul> <li>Provide a high-level overview of the main goals and how they map to business questions and familiar ML concepts:<ul> <li>Text Classification (e.g., spam detection) -&gt; Supervised Classification</li> <li>Sentiment Analysis (e.g., product review analysis) -&gt; A specialized form of Classification</li> <li>Topic Modeling (e.g., finding themes in support tickets) -&gt; Unsupervised Clustering / Dimensionality Reduction</li> <li>Named Entity Recognition (NER) (e.g., extracting company/product names)</li> </ul> </li> </ul> </li> </ul>"},{"location":"adv-data-types/text-analysis/outline/#section-2-the-foundation-text-preparation-and-cleaning","title":"Section 2: The Foundation: Text Preparation and Cleaning","text":"<ul> <li>2.1. Core Concepts: Corpus, Documents, and Tokens<ul> <li>Define these fundamental terms with clear examples.</li> </ul> </li> <li>2.2. The Pre-processing Toolkit<ul> <li>A detailed page for each of the following techniques, explaining the what, why, and how with code snippets using NLTK.<ul> <li>Tokenization &amp; N-grams</li> <li>Stop Word Removal</li> <li>Stemming vs. Lemmatization (with clear examples of the difference)</li> <li>Part-of-Speech (POS) Tagging (explaining how it improves lemmatization)</li> </ul> </li> </ul> </li> <li>2.3. Our Toolkit: NLTK and its Alternatives<ul> <li>Introduce NLTK and show how to download corpora (e.g., Movie Reviews, stopwords).</li> <li>Briefly introduce spaCy as a modern, efficient alternative, highlighting its key differences.</li> </ul> </li> </ul>"},{"location":"adv-data-types/text-analysis/outline/#section-3-from-words-to-numbers-vectorization","title":"Section 3: From Words to Numbers: Vectorization","text":"<ul> <li>3.1. Why Vectorize? Bridging Text and Machine Learning<ul> <li>Explain the necessity of converting text into a numerical format that algorithms can understand.</li> </ul> </li> <li>3.2. Method 1: Bag-of-Words (BoW)<ul> <li>Explain the concept of creating a document-term matrix.</li> <li>Code demo using <code>scikit-learn</code>'s <code>CountVectorizer</code>.</li> </ul> </li> <li>3.3. Method 2: Term Frequency-Inverse Document Frequency (TF-IDF)<ul> <li>Build on BoW to explain the intuition of weighting words by importance.</li> <li>Code demo using <code>scikit-learn</code>'s <code>TfidfVectorizer</code>.</li> </ul> </li> </ul>"},{"location":"adv-data-types/text-analysis/outline/#section-4-analyzing-sentiment-in-text","title":"Section 4: Analyzing Sentiment in Text","text":"<ul> <li>4.1. Approach 1: The Rule-Based Method (VADER)<ul> <li>Concept: Explain how lexicon-based models work using pre-defined scores.</li> <li>Code Demo: Use <code>nltk.sentiment.vader</code> to analyze sample sentences. Highlight its ability to understand punctuation and capitalization.</li> <li>Pros &amp; Cons: Discuss when to use this method (quick analysis, no training data) and its limitations (no domain context).</li> </ul> </li> <li>4.2. Approach 2: The Machine Learning Method (Corpus-Based)<ul> <li>Concept: Explain how we can train a standard classifier on labeled text data.</li> <li>Code Demo: Build a complete pipeline:<ol> <li>Start with the raw NLTK Movie Reviews corpus.</li> <li>Vectorize it using <code>TfidfVectorizer</code>.</li> <li>Train a <code>LogisticRegression</code> model from <code>scikit-learn</code>.</li> <li>Evaluate its performance using <code>cross_val_score</code>.</li> </ol> </li> </ul> </li> </ul> <ul> <li> <p>4.1. Approach 1: The High-Level Abstraction (TextBlob) \ud83d\udca1</p> <ul> <li>Concept: Introduce TextBlob as a user-friendly library that hides complexity. Frame it as the \"quick and easy\" way to get a result.</li> <li>Code Demo: Show the simplicity of <code>TextBlob(\"text\").sentiment</code>.</li> <li>Lesson: Use this to discuss API design and the power of abstraction, reminding students of the steps from Section 2 that are happening \"under the hood.\"</li> </ul> </li> <li> <p>4.2. Approach 2: The Transparent Rule-Based Method (VADER) \ud83d\udd0e</p> <ul> <li>Concept: Present VADER as another rule-based tool, but one that is more transparent. It's part of the NLTK ecosystem they are already familiar with.</li> <li>Code Demo: Use <code>nltk.sentiment.vader</code> and show the detailed output (positive, negative, neutral, compound scores).</li> <li>Lesson: Compare its specialized nature (tuned for social media) with TextBlob's general-purpose sentiment analysis.</li> </ul> </li> <li> <p>4.3. Approach 3: The \"From-Scratch\" Machine Learning Method (Corpus-Based) \u2699\ufe0f</p> <ul> <li>Concept: Frame this as the most powerful and customizable approach, where you build your own model tailored to your specific domain.</li> <li>Code Demo: Build the full pipeline using <code>scikit-learn</code> with the TF-IDF vectors from Section 3 and a classifier like <code>LogisticRegression</code>.</li> <li>Lesson: This directly connects text analysis back to the core supervised learning skills students have already acquired.</li> </ul> </li> </ul>"},{"location":"adv-data-types/text-analysis/outline/#section-5-uncovering-hidden-themes-with-topic-modeling","title":"Section 5: Uncovering Hidden Themes with Topic Modeling","text":"<ul> <li>5.1. The \"What\" and \"Why\" of Topic Modeling<ul> <li>Explain the goal: To automatically discover abstract topics from a collection of documents.</li> <li>Draw the analogy to unsupervised clustering, where documents are grouped by topic instead of data points by feature similarity.</li> </ul> </li> <li>5.2. Implementing Topic Modeling<ul> <li>Introduce Non-Negative Matrix Factorization (NMF) as a common and interpretable technique available in <code>scikit-learn</code>. Briefly mention Latent Dirichlet Allocation (LDA) as another popular method.</li> <li>Code Demo:<ol> <li>Use the TF-IDF vectors created in Section 3.</li> <li>Apply <code>scikit-learn</code>'s <code>NMF</code> model to extract a set number of topics.</li> </ol> </li> </ul> </li> <li>5.3. Interpreting the Results<ul> <li>Show how to inspect the components of the fitted NMF model to see the top words that define each topic.</li> <li>Discuss how a business might use these topics (e.g., \"Topic 1 is about 'billing and payment issues'\", \"Topic 2 is about 'slow delivery'\").</li> </ul> </li> </ul>"},{"location":"adv-data-types/text-analysis/outline/#section-6-conclusion-and-your-next-steps","title":"Section 6: Conclusion and Your Next Steps","text":"<ul> <li>6.1. Tying It All Together<ul> <li>Provide a comprehensive visual summary of the entire text analysis workflow.</li> </ul> </li> <li>6.2. Ethical Considerations in Text Analysis<ul> <li>A brief discussion on how biases in data can create biased models and the importance of responsible AI.</li> </ul> </li> <li>6.3. What to Explore Next<ul> <li>Suggest projects (e.g., analyze your own emails, tweets, or company reviews).</li> <li>Point towards more advanced concepts for further learning: word embeddings (Word2Vec, GloVe) and transformers (BERT, Hugging Face).</li> </ul> </li> </ul>"},{"location":"adv-data-types/text-analysis/sentiment-analysis/","title":"Strategies for Sentiment Analysis","text":"<p>Now that we have learned how to prepare and vectorize text, we can begin performing analysis. One of the most common and valuable applications of text analysis in a business context is Sentiment Analysis\u2014the process of programmatically identifying and categorizing the opinion or emotion expressed in a piece of text.</p> <p>Is a customer review positive or negative? Is the tone of a support email frustrated or satisfied? Answering these questions at scale can provide critical insights into brand perception, product performance, and customer satisfaction.</p> <p>In this section, we will explore three distinct strategies for performing sentiment analysis, each with its own trade-offs in terms of simplicity, control, and performance. We will progress from simple, \"out-of-the-box\" tools to building our own custom machine learning model.</p>"},{"location":"adv-data-types/text-analysis/sentiment-analysis/#approach-1-the-high-level-abstraction-textblob","title":"Approach 1: The High-Level Abstraction (TextBlob)","text":"<p>The fastest way to get started with sentiment analysis is to use a high-level library that handles all the complexity for you. TextBlob is a popular Python library designed for exactly this. It provides a simple, intuitive API for common natural language processing tasks.</p> <p>When you use TextBlob, it performs pre-processing and applies a pre-trained sentiment analysis model behind the scenes. You simply provide the raw text and receive the result.</p> <pre><code># First, you may need to install the library:\n# pip install textblob\n\nfrom textblob import TextBlob\n\n# Let's use some raw text from our GlobalCart dataset\nreview1 = \"The Pro-Grade Blender is a beast! It's powerful and quiet.\"\nreview2 = \"My package arrived damaged, and the box was completely crushed.\"\n\n# Why: We create TextBlob objects from our raw strings.\n# This object now contains various NLP attributes, including sentiment.\nblob1 = TextBlob(review1)\nblob2 = TextBlob(review2)\n\n# Why: The .sentiment attribute returns a named tuple with two scores.\nsentiment1 = blob1.sentiment\nsentiment2 = blob2.sentiment\n</code></pre> <p>Output (in a readable table):</p> Review Text Polarity Subjectivity Interpretation \"The Pro-Grade Blender is a beast!...\" 0.71 0.95 Very positive and highly subjective. \"My package arrived damaged...\" -0.75 0.75 Very negative and fairly subjective. <p>TextBlob's <code>.sentiment</code> property returns two scores:</p> <ul> <li>Polarity: A score from -1.0 (most negative) to +1.0 (most positive).</li> <li>Subjectivity: A score from 0.0 (very objective) to 1.0 (very subjective).</li> </ul> <p>The Power and Peril of Abstraction</p> <p>TextBlob is a fantastic example of a high-level API. It's incredibly fast and easy to use. However, it's a \"black box\"\u2014it hides the pre-processing steps and uses a general-purpose sentiment lexicon that may not be optimized for your specific domain. It's perfect for quick, general-purpose analysis but offers little control.</p>"},{"location":"adv-data-types/text-analysis/sentiment-analysis/#approach-2-the-transparent-rule-based-method-vader","title":"Approach 2: The Transparent Rule-Based Method (VADER)","text":"<p>A second approach is to use another lexicon and rule-based tool, but one that is more transparent and specialized. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a sentiment analysis tool included with <code>NLTK</code>. It is specifically tuned for social media text, making it excellent at understanding slang, emojis, and the emphatic use of punctuation and capitalization.</p> <p>VADER analyzes text against its lexicon of words and rules and returns a dictionary of four scores.</p> <pre><code>from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Why: We initialize the VADER sentiment analyzer.\n# It only needs to be done once.\nsid = SentimentIntensityAnalyzer()\n\n# Let's use a sentence with features VADER is good at handling\nreview3 = \"Fast shipping is GREAT, but the blender's lid seems defective :-(\"\n\n# Why: The polarity_scores method takes a raw string and returns a dictionary\n# of scores for negative, neutral, and positive sentiment, plus a compound score.\nscores = sid.polarity_scores(review3)\n\nprint(scores)\n</code></pre> <p>Output:</p> <pre><code>{'neg': 0.264, 'neu': 0.526, 'pos': 0.21, 'compound': -0.1949}\n</code></pre> <p>The output contains four scores:</p> <ul> <li><code>neg</code>, <code>neu</code>, <code>pos</code>: The proportion of the text that falls into each category.</li> <li><code>compound</code>: The most useful score. It is a normalized, weighted composite score that ranges from -1 (most negative) to +1 (most positive). A common practice is to classify scores greater than 0.05 as positive, less than -0.05 as negative, and the rest as neutral.</li> </ul> <p>VADER offers more transparency than TextBlob and is particularly effective for informal text, but it is still a pre-built tool that doesn't learn from your specific data.</p>"},{"location":"adv-data-types/text-analysis/sentiment-analysis/#approach-3-the-from-scratch-machine-learning-method","title":"Approach 3: The \"From-Scratch\" Machine Learning Method","text":"<p>The most powerful and flexible approach is to build your own sentiment analysis model. This method leverages all the skills you have learned so far: pre-processing with <code>NLTK</code>, vectorization with <code>scikit-learn</code>, and supervised learning with a <code>scikit-learn</code> classifier.</p> <p>The key advantage here is that the model learns the nuances of sentiment from your own labeled data. This allows it to understand domain-specific language (e.g., that \"quiet\" is a highly positive attribute for a blender but might be neutral for a pair of shoes).</p> <p>Building a Custom Sentiment Classifier</p> <p>Let's build a model to classify our GlobalCart reviews.</p> <p>Step 1: Prepare Labeled Data First, we need a labeled dataset. Let's imagine we've manually labeled our four processed documents from the last section. The vectorized data from our <code>TfidfVectorizer</code> is our feature matrix <code>X</code>, and our manual labels are our target vector <code>y</code>.</p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n# X is our tfidf_matrix from Section 3\n\nX = df_tfidf.to_numpy()\n\n# y is our list of manual labels corresponding to each document\n\ny = ['positive', 'negative', 'negative', 'neutral'] # We will filter to two classes for a simple demo\n\n# For this binary classification demo, let's filter for just positive/negative\n\n# In a real scenario, you'd handle all three classes.\n\nX_binary = X[:3] # First 3 documents\ny_binary = y[:3] # First 3 labels ('positive', 'negative', 'negative')\n</code></pre> <p>Step 2: Train the Model</p> <p>We will use <code>LogisticRegression</code>, a standard and interpretable model for classification tasks.</p> <pre><code> model = LogisticRegression()\n model.fit(X_binary, y_binary)\n</code></pre> <p>Step 3: Evaluate and Predict</p> <p>In a real project, you would split your data into training and testing sets. For this small demo, we can see how the model learned to classify our existing data.</p> <pre><code>predictions = model.predict(X_binary)\nprint(\"Model Predictions:\", predictions)\nprint(\n  \"\\nClassification Report:\\n\", \n  classification_report(y_binary, predictions)\n  )\n</code></pre> <p>Output: <pre><code>Model Predictions: ['positive' 'negative' 'negative']\nClassification Report:\n             precision    recall  f1-score   support\n\nnegative       1.00        1.00      1.00         2\npositive       1.00        1.00      1.00         1\n\naccuracy                             1.00         3\nmacro avg      1.00      1.00        1.00         3\nweighted avg   1.00      1.00        1.00         3\n</code></pre></p> <p>Our model learned perfectly from our tiny dataset! With a much larger, real-world dataset, this approach becomes incredibly powerful and is the standard for building high-performance, custom sentiment analysis systems.</p>"},{"location":"adv-data-types/text-analysis/sentiment-analysis/#summary-choosing-your-strategy","title":"Summary: Choosing Your Strategy","text":"<p>The right strategy depends on your specific needs, data availability, and time constraints.</p> <p>Comparison of Sentiment Analysis Approaches</p> Strategy Ease of Use Customization Data Requirement Best For... TextBlob Very Easy None None Quick, general-purpose sentiment checks. VADER Easy Low None Analyzing social media or informal text. Custom Model Hard High Labeled Data High-accuracy, domain-specific tasks. <p>You are now equipped with a range of tools to analyze sentiment. The next section will shift our focus from supervised classification to unsupervised discovery, where we will learn to find hidden themes in our text data.</p>"},{"location":"adv-data-types/text-analysis/text-data-preparation/","title":"Text Preparation with NLTK","text":"<p>Before we can perform any analysis, we must first translate the messy, human-centric world of text into the clean, structured format that a machine can understand. This crucial phase, often called text pre-processing or normalization, involves cleaning and standardizing our text data. The goal is to reduce noise and variability, ensuring that the meaningful signals in the text are not obscured.</p> <p>In this section, we will use the Natural Language Toolkit (NLTK), a powerful and foundational Python library, to perform these essential steps. Think of NLTK as a workshop full of specialized tools; we will learn how to pick the right tool for each job to take our raw text and forge it into something ready for analysis.</p>"},{"location":"adv-data-types/text-analysis/text-data-preparation/#the-goal-from-raw-document-to-clean-tokens","title":"The Goal: From Raw Document to Clean Tokens","text":"<p>As we established in the previous section, our text data is organized in a <code>corpus -&gt; document -&gt; token</code> hierarchy. Our objective in this pre-processing stage is to take each document (a single customer review) and convert it into a list of clean, meaningful tokens.</p> <p>Let's take one of the reviews from our fictional \"GlobalCart\" dataset: <code>\"My package with the Comfort-Fit Running Shoes arrived damaged, and the box was completely crushed. Disappointed.\"</code></p> <p>Our goal is to transform this raw sentence into a standardized list of tokens, perhaps something like: <code>['package', 'comfort-fit', 'running', 'shoe', 'arrive', 'damage', 'box', 'completely', 'crush', 'disappointed']</code>. Notice how punctuation is gone, words are in lowercase, and some words are shortened to their root form. Let's explore the steps to get there.</p>"},{"location":"adv-data-types/text-analysis/text-data-preparation/#the-pre-processing-toolkit","title":"The Pre-processing Toolkit","text":"<p>We will now walk through the most common pre-processing steps. We'll treat each as a distinct tool we are adding to our collection.</p> <p>Setting up your Toolkit</p> <p>To use NLTK's pre-processing tools, you may first need to download some of its data packages. You can do this by running the following Python code. It only needs to be done once.</p> <pre><code>import nltk\n\n# Downloads the 'punkt' tokenizer model, which is used to split text into words.\nnltk.download('punkt_tab')\n\n# Downloads the 'stopwords' corpus, a list of common words to ignore.\nnltk.download('stopwords')\n\n# Downloads the 'averaged_perceptron_tagger', used for Part-of-Speech tagging.\nnltk.download('averaged_perceptron_tagger')\n\n# Downloads 'wordnet', a large lexical database of English used by the lemmatizer.\nnltk.download('wordnet')\n</code></pre>"},{"location":"adv-data-types/text-analysis/text-data-preparation/#tokenization-breaking-down-sentences","title":"Tokenization: Breaking Down Sentences","text":"<p>The first step is always tokenization. This is the process of breaking down a string of text into its individual components, or tokens. These are typically words, numbers, and punctuation marks.</p> <pre><code>from nltk.tokenize import word_tokenize\n\nraw_text = \"My package with the Comfort-Fit Running Shoes arrived damaged.\"\n\ntokens = word_tokenize(raw_text)\n\nprint(tokens)\n</code></pre> <p>Output:</p> <pre><code>['My', 'package', 'with', 'the', 'Comfort-Fit', 'Running', 'Shoes', 'arrived', 'damaged', '.']\n</code></pre> <p>After this step, we are no longer working with a simple string but with a list of tokens that we can programmatically inspect and manipulate. We will also convert all tokens to lowercase to ensure that words like \"Package\" and \"package\" are treated as the same token.</p>"},{"location":"adv-data-types/text-analysis/text-data-preparation/#stop-word-removal-filtering-out-the-noise","title":"Stop Word Removal: Filtering Out the Noise","text":"<p>Many words in the English language, such as \"the,\" \"a,\" \"in,\" and \"with,\" are essential for grammar but add little semantic value for analysis. These common words are called stop words. Removing them helps our analysis focus on the words that carry the most meaning.</p> <pre><code>from nltk.corpus import stopwords\n\n# Why: We create a set of English stop words for fast lookup.\n# Using a set is much faster than using a list for checking if a word is present.\nstop_words = set(stopwords.words('english'))\n\n# Let's use a new list of tokens from our previous step, after lowercasing them\nraw_tokens = ['my', 'package', 'with', 'the', 'comfort-fit', 'running', 'shoes', 'arrived', 'damaged', '.']\n\n# Why: We use a list comprehension to build a new list containing only the tokens\n# that are NOT in our stop_words set.\nfiltered_tokens = [word for word in raw_tokens if word.lower() not in stop_words]\n\nprint(filtered_tokens)\n</code></pre> <p>Output:</p> <pre><code>['package', 'comfort-fit', 'running', 'shoes', 'arrived', 'damaged', '.']\n</code></pre> <p>Notice how the list is now shorter and more focused on the core concepts of the sentence.</p>"},{"location":"adv-data-types/text-analysis/text-data-preparation/#stemming-and-lemmatization-finding-word-roots","title":"Stemming and Lemmatization: Finding Word Roots","text":"<p>Our text contains different forms of the same word, like \"run,\" \"ran,\" and \"running.\" To a computer, these are three distinct tokens. Stemming and Lemmatization are two techniques for reducing words to their root form.</p> <p>Stemming is a crude, rule-based process of chopping off the ends of words. It's fast but can sometimes result in non-dictionary words.</p> <pre><code>from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n\n# Why: We apply the stemmer to each word to reduce it to its 'stem'.\n# This is a blunt but fast way to group word variations together.\nstemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n\nprint(stemmed_tokens)\n</code></pre> <p>Output:</p> <pre><code>['packag', 'comfort-fit', 'run', 'shoe', 'arriv', 'damag', '.']\n</code></pre> <p>Stemming can be Aggressive</p> <p>Notice that \"package\" became \"packag\" and \"arrived\" became \"arriv.\" Stemmers apply simple suffix-stripping rules and don't care about the context or dictionary validity of the resulting stem.</p> <p>Lemmatization, on the other hand, is a more sophisticated process that uses a dictionary and parts of speech to bring a word back to its root form, known as its lemma. This approach is slower but more accurate.</p> <p>To perform lemmatization effectively, we first need to know the part of speech of each word. Part-of-Speech (POS) tagging is the process of labeling each word in a sentence with its corresponding grammatical category (e.g., noun, verb, adjective).</p> <pre><code>from nltk.stem import WordNetLemmatizer\nfrom nltk.tag import pos_tag\n\n# Let's use our filtered tokens from before\ntokens_to_process = ['package', 'comfort-fit', 'running', 'shoes', 'arrived', 'damaged']\n\n# Why: pos_tag analyzes the list of words and assigns a part-of-speech tag to each one.\n# For example, NN for singular noun, VBD for past tense verb.\ntagged_tokens = pos_tag(tokens_to_process)\n</code></pre> <p>Output (formatted for reading and presentation):</p> Token POS Tag Description package <code>NN</code> Noun, singular comfort-fit <code>JJ</code> Adjective running <code>VBG</code> Verb, gerund shoes <code>NNS</code> Noun, plural arrived <code>VBD</code> Verb, past tense damaged <code>VBN</code> Verb, past participle <p>Now, we can use these tags to get more accurate lemmas.</p> <pre><code>lemmatizer = WordNetLemmatizer()\n\n# Why: By providing the POS tag, we give the lemmatizer crucial context.\n# It now knows 'running' is a verb and its lemma is 'run'. Without the tag, it might be treated as a noun.\n# (Note: A helper function is often needed to map NLTK's POS tags to the format the lemmatizer expects).\n# For simplicity, we'll show the concept directly:\nlemma_run = lemmatizer.lemmatize('running', pos='v') # 'v' for verb\nlemma_shoes = lemmatizer.lemmatize('shoes', pos='n')   # 'n' for noun\n\nprint(f\"The lemma for 'running' as a verb is: {lemma_run}\")\nprint(f\"The lemma for 'shoes' as a noun is: {lemma_shoes}\")\n</code></pre> <p>Output:</p> <pre><code>The lemma for 'running' as a verb is: run\nThe lemma for 'shoes' as a noun is: shoe\n</code></pre> <p>Mapping POS Tags for Accurate Lemmatization</p> <p>You may have noticed a practical challenge when trying to use the output of <code>nltk.pos_tag()</code> with the <code>WordNetLemmatizer</code>. The <code>pos_tag()</code> function returns detailed tags from the Penn Treebank tagset (like <code>VBD</code> for a past-tense verb or <code>NNS</code> for a plural noun), but the lemmatizer's <code>pos</code> parameter expects a much simpler format from the WordNet tagset (<code>n</code> for noun, <code>v</code> for verb, <code>a</code> for adjective, <code>r</code> for adverb, or <code>s</code> for satellite adjective).</p> <p>How do we bridge this gap? The solution is to create a simple helper function that maps the tags from one set to the other.</p>"},{"location":"adv-data-types/text-analysis/text-data-preparation/#the-mapping-logic","title":"The Mapping Logic","text":"<p>The key insight is that most Penn Treebank tags for a given part of speech start with the same letter.</p> <ul> <li>All adjective tags start with 'J' (e.g., <code>JJ</code>, <code>JJR</code>, <code>JJS</code>).</li> <li>All verb tags start with 'V' (e.g., <code>VB</code>, <code>VBD</code>, <code>VBG</code>).</li> <li>All noun tags start with 'N' (e.g., <code>NN</code>, <code>NNS</code>).</li> <li>All adverb tags start with 'R' (e.g., <code>RB</code>, <code>RBR</code>).</li> </ul> <p>We can use this pattern to create a function that translates the tags.</p>"},{"location":"adv-data-types/text-analysis/text-data-preparation/#putting-it-into-practice","title":"Putting it into Practice","text":"<p>Here is a complete example showing how to build and use a helper function to perform accurate, POS-aware lemmatization on a sentence.</p> <pre><code>import nltk\n\nnltk.download('punkt_tab')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger_eng')\nnltk.download('universal_tagset')\nnltk.download('tagsets_json')\n\nimport polars as pl\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer as wnl\nfrom nltk.tag import pos_tag\n\n# The helper function that performs the mapping\ndef get_wordnet_pos(treebank_tag):\n    \"\"\"\n    Maps Treebank POS tags to WordNet POS tags.\n    \"\"\"\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        # As a default, assume noun if the tag is not recognized\n        return wordnet.NOUN\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = wnl()\nsentence = \"The children are running and playing in the beautiful gardens\"\n# 1. Tokenize sentence\ntokens = nltk.word_tokenize(sentence)\n# 2. Tag the tokens\ntagged_tokens = pos_tag(nltk.word_tokenize(sentence))\n# 3. Filter the stop words\nfiltered_tokens = [word for word in tokens if word.lower() not in stop_words]\n# 4. Lemmatize the tokens using POS tags\n# Stop words are not skipped here just for demonstration\nlemmas = [\n    lemmatizer.lemmatize(token, get_wordnet_pos(tag))\n    for token, tag in tagged_tokens\n    # for token, tag in pos_tag(filtered_tokens)\n]\nprint(\"Tokens: \")\nprint(tokens)\nprint(\"Filtered Tokens: \")\nprint(filtered_tokens)\nprint(\"Lemmas: \")\nprint(lemmas)\nprint(\"Filtered Lemmas: \")\nprint(list(lemma for lemma in lemmas if lemma.lower() not in stop_words))\n</code></pre> <p>Output: <pre><code>   Tokens: \n   ['The', 'children', 'are', 'running', 'and', 'playing', 'in', 'the', 'beautiful', 'gardens']\n   Filtered Tokens: \n   ['children', 'running', 'playing', 'beautiful', 'gardens']\n   Lemmas: \n   ['The', 'child', 'be', 'run', 'and', 'play', 'in', 'the', 'beautiful', 'garden']\n   Filtered Lemmas: \n   ['child', 'run', 'play', 'beautiful', 'garden']\n</code></pre></p> <p>Notice how \"children\" correctly became \"child,\" \"are\" became \"be,\" and \"running\" became \"run.\" Without this mapping, the lemmatizer would have defaulted to treating every word as a noun, leading to incorrect results.</p> <p>What is a Satellite Adjective?</p> <p>The WordNet lemmatizer also accepts <code>s</code> for satellite adjectives. This is a specific linguistic category for adjectives that are semantically linked to another adjective and often modify its intensity (e.g., the word \"utter\" in \"an utter fool\"). In practice, these are less common, and mapping all adjective tags (<code>J...</code>) to the general adjective category (<code>a</code>) is the standard and effective approach for most text analysis tasks.</p> <p>Stemming vs. Lemmatization: Which to Choose?</p> <ul> <li>Use Stemming when speed is critical and you don't need the output to be human-readable. It's often sufficient for machine learning models that just need to group related words.</li> <li>Use Lemmatization when you need accurate, dictionary-valid root words, especially if the results need to be interpretable by humans. For most business analyses, lemmatization is the preferred approach.</li> </ul>"},{"location":"adv-data-types/text-analysis/text-data-preparation/#our-toolkit-nltk-and-its-alternatives","title":"Our Toolkit: NLTK and Its Alternatives","text":"<p>Throughout this section, we've used NLTK, the Natural Language Toolkit. It's a powerful and highly flexible library that is excellent for learning because it forces you to engage with each step of the process individually. Its modular design gives you complete control over your text-processing pipeline.</p> <p>It's also worth knowing about spaCy, another popular Python library for text analysis. Where NLTK is a flexible workshop, spaCy is a highly optimized, modern assembly line. It's known for its speed, efficiency, and production-readiness. It often performs many pre-processing steps at once in a single, opinionated pipeline.</p> <p>For this course, we will continue to use NLTK to ensure you understand the fundamentals. However, as you advance, exploring spaCy for building applications is a valuable next step.</p>"},{"location":"adv-data-types/text-analysis/text-data-preparation/#code-demo-processing-documents","title":"Code Demo: Processing Documents","text":"<pre><code> def prep_doc(\n    doc,\n    *,\n    lemmatizer=None,\n    stemmer=None,\n    exclude_stop_words=True,\n):\n    \"\"\"Tokenizes, filters, and processes a text document.\n\n    This function takes a raw text string and performs a series of NLP\n    preprocessing steps, including tokenization, optional stop word removal,\n    optional lemmatization, and optional stemming.\n\n    Args:\n        doc (str): The input text document to process.\n        lemmatizer (object, optional): An initialized lemmatizer object with a\n            `.lemmatize()` method (e.g., from NLTK). Defaults to None.\n        stemmer (object, optional): An initialized stemmer object with a\n            `.stem()` method (e.g., from NLTK). Defaults to None.\n        exclude_stop_words (bool, optional): If True, removes common English\n            stop words from the token list. Defaults to True.\n\n    Returns:\n        dict: A dictionary containing the results of the processing steps.\n            The keys are:\n            - \"tokens\" (list): The original tokens from the document.\n            - \"filtered_tokens\" (list): Tokens after stop word removal (if enabled).\n            - \"lemmas\" (list or None): The lemmatized version of the\n              filtered tokens. None if no lemmatizer is provided.\n            - \"stemmed_tokens\" (list or None): The stemmed version of the\n              filtered tokens. None if no stemmer is provided.\n    \"\"\"\n    tokens = nltk.word_tokenize(doc)\n\n    filtered_tokens = tokens\n    lemmas = None\n    stemmed_tokens = None\n\n    if exclude_stop_words:\n        filtered_tokens = [\n            word for word in tokens if word.lower() not in stop_words\n        ]\n\n    if lemmatizer:\n        lemmas = [\n          lemmatizer.lemmatize(token, get_wordnet_pos(tag))\n          for token, tag in pos_tag(filtered_tokens)\n        ]\n\n    if stemmer:\n        stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n\n    return {\n        \"tokens\": tokens,\n        \"filtered_tokens\": filtered_tokens,\n        \"lemmas\": lemmas,\n        \"stemmed_tokens\": stemmed_tokens,\n    }\n\n  feedback = df.select(pl.col('FeedbackText')).to_series().to_list()\n\n  processed_docs = [\n    prep_doc(doc, lemmatizer=lemmatizer, stemmer=snowball) \n    for doc in feedback\n]\n</code></pre> <p>Now that we have a clean, standardized set of tokens for each document, we are ready for the next critical phase: converting these tokens into a numerical format that a machine learning model can understand.</p>"},{"location":"adv-data-types/text-analysis/topic-modeling/","title":"Uncovering Hidden Themes with Topic Modeling","text":"<p>In the last section, we focused on supervised learning: we had predefined labels (\"positive,\" \"negative\") and trained a model to classify our documents. But what if you don't know the categories in advance? What if you have thousands of customer reviews and simply want to discover, \"What are people talking about?\"</p> <p>This is where Topic Modeling comes in. It is an unsupervised learning technique used to scan a corpus of documents, detect word and phrase patterns within them, and automatically discover abstract \"topics\" that are present in the collection.</p>"},{"location":"adv-data-types/text-analysis/topic-modeling/#the-what-and-why-of-topic-modeling","title":"The \"What\" and \"Why\" of Topic Modeling","text":"<p>The goal of topic modeling is discovery. It helps answer broad questions like:</p> <ul> <li>What are the main themes in our customer support tickets?</li> <li>What features do customers mention most often in product reviews?</li> <li>Are there emerging issues or trends in our feedback data?</li> </ul> <p>Think of it as a powerful form of clustering, an unsupervised task you are already familiar with. With K-Means, you grouped data points based on their proximity in a feature space. With topic modeling, you group documents based on the words they contain. Documents that use similar words will be grouped under the same topic.</p>"},{"location":"adv-data-types/text-analysis/topic-modeling/#implementing-topic-modeling","title":"Implementing Topic Modeling","text":"<p>There are several algorithms for topic modeling, but we will use Non-Negative Matrix Factorization (NMF), a popular and highly interpretable method available in <code>scikit-learn</code>.</p> <p>At a high level, NMF works by taking our document-term matrix (like the TF-IDF matrix we built in Section 3) and decomposing it into two smaller matrices: one that maps documents to topics, and another that maps topics to the words in the vocabulary. It is this second matrix that we will inspect to understand what each topic is about.</p>"},{"location":"adv-data-types/text-analysis/topic-modeling/#discovering-topics-with-nmf","title":"Discovering Topics with NMF","text":"<p>We will use the <code>tfidf_matrix</code> we created in Section 3. This demonstrates the power of vectorization\u2014the same numerical representation of our text can be used for both supervised (sentiment analysis) and unsupervised (topic modeling) tasks.</p> <p>Step 1: Choose the Number of Topics</p> <p>Like with K-Means, we must decide in advance how many topics we want the algorithm to find. This is a hyperparameter you would often tune. For our small \"GlobalCart\" corpus, let's look for two distinct topics.</p> <p>Step 2: Build and Train the NMF Model</p> <pre><code>from sklearn.decomposition import NMF\n\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(processed_corpus)\n\n# hyperparameter\nnum_topics = 2\nnmf_model = NMF(n_components=num_topics, random_state=42)\n\nnmf_model.fit(tfidf_matrix)\n</code></pre>"},{"location":"adv-data-types/text-analysis/topic-modeling/#interpreting-the-results","title":"Interpreting the Results","text":"<p>Now for the most important part: interpretation. The NMF model does not give us labels like \"Shipping Problems.\" Instead, it tells us which words are most important for each topic. It is our job as analysts to look at these lists of words and infer the meaning of the topic.</p> <p>We can inspect the <code>components_</code> attribute of our fitted model to see the word-to-topic mapping.</p> <pre><code># Why: This loop iterates through each topic learned by the model.\n# For each topic, it identifies the words with the highest weights. These\n# words are the most representative of the topic.\nfor index, topic in enumerate(nmf_model.components_):\n    # We get the top 5 words for each topic\n    top_words_indices = topic.argsort()[-5:]\n    top_words = [tfidf_vectorizer.get_feature_names_out()[i] for i in top_words_indices]\n    print(f\"Topic #{index}:\")\n    print(\" \".join(top_words))\n    print(\"-\" * 20)\n</code></pre> <p>Output:</p> <pre><code>Topic #0:\nlarge fit easy return counter\n--------------------\nTopic #1:\ncrush disappointed damage box package\n--------------------\n</code></pre> <p>Looking at this output, we can now perform our human interpretation:</p> <ul> <li>Topic #0 contains words like \"return,\" \"large,\" \"fit,\" and \"counter.\"      This topic seems to be about Product Characteristics and Returns,      likely related to the blender review.</li> <li>Topic #1 contains words like \"package,\" \"damage,\" \"box,\" and \"crush.\"      This topic is clearly about Shipping and Delivery Problems.</li> </ul> <p>Topic Modeling is an Art and a Science</p> <p>The quality of your topics is highly dependent on a few factors:</p> <ul> <li>Good Pre-processing: If you don't clean your text well, your topics might be filled with noise.</li> <li>Number of Topics: Choosing the right number of topics (<code>n_components</code>) is key. You often need to experiment with different numbers to see which yields the most coherent and useful topics.</li> <li>Interpretation: The final step always requires human judgment to translate the model's output into a meaningful business insight.</li> </ul> <p>In just a few lines of code, we have gone from a collection of unstructured reviews to a clear, high-level summary of the main themes our customers are discussing. This allows a business to quickly identify areas of concern or praise without manually reading thousands of documents.</p>"},{"location":"adv-data-types/text-analysis/vectorization/","title":"Vectorization: From Words to Numbers","text":"<p>In the previous section, we successfully transformed our raw text into a clean, standardized list of tokens for each document. This was a critical step in reducing noise, but our data is still in a format that machines cannot use for modeling: a list of words. Machine learning algorithms, including those in <code>scikit-learn</code> that you are familiar with, operate on numerical data, not strings of text.</p> <p>This section covers the final and most crucial bridge between our textual data and the machine learning models: vectorization. This is the process of converting a collection of text documents into a matrix of numerical features. After this step, our text data will finally be in a shape that we can feed directly into a model like <code>LogisticRegression</code>.</p>"},{"location":"adv-data-types/text-analysis/vectorization/#preparing-our-corpus-for-vectorization","title":"Preparing Our Corpus for Vectorization","text":"<p>Let's assume we have performed the pre-processing steps from Section 2 on a few of our \"GlobalCart\" reviews. Our starting point for this section\u2014our clean corpus\u2014is a list of documents, where each document is a list of clean tokens.</p> <pre><code>from nltk.tokenize.treebank import TreebankWordDetokenizer\n\n# This is our pre-processed corpus from the end of Section 2.\n# Each inner list represents one document (one customer review).\ncorpus = [\n    ['blender', 'powerful', 'quiet', 'delivery', 'fast'],\n    ['package', 'arrive', 'damage', 'box', 'crush', 'disappointed'],\n    ['return', 'blender', 'large', 'fit', 'counter', 'return', 'easy'],\n    ['shoe', 'waterproof', 'need', 'know', 'buy']\n]\n\n# Why: For scikit-learn's vectorizers to work, they expect a list of strings,\n# not a list of lists. We'll join our tokens back into single strings.\n# This is a common preparatory step before vectorization.\n\n\n# Method 1: Using join()\nprocessed_corpus = [\" \".join(doc) for doc in corpus]\n\n\n# Method 2: Using a detokenizer\ndetokenizer = TreebankWordDetokenizer()\nprocessed_corpus = [detokenizer.detokenize(doc) for doc in corpus]\n\nprint(processed_corpus)\n</code></pre> <p>Output:</p> <pre><code>[\n  'blender powerful quiet delivery fast',\n  'package arrive damage box crush disappointed',\n  'return blender large fit counter return easy',\n  'shoe waterproof need know buy'\n]\n</code></pre>"},{"location":"adv-data-types/text-analysis/vectorization/#why-a-detokenizer-is-often-better","title":"Why a Detokenizer is Often Better \ud83d\udca1","text":"<p>A dedicated detokenizer is \"smarter\" because it understands how to handle punctuation and contractions correctly. A simple <code>join</code> operation just inserts a space between every token, which can lead to awkward formatting.</p> <p>Consider this example:</p> <pre><code>from nltk.tokenize.treebank import TreebankWordDetokenizer\n\ntokens = ['The', 'blender', 'wasn', \"'t\", 'working', '.']\n\n# Method 1: Using join()\njoined_text = \" \".join(tokens)\nprint(\"Using join():\", joined_text)\n\n\n# Method 2: Using a detokenizer\ndetokenizer = TreebankWordDetokenizer()\ndetokenized_text = detokenizer.detokenize(tokens)\nprint(\"Using Detokenizer:\", detokenized_text)\n</code></pre> <p>Output:</p> <pre><code>Using join(): The blender wasn 't working .\nUsing Detokenizer: The blender wasn't working.\n</code></pre> <p>As you can see, the <code>TreebankWordDetokenizer</code> correctly reattaches the contraction <code>n't</code> to <code>wasn</code> and places the period immediately after \"working\" without an extra space.</p>"},{"location":"adv-data-types/text-analysis/vectorization/#when-join-is-sufficient-for-our-workflow","title":"When <code>\" \".join()</code> is Sufficient for Our Workflow","text":"<p>In the workflow we established, we typically perform several cleaning steps before rejoining the tokens, including:</p> <ul> <li>Removing all punctuation.</li> <li>Converting words to their root form (lemmas).</li> </ul> <p>After these steps, our list of tokens might look like <code>['blender', 'be', 'not', 'work']</code>. In this scenario, the primary advantage of a detokenizer (handling punctuation) is no longer relevant. Both <code>\" \".join()</code> and a detokenizer would produce the exact same, correct output: <code>\"blender be not work\"</code>.</p> <p>Given this context, using the simpler, more fundamental <code>\" \".join()</code> method was pedagogically clearer without sacrificing the quality of the final result for our specific pipeline.</p> <p>When to Use a Detokenizer</p> <ul> <li>Use a detokenizer when your workflow requires you to preserve punctuation and contractions, and you need to reconstruct a grammatically correct sentence.</li> <li>Stick with <code>\" \".join()</code> for simplicity when your pre-processing steps have already removed punctuation, and perfect grammatical reconstruction is not the primary goal.</li> </ul> <p>With our corpus in this format, we are ready to convert it into numbers.</p>"},{"location":"adv-data-types/text-analysis/vectorization/#method-1-the-bag-of-words-bow-model","title":"Method 1: The Bag-of-Words (BoW) Model","text":"<p>The most straightforward and intuitive method for vectorization is the Bag-of-Words (BoW) model. The name is a helpful metaphor: the model treats each document as a \"bag\" containing its words, disregarding grammar and word order, and focuses only on the frequency of each word.</p> <p>The process involves two main steps:</p> <ol> <li>Learn Vocabulary: First, it scans the entire corpus and builds a vocabulary of all unique tokens that appear.</li> <li>Create Vectors: For each document, it creates a numerical vector. This vector will have one entry for every word in the vocabulary. The value of the entry is simply the count of how many times that word appeared in the document.</li> </ol> <p>Let's see this in action using <code>scikit-learn</code>'s <code>CountVectorizer</code>.</p> <pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Why: We initialize the CountVectorizer. This object will learn the vocabulary\n# and convert our text documents into numerical vectors based on word counts.\nvectorizer = CountVectorizer()\n\n# Why: The .fit_transform() method does two things:\n# 1. It learns the vocabulary from our entire corpus.\n# 2. It transforms the corpus into a document-term matrix (a matrix of word counts).\ndocument_term_matrix = vectorizer.fit_transform(processed_corpus)\n\n# Why: We can inspect the learned vocabulary. The columns of our matrix\n# will correspond to these words, in this order.\nvocabulary = vectorizer.get_feature_names_out()\n\n# Why: To make the output easy to understand, we'll convert the matrix\n# to a pandas DataFrame and label the columns with the vocabulary.\ndf_bow = pd.DataFrame(document_term_matrix.toarray(), columns=vocabulary)\n\nprint(\"Vocabulary:\", vocabulary)\n</code></pre> <p>Vocabulary: <code>['arrive' 'blender' 'box' 'buy' 'counter' 'crush' 'damage' 'delivery' 'disappointed' 'easy' 'fast' 'fit' 'know' 'large' 'need' 'package' 'powerful' 'quiet' 'return' 'shoe' 'waterproof']</code></p> <p>Rendered Table Output (<code>df_bow</code>):</p> arrive blender box buy counter crush damage delivery disappointed easy fast fit know large need package powerful quiet return shoe waterproof 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 2 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 2 0 0 3 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 <p>Each row now represents one of our original documents, and each column represents a word in our vocabulary. For example, in Document 2 (row index 2), the word \"return\" appears twice, so its corresponding entry is 2. We have successfully vectorized our text! </p> <p>The Limits of Bag-of-Words</p> <p>BoW is simple and effective, but it has two key limitations:</p> <ol> <li>It completely loses the original word order and context.</li> <li>It treats every word as equally important. A word like \"blender\" might be more important than \"large\" in a document, but BoW only sees their counts.</li> </ol>"},{"location":"adv-data-types/text-analysis/vectorization/#method-2-term-frequency-inverse-document-frequency-tf-idf","title":"Method 2: Term Frequency-Inverse Document Frequency (TF-IDF)","text":"<p>To address the limitations of BoW, we can use a more sophisticated scoring method called Term Frequency-Inverse Document Frequency (TF-IDF). The core idea is to assign a weight to each word that reflects its importance to a specific document within the context of the entire corpus.</p> <p>TF-IDF is a product of two metrics:</p> <ul> <li>Term Frequency (TF): This is essentially the same as BoW. It's the frequency of a term in a document.</li> <li>Inverse Document Frequency (IDF): This is the \"smart\" component. It measures how rare or common a word is across all documents. Words that appear in many documents (e.g., \"blender,\" which appears in two of our four documents) get a low IDF score, while words that are rare get a high IDF score.</li> </ul> <p>The final TF-IDF score for a word is <code>TF * IDF</code>. A word gets a high score if it appears frequently in one document but rarely across the rest of the corpus, making it a good indicator of that document's specific content.</p> <p>Let's use <code>scikit-learn</code>'s <code>TfidfVectorizer</code>.</p> <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Why: We initialize the TfidfVectorizer. Its API is intentionally\n# very similar to CountVectorizer, demonstrating scikit-learn's consistency.\ntfidf_vectorizer = TfidfVectorizer()\n\n# Why: Just like before, .fit_transform() learns the vocabulary and transforms\n# the corpus into a document-term matrix, but this time the values are TF-IDF scores.\ntfidf_matrix = tfidf_vectorizer.fit_transform(processed_corpus)\n\n# Why: We create another DataFrame to easily inspect and compare the results.\ndf_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n</code></pre> <p>Rendered Table Output (<code>df_tfidf</code>): (Note: Values are rounded for display)</p> arrive blender box buy counter crush damage delivery disappointed easy fast fit know large need package powerful quiet return shoe waterproof 0 0.00 0.42 0.00 0.00 0.00 0.00 0.00 0.53 0.00 0.00 0.53 0.00 0.00 0.00 0.00 0.00 0.53 0.53 0.00 0.00 0.00 1 0.41 0.00 0.41 0.00 0.00 0.41 0.41 0.00 0.41 0.00 0.00 0.00 0.00 0.00 0.00 0.41 0.00 0.00 0.00 0.00 0.00 2 0.00 0.31 0.00 0.00 0.39 0.00 0.00 0.00 0.00 0.39 0.00 0.39 0.00 0.39 0.00 0.00 0.00 0.00 0.78 0.00 0.00 3 0.00 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.45 0.00 0.00 0.00 0.00 0.45 0.45 <p>Notice the difference. In the BoW matrix, \"blender\" and \"return\" had scores of 1 and 2 in the third document. In the TF-IDF matrix, the score for \"return\" (0.78) is much higher than for \"blender\" (0.31). This is because \"return\" appears twice in that document, while \"blender\" appears only once and is also present in another document, reducing its IDF score. TF-IDF has successfully identified \"return\" as being more significant to that specific document.</p> <p>We have now successfully converted our text into a rich numerical format. This matrix is the <code>X</code> that you are used to working with. It is ready to be fed into any <code>scikit-learn</code> classifier or clustering algorithm to finally unlock the insights hidden within our text.</p>"},{"location":"adv-data-types/text-analysis/vectorization/#enhancing-features-with-n-grams","title":"Enhancing Features with N-grams","text":"<p>So far, both Bag-of-Words and TF-IDF have treated each word as an independent token. This approach is powerful, but it has one significant drawback: it completely ignores word order and, therefore, crucial context.</p> <p>Consider a review that says a product was \"not good.\" Our current methods, called unigram models, would see the tokens \"not\" and \"good\" separately. If \"not\" is removed as a stop word, we would be left with only \"good,\" completely reversing the original meaning.</p> <p>To solve this, we can enhance our feature creation process by including N-grams, which are contiguous sequences of n words. This allows our vectorizer to capture multi-word phrases and preserve local context.</p>"},{"location":"adv-data-types/text-analysis/vectorization/#what-are-n-grams","title":"What Are N-grams? \ud83d\udd17","text":"<p>An N-gram is simply a contiguous sequence of n items from a given sample of text. The \"items\" are typically words. What we have been doing so far\u2014breaking text down into individual words\u2014is actually called using unigrams (where n=1).</p> <ul> <li>Unigram (1-gram): A single word (e.g., \"blender\", \"package\", \"good\").</li> <li>Bigram (2-gram): A sequence of two adjacent words (e.g., \"running shoes\", \"customer service\", \"very fast\").</li> <li>Trigram (3-gram): A sequence of three adjacent words (e.g., \"pro-grade blender\", \"arrived in one day\").</li> </ul> <p>By creating N-grams, you start to capture word combinations and local context that are lost when you treat every word as an independent token.</p>"},{"location":"adv-data-types/text-analysis/vectorization/#why-are-n-grams-important","title":"Why Are N-grams Important?","text":"<p>N-grams are crucial for capturing context and meaning that individual words alone cannot convey.</p> <ol> <li> <p>Capturing Phrases and Compound Nouns: A word like \"running\" and a word like \"shoes\" have separate meanings. The bigram \"running shoes\", however, refers to a specific product. Analyzing only unigrams would lose this specific concept.</p> </li> <li> <p>Understanding Negation and Modifiers: This is one of the most important use cases. Consider the review \"The service was not good.\"</p> <ul> <li>A unigram model would see the tokens \"not\" and \"good\". Since \"not\" is often a stop word, it might be removed, leaving only the positive word \"good\" and completely misinterpreting the sentiment.</li> <li>A bigram model would see the token \"not good\", which is a powerful signal of negative sentiment.</li> </ul> </li> </ol>"},{"location":"adv-data-types/text-analysis/vectorization/#where-do-n-grams-fit-in-the-workflow","title":"Where Do N-grams Fit in the Workflow? \u2699\ufe0f","text":"<p>You don't typically generate N-grams during the initial cleaning phase. Instead, you instruct the vectorizer to create them from your cleaned tokens.</p> <p>Both <code>CountVectorizer</code> and <code>TfidfVectorizer</code> in <code>scikit-learn</code> have a simple but powerful parameter called <code>ngram_range</code>. This parameter is a tuple that defines the minimum and maximum size of the N-grams to be extracted.</p> <p>For example, <code>ngram_range=(1, 2)</code> tells the vectorizer to create features for all unigrams (single words) AND all bigrams (pairs of words).</p> <p>Here\u2019s a practical example:</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\n# A simple, pre-processed document\ndocument = [\"the blender was not good\"]\n\n# Why: ngram_range=(1, 2) instructs the vectorizer to create tokens for\n# individual words (n=1) and pairs of adjacent words (n=2).\nvectorizer = CountVectorizer(ngram_range=(1, 2))\n\nvectorizer.fit_transform(document)\n\n# Let's look at the features it created\nprint(vectorizer.get_feature_names_out())\n</code></pre> <p>Output:</p> <pre><code>['blender' 'blender was' 'good' 'not' 'not good' 'the' 'the blender' 'was' 'was not']\n</code></pre> <p>As you can see, the vectorizer didn't just create features for individual words; it also created features for word pairs like \"not good\" and \"blender was,\" capturing more context.</p> <p>By including bigrams or even trigrams, our resulting document-term matrix becomes a much richer and more context-aware representation of the original text.</p> <p>\"We have now successfully converted our text into a rich numerical format using Bag-of-Words, TF-IDF, and N-grams. This resulting matrix is the <code>X</code> that you are used to working with, ready to be fed into any <code>scikit-learn</code> classifier or clustering algorithm to finally unlock the insights hidden within our text.\"</p>"},{"location":"adv-data-types/time-series/","title":"Time Series Analysis","text":"<ul> <li> Starter Colab Notebook</li> <li> Time Series Foundations </li> <li> Structural Analysis </li> <li> Time Series Models </li> <li> Modeling Workflow </li> <li> Case Study </li> <li> Conclusion &amp; Next Steps</li> </ul>"},{"location":"adv-data-types/time-series/case-study/","title":"Modeling S&amp;P 500 Using GARCH","text":"<p>In financial markets, predicting the direction of a stock price is incredibly difficult. However, predicting its volatility\u2014how much the price is likely to fluctuate\u2014is often more achievable and just as important for managing risk. This project will walk you through an end-to-end workflow for modeling the volatility of the S&amp;P 500 index using a GARCH (Generalized Autoregressive Conditional Heteroskedasticity) model.</p>"},{"location":"adv-data-types/time-series/case-study/#1-data-acquisition-and-preparation","title":"1. Data Acquisition and Preparation","text":"<p>Our first step is to acquire historical data for the S\\&amp;P 500 index (<code>^GSPC</code>) using the <code>yfinance</code> library. We'll focus on the daily 'Close' price from early 2015 to late 2023.</p> <pre><code>ticker = '^GSPC'\nperiod1 = dt.datetime(2015, 1, 1)\nperiod2 = dt.datetime(2023, 9, 22)\ninterval = '1d' # 1d, 1m\n\nSP_500 = yf.download(ticker, start=period1, end=period2, interval=interval, progress=False)\nSP_500 = SP_500.reset_index()\n\ndf=SP_500[[\"Date\",\"Close\"]]\ndf[\"unique_id\"]=\"1\"\ndf.columns=[\"ds\", \"y\", \"unique_id\"]\nStatsForecast.plot(df)\n</code></pre> S&amp;P 500 Time Series Data Visualized <p>The <code>statsforecast</code> library requires the data to be in a specific format with columns named <code>ds</code> (for the date), <code>y</code> (for the value we're analyzing), and <code>unique_id</code> (to identify the time series). We rename our columns accordingly to prepare for modeling.</p>"},{"location":"adv-data-types/time-series/case-study/#2-analyzing-the-raw-price-series","title":"2. Analyzing the Raw Price Series","text":"<p>Before we can model volatility, we need to understand the characteristics of our data. Most statistical models, including those for volatility, assume the data is stationary.  We can use the Augmented Dickey-Fuller (ADF) test to check for stationarity. </p> <pre><code>def Augmented_Dickey_Fuller_Test_func(series , column_name):\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] &lt;= 0.05:\n        print(\"Conclusion:====&gt;\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====&gt;\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n\n\nAugmented_Dickey_Fuller_Test_func(df[\"y\"],'S&amp;P500')\n</code></pre> <p>Interpreting the ADF Test Results</p> <p>When we run the ADF test on the raw S\\&amp;P 500 closing prices, the p-value will be significantly greater than 0.05. This means we cannot reject the null hypothesis, leading to the conclusion that the raw price data is not stationary. It has a clear trend, making it unsuitable for direct modeling.</p>"},{"location":"adv-data-types/time-series/case-study/#3-transforming-data-to-returns","title":"3. Transforming Data to Returns","text":"<p>To address the non-stationarity, we'll transform the price series into a series of daily returns. Returns are stationary and represent the percentage change in price from one day to the next.</p> <pre><code>df['return'] = 100 * df[\"y\"].pct_change()\ndf.dropna(inplace=True, how='any')\ndf['sq_return'] = df[\"return\"].mul(df[\"return\"])\n\n# visualizing the time series\n\nbase = alt.Chart(df).mark_line().encode(\n    x='ds',\n    # y='return',\n).properties(\n    width=800,\n    height=400,\n    # title=\"SP500 Return Chart\"\n)\nreturn_chart = base.encode(\n    y='return'\n).properties(\n    title=\"SP500 Return Chart\"\n)\nsq_return_chart = base.encode(\n    y='sq_return'\n).properties(\n    title=\"SP500 Squared Return Chart\"\n)\n\n(return_chart | sq_return_chart).properties(\n    title=\"SP500 Return and Squared Return Chart\"\n)\n</code></pre> S&amp;P 500 Returns vs. Squared Returns <p>We also calculate the squared returns, as this is often used as a proxy for financial variance or volatility. The visualization of the returns and squared returns reveals a key characteristic of financial data known as volatility clustering. Notice how periods of high fluctuation are clumped together, followed by periods of relative calm. This is precisely the phenomenon that GARCH models are designed to capture.</p>"},{"location":"adv-data-types/time-series/case-study/#4-checking-for-autocorrelation","title":"4. Checking for Autocorrelation","text":"<p>Next, we use the Ljung-Box test to check if there is significant autocorrelation in our returns data. Autocorrelation means that past values are correlated with future values.</p> <pre><code>ljung_res = acorr_ljungbox(df[\"return\"], lags= 40, boxpierce=True)\n\n# ln_pvalue &lt; 0.05 ? reject null hypotheses i.e. no autocorrelation\nljung_res.head()\n\n\ndf=df[[\"ds\",\"unique_id\",\"return\"]]\ndf.columns=[\"ds\", \"unique_id\", \"y\"]\ntrain = df[df.ds&lt;='2023-05-31'] # Let's forecast the last 30 days\ntest = df[df.ds&gt;'2023-05-31']\ntrain.shape, test.shape\n</code></pre> <p>Interpreting the Ljung-Box Test Results</p> <p>The test results will show very small p-values &lt;0.05 for our returns data. This leads us to reject the null hypothesis of no autocorrelation, confirming that the returns are indeed correlated with their past values. This is another signal that a time-dependent model like GARCH is appropriate.</p>"},{"location":"adv-data-types/time-series/case-study/#5-model-selection-with-cross-validation","title":"5. Model Selection with Cross-Validation","text":"<p>A GARCH model is defined by two parameters, <code>p</code> and <code>q</code>, which represent the number of past squared returns and past variances to include in the model, respectively. To find the best combination of <code>(p,q)</code>, we use cross-validation.</p> <p><code>StatsForecast</code>'s <code>cross_validation</code> function automates this process by creating multiple \"windows\" of training data and testing how well different GARCH models perform at forecasting. We evaluate the models using the Root Mean Squared Error (RMSE), with the goal of finding the model order that produces the lowest average error.</p> <pre><code>season_length = 7 # Dayly data\nhorizon = len(test) # number of predictions biasadj=True, include_drift=True,\n\nmodels = [GARCH(1,1),\n          GARCH(1,2),\n          GARCH(2,2),\n          GARCH(2,1),\n          GARCH(3,1),\n          GARCH(3,2),\n          GARCH(3,3),\n          GARCH(1,3),\n          GARCH(2,3)]\n\nsf = StatsForecast(\n    models=models,\n    freq='C', # custom business day frequency\n)\n\ncrossvalidation_df = sf.cross_validation(df=train,\n                                         h=horizon,\n                                         step_size=6,\n                                         n_windows=5)\n\nprint(crossvalidation_df)\n\n\nevals = evaluate(crossvalidation_df.drop(columns='cutoff'), metrics=[rmse], agg_fn='mean')\nprint(evals)\n</code></pre> <p>Based on the cross-validation results, the <code>GARCH(1,1)</code> model is often a strong performer for financial data, so we'll select that for our final model.</p>"},{"location":"adv-data-types/time-series/case-study/#6-fitting-and-forecasting","title":"6. Fitting and Forecasting","text":"<p>Now, we fit our chosen <code>GARCH(1,1)</code> model to the training dataset and use it to forecast volatility for the hold-out test period.</p> <pre><code>season_length = 7 \nhorizon = len(test) \nmodels = [GARCH(1,1)]\n\nsf = StatsForecast(models=models,\n                   freq='C', \n                  )\nsf.fit(df=train)\n\nStatsForecast(models=[GARCH(1,1)], freq='C')\n\nresult=sf.fitted_[0,0].model_\ndisplay(result)\n\nresidual=pd.DataFrame(result.get(\"actual_residuals\"), columns=[\"residual Model\"])\ndisplay(residual)\n\n\nY_hat = sf.forecast(df=train, h=horizon, fitted=True, level=[95])\ndisplay(Y_hat.head())\nvalues=sf.forecast_fitted_values()\ndisplay(values.head())\nsf.plot(train, Y_hat.merge(test), max_insample_length=200)\n\n\nforecast_df = sf.predict(h=horizon, level=[80,95])\nsf.plot(train, test.merge(forecast_df), level=[80, 95], max_insample_length=200)\n</code></pre> Predicted Returns and the Intervals <p>The plots generated by <code>StatsForecast</code> allow us to visually inspect how well our model's predictions align with the actual returns in the test set. The shaded regions represent the 80% and 95% prediction intervals, giving us a probabilistic range for our forecasts.</p>"},{"location":"adv-data-types/time-series/case-study/#7-final-model-evaluation","title":"7. Final Model Evaluation","text":"<p>Finally, we quantitatively evaluate the model's performance on the test set using a variety of metrics.</p> <pre><code>evaluate(\n    test.merge(Y_hat),\n    metrics=[mae, mape, partial(mase, seasonality=season_length), rmse, smape],\n    train_df=train,\n)\n</code></pre> <p>Metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) tell us the average magnitude of our forecast errors in the same units as the original data (daily returns). The Mean Absolute Percentage Error (MAPE) gives us a sense of the error in percentage terms, which is often easier to interpret. This final evaluation gives us a concrete measure of how well our GARCH model can predict the volatility of the S&amp;P 500.</p>"},{"location":"adv-data-types/time-series/decomposition/","title":"Analyzing Time Series Structure","text":"<p>In time series analysis, \"structure\" refers to the underlying, predictable patterns within the data. We begin by using decomposition to reveal this structure, separating a series into its large-scale components like trend and seasonality. The presence of these components determines a critical property\u2014whether the structure is stationary (stable over time). If it is not, we use techniques like differencing to alter the structure by removing the trend. Finally, once the major patterns are handled, we use ACF/PACF plots to diagnose the remaining, finer-grained correlation structure, which is essential for building an accurate forecasting model.</p>"},{"location":"adv-data-types/time-series/decomposition/#why-this-analysis-matters-from-visuals-to-quantifiables","title":"Why This Analysis Matters: From Visuals to Quantifiables","text":"<p>In any predictive analysis, the goal is to model features that account for the variance in a target variable. In time series analysis, this means understanding where the variance is coming from.</p> <p>A significant portion may come from the long-term trend \ud83d\udcc8, which can be influenced by major factors like new government regulations or shifting market dynamics. For tactical and operational planning, another part of the variance is explained by seasonality \ud83d\udcc5\u2014the predictable patterns that repeat at regular intervals, such as holiday sales spikes.</p> <p>What remains are the residuals \u2753. These help us understand events idiosyncratic to a specific time, like a one-off market shock or a targeted sales campaign. Isolating each of these components is the key to building a comprehensive and reliable forecasting model.</p>"},{"location":"adv-data-types/time-series/decomposition/#time-series-decomposition","title":"Time Series Decomposition","text":"<p>Decomposition is the process of breaking down a time series into its fundamental structural components. This allows us to isolate and analyze each part individually. A time series (\\(Y_t\\)) is typically viewed as a combination of three components:</p> <ul> <li>Trend (\\(T_t\\)): The long-term direction of the series.</li> <li>Seasonality (\\(S_t\\)): The cyclical patterns tied to the calendar.</li> <li>Residual (\\(R_t\\)): The irregular, random noise left over after accounting for the trend and seasonality.</li> </ul> <p>There are two primary models for combining these components:</p> <ol> <li> <p>Additive Model: \\(Y_t = T_t + S_t + R_t\\)</p> <ul> <li>Use this when the magnitude of the seasonal fluctuation is constant and does not depend on the level of the trend.</li> </ul> </li> <li> <p>Multiplicative Model: \\(Y_t = T_t \\times S_t \\times R_t\\)</p> <ul> <li>Use this when the seasonal fluctuation scales with the trend. For example, if holiday sales are consistently 20% higher than the baseline, the absolute size of that fluctuation will grow as the baseline sales trend upwards. This is common in economic data.</li> </ul> </li> </ol> <p>It\u2019s natural to first think of combining components additively (\\(Y = T + S + R\\)), and for many series, this is correct. You should use an additive model when the seasonal variation is roughly constant in absolute terms, regardless of the trend. For example, a local bakery might sell about 200 more loaves every Saturday than on a weekday. This 200-loaf difference remains stable whether their overall weekly sales are trending up or down.</p> <p>However, you should use a multiplicative model (\\(Y = T \\times S \\times R\\)) when the size of the seasonal swing is proportional to the level of the trend. Think of a national retailer whose fourth-quarter sales are consistently 30% higher than other quarters. When the company's annual revenue is $10 million, that 30% jump is $3 million. When the company grows and its revenue is $100 million, that same 30% seasonality now represents a $30 million jump. The seasonal effect grows as the trend grows. This is very common in economic and sales data.</p>"},{"location":"adv-data-types/time-series/decomposition/#code-demo-decomposition-with-statsmodels","title":"Code Demo: Decomposition with <code>statsmodels</code>","text":"<p>The <code>statsmodels</code> library provides a simple function to perform this decomposition. We'll use the classic \"Air Passengers\" dataset, which exhibits a clear trend and multiplicative seasonality.</p> <pre><code># Data prep\ndf = pd.read_csv(\"/content/drive/MyDrive/dataprogpy/data/AirPassengers.csv\", parse_dates=[0])\ndf.columns = [\"month\", \"passengers\"]\ndf.set_index(\"month\", inplace=True)\n\n# Decomposition\ndecomposition = sm.tsa.seasonal_decompose(df['passengers'], model='multiplicative', period=12)\n\n# Visualization\nfig = decomposition.plot()\nfig.set_size_inches(10, 8)\nplt.show()\n</code></pre> Air Passengers Data Decomposed: Trend, Seasonality, and Residual <p>The output plot displays the original observed series and its three extracted components. Analyzing these individual plots is far more insightful than looking at the original series alone.</p>"},{"location":"adv-data-types/time-series/decomposition/#autocorrelation-acf-and-pacf","title":"Autocorrelation (ACF and PACF)","text":"<p>While decomposition reveals the primary trend and seasonal patterns, we also need to understand how each observation relates to its immediate past values. This is measured by autocorrelation.</p> <ul> <li> <p>Autocorrelation Function (ACF): This function measures the correlation of a time series with a lagged version of itself. For example, the ACF at lag-12 measures the correlation between a month's value and its value from the same month one year prior. High ACF values at seasonal lags confirm the presence of seasonality.</p> </li> <li> <p>Partial Autocorrelation Function (PACF): This measures the correlation between a time series and a lagged value after removing the effects of all shorter lags. For example, the PACF at lag-3 measures the \"direct\" effect of the observation three periods ago, after accounting for the influence of the observations at lag-1 and lag-2.</p> </li> </ul> <p>These two functions are the primary diagnostic tools for determining the parameters of ARIMA models once a series has been made stationary.</p> ACF/PACF vs. Standard Correlation <ul> <li>Correlation Coefficient vs. Function:  A Pearson correlation coefficient is a single number that measures the linear relationship between two different variables (e.g., advertising spend and sales). </li> <li>An Autocorrelation Function (ACF) applies the same math, but it calculates the correlation between a series and itself at various past points (lags). It\u2019s a \"function\" because it gives you a whole set of correlation coefficients\u2014one for each lag you examine.</li> </ul> Do I Need Both ACF and PACF? <p>You cannot just pick one because they tell you different things.</p> <ul> <li>ACF shows the \"full\" correlation between a point and a lag. This includes both the direct relationship and indirect relationships that work through the shorter lags in between.</li> <li>PACF shows only the \"direct\" correlation between a point and a lag, after mathematically removing the influence of the shorter, intervening lags.</li> </ul> <p>Analogy: Imagine three people in a line whispering a secret: A -&gt; B -&gt; C. The ACF between A and C would be high because A's message gets to C through B. But the PACF between A and C would be zero, because there is no direct link from A to C; it's entirely mediated by B. The ACF and PACF plots give us these two different views, and the unique patterns they create are what allow us to diagnose the underlying structure of the series for modeling.</p> How do I choose a lag value <p>Remember, the big idea of a lag is to look at a past version of the data to see if it relates to the present. When students ask, \"How do I choose the lag value?\" the answer is that during the initial analysis, you don't choose a single lag. Instead, you use tools like the ACF and PACF plots to examine a whole range of lags at once.</p> <p>The goal is to spot patterns in these plots. For example, in monthly sales data, you might see a significant spike at lag 12, lag 24, and lag 36. This pattern tells you that there is a yearly seasonal relationship in your data. The choice of specific lag values comes later, during the modeling stage, and it's directly guided by the significant lags you identify in these plots.</p>"},{"location":"adv-data-types/time-series/decomposition/#achieving-stationarity-with-differencing","title":"Achieving Stationarity with Differencing","text":"<p>As discussed in Module 1, many models require a series to be stationary. The most common method to remove a trend and achieve stationarity is differencing. This is the process of computing the difference between consecutive observations.</p> <p>Relationship Between Decomposition and <code>diff()</code></p> <p>These concepts are related but serve different purposes. They aren't interchangeable.</p> <ul> <li> <p>Decomposition (Additive/Multiplicative) is an analytical framework. Its purpose is to help you understand and see the different structural components of a series (Trend, Seasonality, Residual). It\u2019s a diagnostic tool.</p> </li> <li> <p>Differencing (<code>diff()</code>) is a transformation technique. Its purpose is to modify the series to achieve stationarity, most commonly by removing the trend component that you identified during your analysis.</p> </li> </ul> <p>Think of it this way: Decomposition is the diagnosis (e.g., \"This series has a strong trend component, making it non-stationary\"). Differencing is the treatment (\"Let's apply differencing to remove that trend\").</p> <pre><code># Create a new column with the differenced passenger counts\ndf_diff = pl.from_pandas(df).with_columns(\n    pl.col(\"Passengers\").diff(n=1).alias(\"passengers_diff\")\n)\n\n# Plot the differenced series\ndf_diff.plot(x=\"Month\", y=\"passengers_diff\")\n</code></pre> Achieving Stationarity via Differencing <p>Notice how the plot of the differenced data no longer has a clear trend; its mean appears constant.</p>"},{"location":"adv-data-types/time-series/decomposition/#code-demo-acfpacf-plots","title":"Code Demo: ACF/PACF Plots","text":"<p>We use <code>statsmodels</code> to plot the ACF and PACF, typically on the stationary (differenced) data.</p> <pre><code>from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# Drop missing values resulting from the differencing\ndifferenced_series = df['Passengers'].diff().dropna()\n\n# Plot the ACF and PACF\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n\nplot_acf(differenced_series, ax=ax1, lags=40)\nplot_pacf(differenced_series, ax=ax2, lags=40)\n\nplt.show()\n</code></pre> Autocorrelation Function (ACF) and Parital Autocorrelation Function Plots Produced Using `statsmodels` <p>How to interpret the plots:</p> <ul> <li>The x-axis represents the number of lags.</li> <li>The y-axis represents the correlation coefficient, from -1 to 1.</li> <li>The blue shaded area is the confidence interval. Spikes that extend beyond this area are considered statistically significant. The patterns of these significant spikes help us choose our model parameters.</li> </ul>"},{"location":"adv-data-types/time-series/decomposition/#a-practical-workflow-for-pre-modeling-analysis","title":"A Practical Workflow for Pre-Modeling Analysis","text":"<p>The concepts we've discussed fit together into a logical workflow that takes you from raw data to a series that is ready for forecasting. This pre-modeling analysis begins with understanding the big picture. Your first step is to decompose the time series to visually identify its primary drivers. This involves separating the data into its Trend, Seasonal, and Residual components, which also forces you to decide between an additive or multiplicative model based on the nature of the seasonality.</p> <p>Once you have a high-level view, the next step is to formally assess stationarity. By examining the trend component from your decomposition, you can diagnose if your series is non-stationary, which is common for most real-world business data.</p> <p>If the series is indeed non-stationary, the third step is to transform it. The goal here is to create a new, stationary version of your data. This is most often accomplished through differencing, which effectively removes the trend. After applying the transformation, you should plot the new series to confirm that it appears stationary.</p> <p>The final step in this preparatory workflow is to analyze the stationary series using ACF and PACF plots. With the major trend and seasonal components handled, these plots allow you to diagnose the finer-grained correlation structure that remains. The patterns you discover here are the crucial clues that will guide you in selecting and configuring an appropriate forecasting model in the next module.</p>"},{"location":"adv-data-types/time-series/foundations/","title":"Foundations of Time Series Analysis","text":""},{"location":"adv-data-types/time-series/foundations/#the-nature-of-time-ordered-data","title":"The Nature of Time-Ordered Data","text":"<p>In business analytics, we encounter two primary types of datasets. The first is cross-sectional data, where the order of observations is not important. For example, when performing customer segmentation, each row represents a different customer, and shuffling the rows does not change the analytical outcome.</p> <p>The second type is time series data, where observations are recorded sequentially over time. Examples include quarterly company earnings, daily website traffic, or minute-by-minute stock prices. In this context, the sequence is critical; the core assumption is that past values hold information that can help explain or forecast future values. Shuffling the data would destroy these vital temporal patterns. This lesson focuses exclusively on this second type of data.</p>"},{"location":"adv-data-types/time-series/foundations/#fundamental-concepts-for-analyzing-a-series","title":"Fundamental Concepts for Analyzing a Series","text":"<p>Before analyzing time series data, it's essential to understand a few key concepts that form the basis of many techniques. These concepts define how we \"view\" and transform the data to uncover insights.</p> <p>Window</p> <p>A window is a selection or \"slice\" of data points of a fixed size from a time series. Imagine you have daily sales data for a year. A window could be a seven-day period. This window can be moved across the data to perform calculations on small, sequential segments.</p> <ul> <li>Fixed Window: A single block of time (e.g., analyzing data for only the first quarter).</li> <li>Rolling Window: This is a more common and powerful concept where the window \"slides\" or \"rolls\" across the time series. For each step forward in time, the window moves one period, a new observation enters the window, and the oldest observation leaves.</li> </ul> <p>Moving Average</p> <p>A moving average is a practical application of a rolling window. It's a technique used to smooth out short-term fluctuations or \"noise\" in a time series, helping to reveal the underlying trend.</p> <p>It is calculated by averaging the data points within a rolling window as it moves through the series. The \"width\" of the window (e.g., 7 days, 30 days) is a critical parameter.</p> <ul> <li>Smaller Window Width (e.g., a 10-day moving average): More responsive to recent changes but less smooth.</li> <li>Larger Window Width (e.g., a 90-day moving average): Produces a smoother line that highlights the long-term trend but is slower to react to new information.</li> </ul> <p>Lag</p> <p>A lag refers to a past value in a time series. A lag of 1 (<code>t-1</code>) for a daily series is the value from the previous day. A lag of 12 (<code>t-12</code>) for a monthly series is the value from the same month one year prior. The concept of a lag is fundamental to understanding how a series relates to its own past, a property known as autocorrelation, which will be covered in the next section of the lesson.</p>"},{"location":"adv-data-types/time-series/foundations/#structural-components-of-a-time-series","title":"Structural Components of a Time Series","text":"<p>Most time series can be conceptually broken down into several underlying components that describe their behavior. The primary goal of exploratory analysis is to identify and understand these components, as they are key to selecting an appropriate forecasting model.</p>"},{"location":"adv-data-types/time-series/foundations/#trend","title":"Trend \ud83d\udcc8","text":"<p>The trend represents the long-term, underlying direction of the data, ignoring the short-term ups and downs. It reflects the persistent, long-run growth or decline of the series. For example, a company's annual revenue might show a consistent upward trend over a decade, even with some variability in quarterly earnings. Identifying the trend is crucial for long-range planning and forecasting.</p>"},{"location":"adv-data-types/time-series/foundations/#seasonality","title":"Seasonality \ud83d\udcc5","text":"<p>Seasonality refers to predictable, repeating patterns or fluctuations that occur at fixed intervals of time. These patterns are tied to a calendar, such as the day of the week, the month, or a quarter. A classic example is the surge in retail sales every fourth quarter due to holiday shopping. Recognizing seasonality is key for inventory management, staffing, and short-term forecasting.</p>"},{"location":"adv-data-types/time-series/foundations/#volatility","title":"Volatility \u26a1","text":"<p>Volatility describes the magnitude of random, unpredictable fluctuations in the data. It's a measure of how much the series tends to vary over time. While all series have some randomness, financial data like stock prices are often characterized by periods of high volatility (large, rapid price swings) and low volatility (relative calm). Understanding volatility is critical for risk assessment and financial modeling.</p>"},{"location":"adv-data-types/time-series/foundations/#stationarity","title":"Stationarity \u2696\ufe0f","text":"<p>A time series is stationary if its statistical properties\u2014specifically its mean, variance, and autocorrelation\u2014are all constant over time. In simpler terms, a stationary series is one whose fundamental behavior doesn't change with time. A series with a clear trend or seasonality is, by definition, non-stationary because its mean or cyclical patterns change over time.</p> <p>The reason stationarity is so important is that many powerful time series forecasting models (like ARIMA models) are designed to work on stationary data. These models assume that the process generating the data is stable, making it possible to model and predict future values. To use these models on non-stationary data, we must first apply transformations to make the series stationary. A common method to remove a trend is differencing, which we will explore in later modules.</p> <p>Before we can effectively apply analytical concepts like moving averages, we must first ensure our data is clean, complete, and uniformly structured. Real-world time series data is rarely perfect. The following section addresses the most common data preparation tasks that are specific to time-ordered data.</p>"},{"location":"adv-data-types/time-series/foundations/#common-data-wrangling-tasks-for-time-series","title":"Common Data Wrangling Tasks for Time Series","text":"<p>While standard data cleaning is always necessary, time series data presents several unique challenges. The methods used to address these issues must respect the temporal ordering of the data to avoid introducing bias or error.</p>"},{"location":"adv-data-types/time-series/foundations/#handling-missing-data","title":"Handling Missing Data","text":"<p>In a standard dataset, you might fill missing values with the mean or median of the entire column. With time series data, this is often a poor choice as it ignores the temporal structure. More appropriate methods include:</p> <ul> <li>Forward Fill (<code>ffill</code>) or Backward Fill (<code>bfill</code>): This propagates the last known observation forward or the next known observation backward. It's a simple method, useful when observations don't change rapidly.</li> <li>Interpolation: This involves filling missing values by estimating them based on neighboring points. Linear interpolation, which assumes a straight line between two points, is common. More complex methods like polynomial or spline interpolation can also be used.</li> <li>Seasonal Adjustment: For data with strong seasonality, missing values can be imputed using the value from a previous season (e.g., the same month in the prior year).</li> </ul> <pre><code>import polars as pl\nfrom datetime import date\n\ndf_missing = pl.DataFrame({\n    \"date\": [dt.date(2024, 1, 1), dt.date(2024, 1, 2), dt.date(2024, 1, 3)],\n    \"value\": [10, None, 15]\n})\n\ndf = (\n    df_missing\n    .with_columns(\n        pl.col(\"value\")\n        .fill_null(strategy=\"forward\")\n        .alias(\"value_forward\")\n        )\n    .with_columns(\n        pl.col(\"value\")\n        .fill_null(strategy=\"backward\")\n        .alias(\"value_backward\")\n    )\n    .with_columns(\n        pl.col(\"value\")\n        .fill_null(strategy=\"mean\")\n        .alias(\"value_mean\")\n    )\n)\nprint(df)\n</code></pre>"},{"location":"adv-data-types/time-series/foundations/#resampling","title":"Resampling","text":"<p>Resampling is the process of changing the data's frequency. It's a common and powerful technique:</p> <ul> <li>Downsampling: Aggregating data from a high frequency to a lower frequency (e.g., from daily to monthly). This is used to smooth out noise and identify long-term trends. You must choose an appropriate aggregation method, such as <code>mean()</code>, <code>sum()</code>, or <code>last()</code>.</li> <li>Upsampling: Increasing the frequency of the data (e.g., from monthly to daily). This is less common and requires careful application of an imputation method (like forward fill or interpolation) to fill in the newly created gaps.</li> </ul> <pre><code># Example DataFrame with daily data\ndf_daily = pl.DataFrame({\n    \"date\": pl.date_range(date(2024, 1, 1), date(2024, 2, 15), \"1d\", eager=True),\n    \"sales\": range(46)\n})\n\n# Downsample to monthly average sales\ndf_monthly = df_daily.group_by_dynamic(\n    \"date\", every=\"1mo\"\n).agg(\n    pl.col(\"sales\").mean().alias(\"average_sales\")\n)\nprint(df_monthly)\n</code></pre>"},{"location":"adv-data-types/time-series/foundations/#dealing-with-irregular-timestamps","title":"Dealing with Irregular Timestamps","text":"<p>Not all time series are recorded at perfect intervals. Financial transaction data or sensor readings, for example, may arrive at irregular times. Many statistical models assume a fixed frequency. To handle this, you may need to regularize the time series by creating a fixed-interval time index and mapping the irregular observations to this new index, using an appropriate aggregation or imputation method to fill the gaps.</p>"},{"location":"adv-data-types/time-series/foundations/#timezone-localization-and-conversion","title":"Timezone Localization and Conversion","text":"<p>Data collected from different geographic locations or systems may have different or nonexistent timezone information. It is critical to standardize all timestamps to a single timezone, typically Coordinated Universal Time (UTC), to ensure that calculations and sequences are correct. This process involves:</p> <ul> <li>Localization: Assigning a timezone to \"naive\" timestamps that lack this information.</li> <li>Conversion: Converting \"aware\" timestamps from one timezone to another.</li> </ul>"},{"location":"adv-data-types/time-series/foundations/#code-demo-eda-with-stock-prices","title":"Code Demo: EDA with Stock Prices","text":"<p>Now, let's apply these concepts using Polars and Altair to analyze daily stock price data for a company like Google (GOOG).</p> <p>First, we load the data and ensure the <code>Date</code> column is correctly formatted as a <code>datetime</code> object.</p> <pre><code>import polars as pl\nimport altair as alt\n\n# This example assumes you have a CSV with stock data\n# You can often download this from financial sites like Yahoo Finance\nfile_path = 'path/to/your/GOOG.csv'\ndf = pl.read_csv(file_path)\n\n# Ensure the Date column is a datetime object for time series operations\ndf = df.with_columns(\n    pl.col(\"Date\").str.to_date(format=\"%Y-%m-%d\")\n)\n\n# Plot the daily closing price\nbase = alt.Chart(df).encode(x='Date:T')\n\nclosing_price = base.mark_line().encode(\n    y=alt.Y('Close:Q', title='Closing Price (USD)')\n).properties(\n    title='Google (GOOG) Daily Stock Price'\n)\n\nclosing_price.display()\n</code></pre> <p>The raw price data is volatile. To better see the trend, we can calculate and plot moving averages using a rolling window. Notice how we explicitly define the window width (<code>window_size</code>).</p> <pre><code># Calculate 30-day and 90-day moving averages\ndf_with_ma = df.with_columns(\n    pl.col(\"Close\").rolling_mean(window_size=30).alias(\"ma_30_day\"),\n    pl.col(\"Close\").rolling_mean(window_size=90).alias(\"ma_90_day\")\n)\n\n# Create layers for the moving averages\nma_30 = base.mark_line(color='orange').encode(\n    y='ma_30_day:Q'\n)\n\nma_90 = base.mark_line(color='red').encode(\n    y='ma_90_day:Q'\n)\n\n# Combine the original price plot with the moving average plots\n(closing_price + ma_30 + ma_90).interactive().display()\n</code></pre> Comparing 30 and 90 day moving averages with daily price changes. <p>In the combined chart, you can clearly see how the 30-day moving average (orange) follows the price more closely, while the 90-day moving average (red) provides a much smoother line, making the longer-term trend easier to discern. This demonstrates the practical effect of choosing different window widths for analysis.</p>"},{"location":"adv-data-types/time-series/modeling-workflow/","title":"Modeling Workflow in Action","text":"<p>In this demonstration, we'll explore how to automate the process of finding a good forecasting model and how to empirically compare different types of models to select the best one for our data. We'll be using the <code>StatsForecast</code> library, which is designed for high-performance time series forecasting.</p>"},{"location":"adv-data-types/time-series/modeling-workflow/#1-automated-arima-modeling","title":"1. Automated ARIMA Modeling","text":"<p>Previously, you learned how to manually select the parameters for an ARIMA model by interpreting ACF and PACF plots. While this is a valuable skill, it can be time-consuming and subjective. The <code>AutoARIMA</code> model automates this process.</p> <p>Conceptually, <code>AutoARIMA</code> intelligently searches through numerous combinations of ARIMA parameters <code>(p,d,q)(P,D,Q)</code> and selects the model that best fits the data according to a statistical criterion (like AIC). This saves you time and provides a reproducible, data-driven starting point.</p> <p>At the implementation level, we instantiate <code>StatsForecast</code> with an <code>AutoARIMA</code> model. We must provide the <code>season_length</code>, which is 12 for the monthly Air Passengers data, so the model knows to look for yearly patterns. We then fit the model and generate a 12-month forecast.</p> <pre><code>df = AirPassengersDF\nsf = StatsForecast(\n    models=[AutoARIMA(season_length = 12)],\n    freq='MS',\n)\nsf.fit(df)\nforecast_df = sf.predict(h=12, level=[90])\nforecast_df.tail()\n</code></pre>"},{"location":"adv-data-types/time-series/modeling-workflow/#2-comparing-multiple-forecasting-models","title":"2. Comparing Multiple Forecasting Models","text":"<p>No single forecasting model is the best for every situation. Different models have different underlying assumptions and excel at capturing different types of patterns. Therefore, a common and effective strategy is to run a \"horse race\" between several models and see which one performs best on your specific data.</p> <p>Conceptually, we are testing a diverse set of models. For example:</p> <ul> <li><code>HoltWinters</code>: A classic model that explicitly captures trend and seasonality.</li> <li><code>SeasonalNaive</code>: A simple but powerful baseline model that predicts the value will be the same as it was in the last season (e.g., this July's value will be the same as last July's).</li> <li><code>HistoricAverage</code>: A very simple model that predicts the average of all past observations.</li> <li><code>DOT (DynamicOptimizedTheta)</code>: A powerful forecasting method based on the Theta decomposition principle. </li> </ul> <p>By comparing these against each other, we can get a much better sense of which approach works best.</p> <p>At the implementation level, we create a list of different model instances and pass them to the <code>StatsForecast</code> class. The library will then fit each of these models to our data simultaneously.</p> <pre><code># Create a list of models and instantiation parameters\nmodels = [\n    HoltWinters(),\n    Croston(),\n    SeasonalNaive(season_length=12),\n    HistoricAverage(),\n    DOT(season_length=12)\n]\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast( \n    models=models,\n    freq='MS', \n    n_jobs=-1,\n    fallback_model=SeasonalNaive(season_length=7),\n    verbose=True,\n)\n\n# Fit &amp; Predict\nforecasts_df = sf.forecast(df=df, h=48, level=[90])\nforecasts_df.head()\n</code></pre> <p>We can then use the built-in <code>plot</code> function to visually inspect the forecasts generated by all the models against the historical data.</p>"},{"location":"adv-data-types/time-series/modeling-workflow/#3-rigorous-model-selection-with-cross-validation","title":"3. Rigorous Model Selection with Cross-Validation","text":"<p>Visual inspection is useful, but for a robust and unbiased comparison, we need to use backtesting, also known as time series cross-validation.</p> <p>Conceptually, cross-validation simulates how our models would have performed in the past. It works by creating several \"windows\" of data. For each window, it trains the models on an initial part of the data and tests their performance on a subsequent part. By averaging the performance across these windows, we get a reliable estimate of each model's true predictive accuracy.</p> <p>At the implementation level, we use the <code>cross_validation</code> method. We specify <code>h=12</code> to test the models' ability to forecast 12 months ahead. The <code>n_windows=2</code> parameter tells the function to create two separate training and testing periods.</p> <pre><code>cv_df = sf.cross_validation(\n    df=df,\n    h=12,\n    step_size=12,\n    n_windows=2\n)\ncv_df.head()\n</code></pre> Prediction Performace by Various Models Prediction Performace by the Best Model"},{"location":"adv-data-types/time-series/modeling-workflow/#4-evaluating-the-results","title":"4. Evaluating the Results","text":"<p>The final step is to calculate a performance metric, like the Mean Squared Error (MSE), for each model on the cross-validation results. The model with the lowest average error is considered the best for this dataset.</p> <p>Conceptually, we are summarizing the performance from our backtesting experiment into a single number for each model, allowing for a direct and objective comparison.</p> <p>At the implementation level, we create a simple helper function to calculate the MSE for each model and identify the one with the minimum error.</p> <pre><code>def evaluate_cv(df, metric):\n    models = df.columns.drop(['unique_id', 'ds', 'y', 'cutoff']).tolist()\n    evals = metric(df, models=models)\n    evals['best_model'] = evals[models].idxmin(axis=1)\n    return evals\n\nevaluation_df = evaluate_cv(cv_df, mse)\ndisplay(evaluation_df.head())\n</code></pre> <p>Finally, we can count the number of times each model was selected as the \"best model\" across the different cross-validation windows. This gives us strong, empirical evidence to support our choice of which model to deploy for future forecasting.</p> <pre><code>evaluation_df['best_model'].value_counts()\n</code></pre>"},{"location":"adv-data-types/time-series/modeling/","title":"Time Series Forecasting Models","text":"<p>Having analyzed our data's structure, we can now move to the primary goal of most time series projects: forecasting. We'll first introduce several powerful statistical models, explaining what they do conceptually before showing how to implement them in Python.</p>"},{"location":"adv-data-types/time-series/modeling/#a-conceptual-guide-to-forecasting-models","title":"A Conceptual Guide to Forecasting Models","text":"<p>Understanding which model to use for a specific job is the most critical skill in forecasting. Each model has a different purpose and set of assumptions.</p>"},{"location":"adv-data-types/time-series/modeling/#arima-the-foundational-forecaster","title":"ARIMA: The Foundational Forecaster","text":"<p>The ARIMA model is the workhorse for forecasting stationary (or stationarized) time series data. The name itself describes how it works:</p> <ul> <li> <p>AR (AutoRegressive) \ud83d\udcc8: This part assumes that the future value is a linear combination of past values. The \"p\" parameter in the model, which tells us how many past values to include, is typically identified by looking for significant spikes in the PACF plot.</p> </li> <li> <p>I (Integrated) \ud83d\udd28: This refers to the differencing step. The \"d\" parameter simply states how many times we had to difference the data to make it stationary. If the data was already stationary, d=0.</p> </li> <li> <p>MA (Moving Average) \ud83d\udcc9: This part assumes that the future value is a function of past forecast errors. The \"q\" parameter, which tells us how many past errors to consider, is identified by looking at the ACF plot.</p> </li> </ul> <p>An ARIMA model is specified by its order <code>(p, d, q)</code>. The analysis you performed in Module 2 is what allows you to choose these parameters intelligently.</p>"},{"location":"adv-data-types/time-series/modeling/#sarima-the-seasonal-specialist","title":"SARIMA: The Seasonal Specialist","text":"<p>If your data exhibits clear seasonality, the SARIMA model is the appropriate tool. It is an extension of ARIMA that adds a second set of parameters to account for the seasonal patterns.</p> <p>Think of it as two ARIMA models working in tandem: one for the non-seasonal part and one for the seasonal part. It's specified as <code>(p,d,q)(P,D,Q)m</code>, where:</p> <ul> <li><code>(p,d,q)</code> are the non-seasonal parameters we've already discussed.</li> <li><code>(P,D,Q)</code> are the seasonal counterparts. They work just like the non-seasonal parameters but operate on data at the seasonal lag.</li> <li><code>m</code> is the number of periods in each season (e.g., m=12 for monthly data with a yearly season).</li> </ul>"},{"location":"adv-data-types/time-series/modeling/#garch-the-volatility-forecaster","title":"GARCH: The Volatility Forecaster","text":"<p>ARIMA and SARIMA are used to model and predict the value (the mean) of a series. GARCH models are used for a different purpose: modeling and predicting the variance or volatility of a series.</p> <p>This is particularly useful in finance, where the volatility of an asset's return is a key measure of risk. GARCH is designed to capture volatility clustering, a common phenomenon where periods of high volatility are followed by more high volatility, and periods of calm are followed by more calm. While ARIMA might predict the stock price, GARCH would predict how unstable or risky that price might be.</p>"},{"location":"adv-data-types/time-series/modeling/#model-fitting","title":"Model Fitting","text":"<p>Now we apply these concepts using the <code>statsmodels</code> and <code>arch</code> Python libraries.</p>"},{"location":"adv-data-types/time-series/modeling/#code-demo-fitting-and-forecasting-with-arima","title":"Code Demo: Fitting and Forecasting with ARIMA","text":"<p>We'll use the <code>statsmodels</code> library to fit an ARIMA model. The key is to provide the <code>order=(p,d,q)</code> that you determined during your analysis in Module 2.</p> <pre><code># From our ACF/PACF analysis, let's say we chose an order of (1,0,1)\n# Note: Since the data is already differenced, 'd' is 0 here.\n\n# Fit the ARIMA model\nmodel = sm.tsa.ARIMA(differenced_series, order=(1, 0, 1))\nresults = model.fit()\n\n# Print the model summary\nprint(results.summary())\n\n# Generate forecasts\nforecast_steps = 24\nforecast = results.forecast(steps=forecast_steps)\n\n# Plot the original series and the forecast\nplt.figure(figsize=(12, 6))\nplt.plot(differenced_series, label='Observed')\nplt.plot(forecast, label='Forecast', color='red')\nplt.title('ARIMA Forecast')\nplt.legend()\nplt.show()\n</code></pre> <p>The <code>results.summary()</code> output provides a wealth of information, including the model coefficients and their statistical significance, which helps you validate your model's structure.</p>"},{"location":"adv-data-types/time-series/next-steps/","title":"Conclusion &amp; Next Steps","text":"<p>Congratulations on completing this introduction to time series analysis! You now have a solid framework for tackling time series problems:</p> <ul> <li>You started by exploring and decomposing data to understand its core structure, identifying key components like trend and seasonality.</li> <li>You learned how to prepare data for modeling by handling stationarity through differencing and engineering time-based features for machine learning models.</li> <li>You built and compared different types of models, like ARMIA, SARIMA, AutoARIMA, and GARCH.</li> <li>Finally, you learned how to rigorously select models using cross validation and evaluate your models using various metrics. </li> </ul> <p>This end-to-end process gives you the ability to turn historical time-ordered data into valuable insights and predictions.</p>"},{"location":"adv-data-types/time-series/next-steps/#next-steps-for-your-learning-journey","title":"Next Steps for Your Learning Journey \ud83d\ude80","text":"<p>Time series forecasting is a deep and fascinating field. This lesson provides a strong foundation, and here are some specific ways you can continue to build on it.</p>"},{"location":"adv-data-types/time-series/next-steps/#apply-these-techniques-to-a-new-project","title":"Apply These Techniques to a New Project","text":"<p>The best way to solidify your knowledge is to apply it. Find a time series dataset relevant to your work or interests\u2014such as your company's sales data, website traffic, or the stock price of a company you follow\u2014and perform the full end-to-end workflow we demonstrated in the case study. Challenge yourself to build both a statistical and a machine learning model and compare their backtested performance.</p>"},{"location":"adv-data-types/time-series/next-steps/#deepen-your-knowledge-of-the-models","title":"Deepen Your Knowledge of the Models","text":"<p>We covered the \"what\" and \"why\" of several models, but there is always more to learn about their inner workings.</p> <ul> <li>Specific Suggestion: Read the user guide for the StatsForecast library or the statsmodels time series analysis section. Also, explore other popular forecasting models we didn't cover, such as Meta's Prophet model, which is known for its ease of use and ability to handle holidays.</li> </ul>"},{"location":"adv-data-types/time-series/next-steps/#explore-advanced-evaluation-metrics","title":"Explore Advanced Evaluation Metrics","text":"<p>While RMSE and MAE are excellent general-purpose metrics, different business problems often require different ways of measuring forecast accuracy.</p> <ul> <li>Specific Suggestion: Research and implement these two metrics:<ol> <li>MASE (Mean Absolute Scaled Error): This is a great scale-independent metric that compares your model's forecast error to the error of a naive seasonal baseline. It helps answer the question, \"Is my complex model actually better than a very simple one?\"</li> <li>Quantile Loss: This metric is used when you care about the accuracy of your prediction intervals, not just a single point forecast. It is essential for problems where understanding the range of uncertainty is critical, such as in inventory management or financial risk assessment.</li> </ol> </li> </ul>"},{"location":"eda/","title":"Exploratory Data Analysis","text":"<ul> <li> <p> Data Wrangling with Polars</p> <p> Introduction to Data Wrangling</p> </li> <li> <p> Data Visualization with Altair</p> <p> Introduction to Data Visualization</p> </li> <li> <p> Data Storytelling</p> <p> What is Data Storytelling and Why a Workflow?</p> </li> </ul>"},{"location":"eda/data-storytelling/","title":"Data Storytelling Workflow","text":"<ul> <li> Starter Colab Notebook</li> <li> Data Storytelling Workflow </li> <li> Analytical Goal Setting </li> <li> Initial Exploration </li> <li> Iterative Execution </li> <li> Narrative Development </li> <li> Review &amp; Validation </li> <li> Conclusion </li> </ul>"},{"location":"eda/data-storytelling/analytical-goal-setting/","title":"Understanding the Landscape &amp; Defining Goals","text":"<p>Every data story begins with understanding the raw material: your data. But data alone doesn't speak; we need to give it a voice by asking the right questions and defining clear goals for our narrative. This phase is about laying that crucial groundwork.</p>"},{"location":"eda/data-storytelling/analytical-goal-setting/#step-1-the-data-dictionary-your-starting-point","title":"Step 1: The Data Dictionary - Your Starting Point \ud83d\uddfa\ufe0f","text":"<p>Before you can ask meaningful questions of your data, you need to understand what information you have, what each piece of information means, its format, and any potential limitations. This is where the data dictionary comes in. It's your map to the dataset.</p> <p>For this lesson, we'll be working with a dataset containing information about house sales in King County, WA (which includes Seattle). Here's a look at its structure:</p> Variable Description Data Type (Examples) <code>id</code> Unique identification number for each house sale. Integer (e.g., 7129300520) <code>date</code> Date the house was sold. String/Date (e.g., \"20141013T000000\") <code>price</code> Sale price of the house. Float/Integer (e.g., 221900.0) <code>bedrooms</code> Number of bedrooms. Integer (e.g., 3) <code>bathrooms</code> Number of bathrooms (can be fractional, e.g., 0.5 represents a half bath). Float (e.g., 1.0, 2.5) <code>sqft_living</code> Square footage of the interior living space. Integer (e.g., 1180) <code>sqft_lot</code> Square footage of the land lot. Integer (e.g., 5650) <code>floors</code> Total number of floors (levels) in the house. Float (e.g., 1.0, 1.5) <code>waterfront</code> Indicates if the property has a waterfront view. <code>1</code> if yes, <code>0</code> if no. Integer (0 or 1) <code>view</code> An index from 0 to 4 of how good the view from the property was (0 = no view, 4 = excellent view). Integer (0-4) <code>condition</code> Rates the overall condition of the house, from 1 (poor) to 5 (very good). Integer (1-5) <code>grade</code> Rates the quality of construction and design, from 1-13. Higher grade indicates better construction and materials. See King County's Glossary for details. Integer (e.g., 7, 8) <code>sqft_above</code> Square footage of interior housing space that is above ground level. Integer (e.g., 1180) <code>sqft_basmt</code> Square footage of interior housing space that is below ground level (basement). <code>0</code> if no basement. Integer (e.g., 0, 500) <code>yr_built</code> Year the house was built. Integer (e.g., 1955) <code>yr_renov</code> Year the house was renovated. <code>0</code> if never renovated or if renovation status is unknown. Integer (e.g., 0, 1991) <code>zipcode</code> 5-digit zip code of the property. String/Integer (e.g., 98178) <code>lat</code> Latitude coordinate of the property. Float (e.g., 47.5112) <code>long</code> Longitude coordinate of the property. Float (e.g., -122.257) <code>squft_liv15</code> Average interior living space (in sqft) of the nearest 15 neighbors. Integer (e.g., 1340) <code>squft_lot15</code> Average land lot size (in sqft) of the nearest 15 neighbors. Integer (e.g., 5080) <code>Shape_leng</code> Polygon length in meters (often GIS-related administrative data). Float <code>Shape_Area</code> Polygon area in meters (often GIS-related administrative data). Float <p>Why spend time on this?</p> <ul> <li>Identify Key Variables: Which variables seem most relevant to potential questions about housing? (e.g., <code>price</code>, <code>sqft_living</code>, <code>bedrooms</code>, <code>zipcode</code>, <code>waterfront</code>).</li> <li>Understand Data Types: Knowing if a variable is numerical (integer, float) or categorical (like <code>zipcode</code>, even if it looks like a number) is crucial for choosing appropriate analysis and visualization methods. For instance, you might calculate an average <code>price</code> (numerical) but you wouldn't average <code>zipcode</code>s.</li> <li>Spot Potential Issues:<ul> <li>Are there missing values or placeholder values (like <code>0</code> for <code>yr_renov</code>) that need special handling?</li> <li>Are there codes or indices (like <code>view</code> or <code>condition</code>) where you need to understand the scale?</li> <li>Does the <code>date</code> field need conversion to a proper date format for time-based analysis? (Hint: Yes, it often does!)</li> </ul> </li> <li>Clarify Definitions: What exactly does <code>grade</code> mean? The link provided gives more context. Understanding these nuances is key to accurate storytelling.</li> </ul> <p>First Look &amp; Familiarization:</p> <p>Take a moment to look through this data dictionary.</p> <ul> <li>What variables immediately catch your eye as potentially interesting for a story about house prices?</li> <li>Are there any variables whose meaning isn't immediately obvious?</li> <li>What kind of audience might be interested in insights from this data? (e.g., potential homebuyers, sellers, real estate agents, urban planners).</li> </ul> \ud83d\udca1 Activity: Initial Variable Brainstorm <p>Jot down 3-5 variables from this list that you think could be central to understanding housing prices or trends in this area. Consider why each might be important. We'll use this as a starting point for formulating questions.</p>"},{"location":"eda/data-storytelling/analytical-goal-setting/#step-2-articulating-analytical-goals-research-questions","title":"Step 2: Articulating Analytical Goals &amp; Research Questions \ud83c\udfaf","text":"<p>Now that we have a basic understanding of our dataset, the next crucial step is to define what we want to achieve with our data story. Simply exploring data without a clear purpose can lead to a scattered collection of charts rather than a focused narrative.</p> <p>Before you ask \"What can the data tell me?\", first ask \"Who am I talking to and what do they need to know?\"</p>"},{"location":"eda/data-storytelling/analytical-goal-setting/#introducing-audience-personas","title":"Introducing Audience Personas","text":"<p>To help focus your efforts, it's incredibly useful to define an audience persona. A persona is a semi-fictional representation of your ideal audience member, based on their characteristics, goals, needs, and pain points. For data storytelling, this helps you:</p> <ul> <li>Empathize: Step into their shoes and see the data from their perspective.</li> <li>Focus: Prioritize questions and insights that are most relevant and valuable to them.</li> <li>Tailor Communication: Choose the right language, complexity, and visual style.</li> <li>Select Key Variables: Identify which parts of the data will resonate most strongly or answer their specific concerns.</li> </ul> <p>Creating a Simple Persona:</p> <p>You don't need an elaborate persona for every analysis, but even a simple one can be powerful. Consider:</p> <ul> <li>Role/Title: Who are they? (e.g., \"First-time Homebuyer,\" \"Real Estate Investor,\" \"City Planner,\" \"Marketing Manager for a Real Estate Agency\").</li> <li>Primary Goal (related to your data): What are they trying to achieve or understand? (e.g., \"Find an affordable 3-bedroom house in a good neighborhood,\" \"Identify undervalued properties for investment,\" \"Understand housing development trends\").</li> <li>Key Questions/Concerns: What specific questions might they have that your data could answer? (e.g., \"What's the typical price range?\", \"Which areas are appreciating fastest?\", \"How do property features like bedrooms or waterfront affect price?\").</li> <li>Data Familiarity: How comfortable are they with data and charts? (This influences how you design your visuals and explanations).</li> </ul> <p>Example Persona for our Housing Data:</p> <ul> <li>Persona Name: \"The Savvy Homebuyer (Sarah)\"</li> <li>Role: A working professional looking to buy her first home.</li> <li>Goal: To understand the King County housing market to find a 2-3 bedroom house that balances affordability, good condition, and reasonable commute, within the next 6-9 months.</li> <li> <p>Key Questions:</p> <ul> <li>What is the general price distribution for houses?</li> <li>How much does <code>sqft_living</code>, <code>bedrooms</code>, or <code>bathrooms</code> impact <code>price</code>?</li> <li>Are there significant price differences between <code>zipcode</code>s?</li> <li>Is a <code>waterfront</code> property completely out of reach? What's the premium?</li> <li>How much does the <code>condition</code> or <code>grade</code> of a house influence its value?</li> <li>Data Familiarity: Comfortable with basic charts, but prefers clear takeaways. Not a data scientist.</li> </ul> </li> </ul> <p>By defining \"Sarah,\" we can now look at our data dictionary (from Step 1) and select variables that are most pertinent to her needs (e.g., <code>price</code>, <code>bedrooms</code>, <code>bathrooms</code>, <code>sqft_living</code>, <code>zipcode</code>, <code>condition</code>, <code>grade</code>, <code>waterfront</code>). Variables like <code>Shape_leng</code> or <code>Shape_Area</code> might be less immediately relevant to Sarah, so we might deprioritize them for a story aimed at her.</p> \ud83d\udca1 Activity: Define Your Audience Persona <p>Before we move on to brainstorming specific research questions, take 5 minutes to sketch out a simple persona for whom you might be analyzing this housing data.</p> <ul> <li>Give them a name/role.</li> <li>What's their primary goal related to this housing data?</li> <li>List 2-3 key questions they might have.</li> </ul> <p>This persona will guide your choices in the subsequent steps.</p> <p>Now that you have a clearer picture of who your data story is for and what they care about, let's translate those needs into concrete analytical goals and research questions. Your persona is your North Star for this process.</p>"},{"location":"eda/data-storytelling/analytical-goal-setting/#approach-1-revisit-your-personas-goals-questions","title":"Approach 1: Revisit Your Persona's Goals &amp; Questions","text":"<p>Look back at the persona you just created.</p> <ul> <li>What are their primary goals?</li> <li>What key questions did you list for them?</li> </ul> <p>These directly inform your analytical objectives. If Sarah the Savvy Homebuyer wants to \"understand the general price distribution,\" then a primary analytical goal for you is to explore and summarize house prices. Her question, \"How much does <code>sqft_living</code> impact <code>price</code>?\" points directly to an analysis of the relationship between these two variables.</p>"},{"location":"eda/data-storytelling/analytical-goal-setting/#approach-2-crafting-simplified-user-stories","title":"Approach 2: Crafting Simplified User Stories","text":"<p>User stories are a common tool in project management and product development, and a simplified version can be very effective for focusing data analysis. They help ensure your work is tied to a specific user need and a desired outcome.</p> <p>The format is typically:</p> <p>\"As a [persona/type of user], I want to [action/understand something] so that I can [achieve a goal/make a decision].\"</p> <p>Let's try this with our \"Sarah the Savvy Homebuyer\" persona and the housing dataset:</p> <ul> <li> <p>Example 1 (Focus on Price Understanding):</p> <ul> <li>\"As Sarah the Savvy Homebuyer, I want to understand the typical range of house prices and how many houses fall into different price brackets so that I can set realistic expectations for my budget.\"</li> <li>This points towards needing a distribution of the <code>price</code> variable.</li> </ul> </li> <li> <p>Example 2 (Focus on Key Features):</p> <ul> <li>\"As Sarah the Savvy Homebuyer, I want to see how features like the number of <code>bedrooms</code>, <code>bathrooms</code>, and <code>sqft_living</code> affect the <code>price</code> so that I can evaluate trade-offs and identify properties that meet my needs and budget.\"</li> <li>This suggests exploring relationships between <code>price</code> and <code>bedrooms</code>, <code>bathrooms</code>, <code>sqft_living</code>.</li> </ul> </li> <li> <p>Example 3 (Focus on Location):</p> <ul> <li>\"As Sarah the Savvy Homebuyer, I want to compare average prices across different <code>zipcode</code>s so that I can identify potentially more affordable neighborhoods that still meet my criteria.\"</li> <li>This indicates a need to analyze <code>price</code> grouped by <code>zipcode</code>.</li> </ul> </li> </ul> <p>Benefits of User Stories for Data Analysis:</p> <ul> <li>Keeps the \"Why\" Front and Center: Connects data work directly to user needs.</li> <li>Aids Prioritization: Helps you decide which analyses are most critical.</li> <li>Facilitates Communication: Clearly articulates the purpose of your analysis to others (e.g., your manager or team).</li> </ul>"},{"location":"eda/data-storytelling/analytical-goal-setting/#approach-3-brainstorming-broad-themes-specific-answerable-questions","title":"Approach 3: Brainstorming Broad Themes &amp; Specific, Answerable Questions","text":"<p>Another approach is to start with broad themes relevant to your persona and dataset, and then drill down into specific, answerable questions.</p> <p>1. Identify Broad Themes: Based on your persona and a general understanding of the dataset, what are the major areas of interest? For the housing data and a homebuyer persona, themes could include:</p> <ul> <li>Price Drivers: What makes houses more or less expensive?</li> <li>Location Impact: How does geography affect value?</li> <li>Property Characteristics: What features are common or desirable?</li> <li>Market Conditions: Are there any discernible trends (if the <code>date</code> field allows for meaningful time-series analysis)?</li> <li>Value for Money: How do condition and grade relate to price?</li> </ul> <p>2. Formulate Specific, Answerable Questions under each Theme:</p> <ul> <li> <p>Theme: Price Drivers</p> <ul> <li>How is <code>price</code> distributed overall?</li> <li>What is the relationship between <code>sqft_living</code> and <code>price</code>?</li> <li>Is there a significant price difference for properties with <code>waterfront</code> access?</li> <li>How does the number of <code>bedrooms</code> or <code>bathrooms</code> correlate with <code>price</code>?</li> <li>Does <code>yr_built</code> or <code>yr_renov</code> have a noticeable impact on <code>price</code>?</li> </ul> </li> <li> <p>Theme: Location Impact</p> <ul> <li>What are the average/median prices per <code>zipcode</code>?</li> <li>Are houses closer to certain amenities (if we could infer this from <code>lat</code>/<code>long</code>) more expensive? (This might be an advanced question requiring external data).</li> </ul> </li> <li>Theme: Property Characteristics<ul> <li>What are the typical ranges for <code>bedrooms</code>, <code>bathrooms</code>, <code>sqft_living</code>?</li> <li>How common are basements (<code>sqft_basmt</code> &gt; 0)?</li> </ul> </li> <li>Theme: Value for Money<ul> <li>How does <code>condition</code> affect <code>price</code>, controlling for size?</li> <li>Is there a clear price premium associated with higher <code>grade</code> levels?</li> </ul> </li> </ul> <p>Characteristics of Good Analytical Questions:</p> <ul> <li>Specific: Not too vague (e.g., \"What about prices?\" is too broad).</li> <li>Measurable/Answerable with Data: You should be able to answer them using the variables available in your dataset.</li> <li>Relevant: Directly related to the persona's needs or the project's goals.</li> <li>Actionable (Ideally): The answers might lead to a decision or a deeper understanding.</li> </ul> <p>It's okay to brainstorm many questions initially. You'll prioritize them later based on relevance to your story and feasibility.</p> \ud83d\udca1 Activity: Draft Your Analytical Questions <p>Based on the persona you created and the techniques above (revisiting persona needs, user stories, or brainstorming themes/questions):</p> <ol> <li>Draft 2-3 specific analytical questions you want to explore using the King County housing dataset.</li> <li>For each question, briefly note which variables from the data dictionary seem most relevant to answering it.</li> </ol> <p>Example:</p> <ul> <li>Question: \"How does the <code>grade</code> of a house relate to its <code>price</code>, and does this relationship differ significantly for houses with and without a <code>waterfront</code>?\"</li> <li>Relevant Variables: <code>grade</code>, <code>price</code>, <code>waterfront</code>. These questions will form the foundation for your data visualization task list in the next step.</li> </ul>"},{"location":"eda/data-storytelling/analytical-goal-setting/#your-milestone-a-clear-set-of-guiding-questions","title":"Your Milestone: A Clear Set of Guiding Questions","text":"<p>Regardless of whether you leaned more heavily on developing a detailed persona, crafting user stories, or brainstorming from broad themes, the critical outcome of this step is a clear, concise set of analytical questions. Think of these questions as the foundation and compass for your entire data story.</p> <p>Why is this set of questions so important at this stage?</p> <ol> <li>Defines Scope: Your questions set the boundaries for your exploration. They help you (and your stakeholders, if any) agree on what's in and what's out of scope for this particular data story, preventing scope creep later on.</li> <li>Directs Analysis: Each question points towards specific variables to examine, relationships to investigate, and comparisons to make. This focused approach is far more efficient than aimless data dredging.</li> <li>Informs Visualization Strategy: As we'll see in the next step, these questions directly translate into what you need to visualize. A question like \"How does price vary by zipcode?\" immediately suggests a visualization involving the <code>price</code> and <code>zipcode</code> variables.</li> <li>Guides Data Cleaning: You'll soon discover that your attempts to answer these questions and visualize the data will highlight which data cleaning tasks are necessary. If a key variable for a crucial question has missing data, cleaning that variable becomes a priority.</li> <li>Measures Success: Ultimately, the success of your data story can be measured by how well it answers these initial questions in a clear, compelling, and accurate way for your intended audience.</li> </ol> <p>Before moving on, review the analytical questions you've drafted:</p> <ul> <li>Are they clear and specific?</li> <li>Are they directly relevant to your persona and their goals?</li> <li>Can they be reasonably answered with the provided dataset?</li> </ul> <p>If you can confidently say \"yes\" to these, you've successfully completed this crucial milestone. These questions are not set in stone\u2014you might refine them or even add new ones as you explore the data\u2014but having a strong initial set is key to a purposeful and effective data storytelling process. They are the terms of reference for the investigative journey ahead.</p>"},{"location":"eda/data-storytelling/analytical-goal-setting/#step-3-initial-data-visualization-task-list-v1-applying-the-grammar-of-graphics","title":"Step 3: Initial Data Visualization Task List (V1) - Applying the Grammar of Graphics \ud83d\udcdd\ud83c\udfa8","text":"<p>You've done the crucial work of defining what questions you want to answer for your persona. Now, we need to determine how to visually represent the data to answer these questions effectively. This isn't just about picking a \"pretty chart\"; it's about a deliberate process of mapping data to visual elements to create clarity and insight.</p>"},{"location":"eda/data-storytelling/analytical-goal-setting/#recap-the-grammar-of-graphics-in-altair","title":"Recap: The Grammar of Graphics in Altair","text":"<p>In previous lessons, you were introduced to Altair, a visualization library in Python that embraces the Grammar of Graphics. This is a foundational concept that helps us build visualizations systematically. Let's quickly recap the core components:</p> <ol> <li>Data: The dataset you're working with (or a specific subset/transformation of it). In our case, the King County housing data.</li> <li>Marks: These are the geometric shapes that represent your data points. Common marks include:<ul> <li><code>mark_point()</code>: For scatter plots, showing individual data points.</li> <li><code>mark_bar()</code>: For bar charts, representing magnitudes or counts.</li> <li><code>mark_line()</code>: For line charts, often showing trends over time or continuous relationships.</li> <li><code>mark_area()</code>: For area charts, similar to line charts but emphasizing volume.</li> <li><code>mark_rect()</code>: For heatmaps or 2D histograms.</li> <li><code>mark_geoshape()</code>: For geographic maps (if you have geographic data).</li> </ul> </li> <li>Encodings: This is where the magic happens. Encodings define how data variables are mapped to the visual properties (or channels) of the marks. Key channels include:<ul> <li><code>x</code>: Position along the x-axis.</li> <li><code>y</code>: Position along the y-axis.</li> <li><code>color</code>: The color of the marks.</li> <li><code>size</code>: The size of the marks.</li> <li><code>shape</code>: The shape of point marks.</li> <li><code>opacity</code>: The transparency of marks.</li> <li><code>tooltip</code>: Information displayed when hovering over a mark.</li> <li><code>column</code>/<code>row</code>: For creating faceted charts (small multiples).</li> </ul> </li> </ol> <p>By combining these components, you can construct a vast array of visualizations. The key is to think about what you want to communicate and then choose the marks and encodings that best serve that purpose.</p>"},{"location":"eda/data-storytelling/analytical-goal-setting/#translating-research-questions-into-visual-specifications","title":"Translating Research Questions into Visual Specifications","text":"<p>Now, let's take the analytical questions you drafted in Step 2 and translate each one into a specific plan for a visualization. For each question, ask yourself:</p> <ol> <li>Data Variables: Which specific columns from your dataset are needed to answer this question? (You likely identified these in the previous step).</li> <li>Mark Type: What geometric mark will best represent the entities or relationships in your question?<ul> <li>Comparing individual values for different categories? <code>mark_bar()</code> is often a good choice.</li> <li>Showing the relationship between two numerical variables? <code>mark_point()</code> (a scatter plot) is a classic.</li> <li>Displaying the distribution of a single numerical variable? <code>mark_bar()</code> (for a histogram) or <code>mark_area()</code> could work.</li> <li>Showing a trend over time? <code>mark_line()</code> is standard.</li> </ul> </li> <li>Key Encodings: How will you map your chosen data variables to the visual channels of your selected mark?<ul> <li>What goes on the <code>x</code>-axis? What goes on the <code>y</code>-axis?</li> <li>Could <code>color</code> be used to represent an additional category or a continuous variable?</li> <li>Would <code>size</code> or <code>shape</code> add useful information without cluttering the visual?</li> <li>What information should appear in a <code>tooltip</code> for interactive exploration?</li> </ul> </li> </ol> <p>Let's walk through an example using one of Sarah the Savvy Homebuyer's potential questions:</p> <ul> <li> <p>Research Question: \"How does the size of the living area (<code>sqft_living</code>) relate to the sale <code>price</code>?\"</p> <ol> <li> <p>Data Variables:</p> <ul> <li><code>price</code> (numerical)</li> <li><code>sqft_living</code> (numerical)</li> <li>(Potentially others for adding layers, e.g., <code>waterfront</code> (categorical) or <code>grade</code> (ordinal) to see if they modify the relationship)</li> </ul> </li> <li> <p>Mark Type:</p> <ul> <li>Since we want to see the relationship between two numerical variables for individual houses, <code>mark_point()</code> is a strong candidate. Each point will represent a single house.</li> </ul> </li> <li> <p>Key Encodings:</p> <ul> <li><code>x</code>: <code>alt.X('sqft_living', type='quantitative', title='Living Area (sq ft)')</code></li> <li><code>y</code>: <code>alt.Y('price', type='quantitative', title='Sale Price (USD)', axis=alt.Axis(format='$,.0f'))</code></li> <li><code>tooltip</code>: <code>['price', 'sqft_living', 'bedrooms', 'bathrooms']</code> (to show details on hover)</li> <li>Optional further encoding (to explore more nuance):<ul> <li><code>color</code>: <code>alt.Color('waterfront:N', title='Waterfront')</code> (to see if waterfront properties show a different pattern). The <code>:N</code> tells Altair to treat <code>waterfront</code> as a Nominal (categorical) variable.</li> </ul> </li> </ul> </li> </ol> </li> </ul> <p>Connecting to Common Chart Types: Notice how these choices naturally lead to a familiar chart type. In the example above, using <code>mark_point()</code> with two quantitative variables encoded to the <code>x</code> and <code>y</code> axes results in a scatter plot. If we were answering \"What is the average price per <code>zipcode</code>?\" we might use <code>mark_bar()</code>, encoding <code>zipcode</code> to the <code>x</code>-axis and average <code>price</code> to the <code>y</code>-axis, resulting in a bar chart.</p> <p>The Grammar of Graphics encourages you to think from these fundamental building blocks, giving you more flexibility than just picking a chart type from a predefined list.</p>"},{"location":"eda/data-storytelling/analytical-goal-setting/#creating-the-initial-data-visualization-task-list-v1","title":"Creating the Initial Data Visualization Task List (V1)","text":"<p>Now it's time to formalize these ideas into an initial task list. This list will be your guide as you start coding your visualizations. For each analytical question, create an entry that outlines your visual plan.</p> <p>Example Task List Entry:</p> Research Question Data Variables Mark Type Key Encodings (X, Y, Color, Size, Tooltip, etc.) Anticipated Chart Type Notes / Potential Challenges How is <code>price</code> distributed overall? <code>price</code> <code>bar</code> <code>x: price</code> (binned), <code>y: count()</code> Histogram Might need to handle outliers or try log scale for price. How does <code>sqft_living</code> relate to <code>price</code>? <code>price</code>, <code>sqft_living</code>, <code>waterfront</code> <code>point</code> <code>x: sqft_living (Q)</code>, <code>y: price (Q)</code>, <code>color: waterfront (N)</code>, <code>tooltip: [price, sqft_living, bedrooms]</code> Scatter Plot Large number of points might cause overplotting. What's the average <code>price</code> by number of <code>bedrooms</code>? <code>price</code>, <code>bedrooms</code> <code>bar</code> <code>x: bedrooms (O)</code>, <code>y: mean(price) (Q)</code>, <code>tooltip: [mean(price), count()]</code> Bar Chart Ensure <code>bedrooms</code> is treated as ordinal/discrete. Consider outliers. <p>Emphasis: This is Version 1!</p> <p>This task list is your starting hypothesis. As you actually create these visualizations (Phase 3), you'll inevitably discover things:</p> <ul> <li>The data might not be in the format you expected.</li> <li>A chosen mark or encoding might not work as well as you thought.</li> <li>The visualization might reveal unexpected patterns that lead to new questions or a desire to try a different visual approach.</li> </ul> <p>This is perfectly normal! Your task list is a living document that you will update and refine.</p> \ud83d\udca1 Activity: Draft Your Initial Visualization Task List <p>Take the 2-3 analytical questions you drafted in Step 2. For each question:</p> <ol> <li>Identify the primary data variables involved.</li> <li>Choose an appropriate mark type from Altair.</li> <li>Define the key encodings (at least <code>x</code> and <code>y</code>, but also consider <code>color</code>, <code>size</code>, <code>tooltip</code>, etc.). Think about the <code>type</code> for each encoding (Quantitative, Nominal, Ordinal, Temporal).</li> <li>(Optional but helpful) Note the anticipated common chart type that these choices would produce.</li> <li>(Optional) Briefly note any initial thoughts or potential challenges.</li> </ol> <p>Create a simple table or list for your tasks, similar to the example above. This will be your first concrete plan for building your data story's visuals.</p>"},{"location":"eda/data-storytelling/analytical-goal-setting/#your-milestone-a-visual-blueprint","title":"Your Milestone: A Visual Blueprint","text":"<p>Congratulations! By translating your analytical questions into a list of visual specifications\u2014defining data, marks, and encodings\u2014you've created the initial blueprint for your data story's visuals. This Data Visualization Task List (V1) is more than just a to-do list; it's a critical output that bridges your analytical goals with the practical steps of visual creation.</p> <p>Why is this \"Visual Blueprint\" so important at this stage?</p> <ol> <li>Tangible Plan for Action: It moves you from abstract questions to a concrete plan for what charts to build. You now have specific starting points for when you begin coding with Altair.</li> <li>Reinforces Purposeful Design: By thinking through marks and encodings for each question, you're practicing deliberate visual design, ensuring each chart element has a purpose related to answering your question and guiding audience attention.</li> <li>Anticipates Data Needs (and a Sneak Peek into Cleaning): Listing the specific variables for each chart often highlights which data will be heavily used. As you attempt to create these visuals (in the upcoming phases), any issues with these key variables (missing values, incorrect types, outliers) will become immediately apparent, directly informing your data cleaning priorities.</li> <li>Facilitates Iteration: While this is V1, having it documented makes it easier to track changes, make decisions about which visuals to refine or discard, and explain your design choices as you iterate.</li> <li>Foundation for the \"Story\": This list is the first glimpse of how individual pieces of analysis (the charts) will eventually come together to form a narrative. You can start to see how different visualizations might connect to tell a larger story.</li> </ol> <p>Before moving on, take a quick look at your Data Visualization Task List:</p> <ul> <li>Does each entry clearly link back to one of your analytical questions?</li> <li>Have you considered the data types (Quantitative, Nominal, Ordinal) for your encodings?</li> <li>Does your choice of marks and encodings seem appropriate for the type of question you're trying to answer (e.g., comparison, relationship, distribution)?</li> </ul> <p>This blueprint isn't rigid; it's meant to evolve. But having this thoughtful initial plan is essential for an efficient and effective journey into the data. It prepares you for the hands-on work of data ingestion, exploration, and the iterative process of cleaning and visualizing that we're about to embark on.</p>"},{"location":"eda/data-storytelling/conclusion/","title":"Conclusion &amp; Best Practices","text":"<p>Congratulations! You've successfully navigated a simulated end-to-end data storytelling workflow. Throughout this lesson, we've aimed to demonstrate that crafting compelling data narratives is not just about technical skill with tools like Python, Polars, and Altair, but also about a structured, thoughtful, and iterative process.</p> <p>Recap of Our Data Storytelling Workflow:</p> <p>We journeyed through several key phases and steps:</p> <p>Phase 1: Understanding the Landscape &amp; Defining Goals</p> <ul> <li>Step 1: Understanding the Data Dictionary.</li> <li>Step 2: Articulating Analytical Goals &amp; Research Questions (using Personas and guiding techniques).</li> <li>Step 3: Creating an Initial Data Visualization Task List (applying the Grammar of Graphics).</li> </ul> <p>Phase 2: Data Ingestion and Initial Exploration</p> <ul> <li>Step 4: Ingesting Data with Polars and performing initial inspections (including a \"smoke test\").</li> <li>Step 5: The \"Reality Check\" of the first pass visualization and identifying \"Uh-Oh\" moments.</li> </ul> <p>Phase 3: Iterative Cleaning and Visualization</p> <ul> <li>Step 6: Developing a Responsive Data Cleaning Task List based on visualization needs and analytical goals.</li> <li>Step 7: Executing Cleaning with Polars and practicing \"Vertical Visual Development\" with Altair, while updating task lists.</li> </ul> <p>Phase 4: Building the Narrative - Horizontal Development</p> <ul> <li>Step 8: Creating a Cohesive Set of Visuals (the \"Comic Strip\" or \"Horizontal Development\"), potentially using Altair's concatenation.</li> <li>Step 9: Adding Narrative Text and Annotations to guide the audience and explain insights.</li> </ul> <p>Phase 5: Review and Validation</p> <ul> <li>Step 10: Checking Against Goals and adopting an MVP (Minimum Viable Product/Story) mindset.</li> <li>Step 11: Applying Final Polish and considering presentation.</li> </ul> <p>Key Best Practices and Takeaways:</p> <ol> <li>Audience-Centricity is Paramount: Always start by thinking about your audience (your persona). Their needs, questions, and level of understanding should guide every decision you make, from formulating analytical questions to choosing chart types and crafting your narrative.</li> <li>The Power of a Plan (and an Iterative One!):<ul> <li>Explicitly listing your analytical questions, visualization tasks, and data cleaning tasks is crucial for focus, project management, and avoiding scope creep.</li> <li>Embrace iteration. Your initial plans are hypotheses. Be prepared to revisit and refine them as you learn more from the data. The \"Uh-Oh\" moments are learning opportunities, not failures.</li> </ul> </li> <li>Visualization as Communication and Guidance: Remember that data visualization is not just about presenting data; it's about communicating insights and strategically guiding your audience's attention to what matters most.<ul> <li>Vertical development (layering within a chart) helps build rich, informative individual visuals.</li> <li>Horizontal development (sequencing charts) helps build a coherent narrative flow.</li> </ul> </li> <li>Clean with Purpose: Data cleaning should be responsive to your visualization needs and analytical goals. Don't aim for a \"perfectly clean\" dataset in a vacuum; aim for data that is fit for your specific storytelling purpose.</li> <li>The Grammar of Graphics Provides Structure: Thinking in terms of data, marks, and encodings (as Altair encourages) gives you a powerful and flexible framework for designing effective visualizations.</li> <li>Words Complete the Story: Visuals are powerful, but narrative text (titles, captions, explanations, transitions) and annotations provide essential context, clarify insights, and ensure your message lands effectively.</li> <li>Embrace the MVP Mindset: Focus on delivering a clear and accurate core message that answers your primary questions first. Nice-to-have refinements can come later. This is especially important in professional settings with deadlines.</li> </ol> <p>Data Storytelling: A Skill Honed Through Practice</p> <p>The workflow and principles we've discussed provide a strong foundation. Like any craft, data storytelling is a skill that improves with practice. The more you apply this process to different datasets and different analytical challenges, the more intuitive it will become.</p> <p>I encourage you to take this workflow and adapt it to your own projects. Start with a question, understand your audience, plan your approach, embrace the iterative process of exploration and refinement, and always aim to communicate your findings with clarity and impact.</p>"},{"location":"eda/data-storytelling/ingest-explore/","title":"Data Ingestion and Initial Exploration","text":"<p>This phase is where the rubber meets the road. We'll load our dataset using Polars and then attempt our first visualizations based on the task list we created. This initial foray into the data often provides a crucial \"reality check,\" revealing challenges and insights that will guide our next steps.</p>"},{"location":"eda/data-storytelling/ingest-explore/#step-4-ingesting-data-with-polars","title":"Step 4: Ingesting Data with Polars \ud83c\udd7f\ufe0f","text":"<p>Our first task is to load the King County housing dataset into a Polars DataFrame. Polars is a blazingly fast DataFrame library written in Rust, offering a modern and efficient way to work with tabular data in Python.</p> <p>Let's assume our data is in a CSV file named <code>kc_house_data.csv</code>.</p> <pre><code># First, ensure you have Polars installed:\n# pip install polars\n\nimport polars as pl\n\n# If you are constantly working across OS platforms, tools like Path will \n# help you write robust code\n# for purely Colab based development environment, this is an overkill.\n\nfrom pathlib import Path\n\n# Define the path to your dataset\n# data root directory is typically where you place all your data file for a project.\n# Some kind of thoughtful directory hierarchy that takes your project specifc \n# workflow into account would be save you a ton of time. \n# Adjust this path to where your file is located.\n\ndata_root = Path(\"/content/drive/MyDrive/dataprogpy/data\")\nhouse_data_path = Path(\"kc_house_data.csv\")\ncounty_file_path = Path(\"kingcounty/King_county_zip.shp\")\nschdst_file_path = Path(\"School_Districts_in_King_County___schdst_area/School_Districts_in_King_County___schdst_area.shp\")\n\n# Load the CSV file into a Polars DataFrame\nhousing_df = pl.read_csv(data_root / house_data_path)\n</code></pre> <p>Once the data is loaded, the very next step is to perform some initial inspections to ensure it loaded correctly and to get a feel for its structure and contents. Polars provides several helpful methods for this:</p> <ol> <li> <p><code>.head()</code> - View the First Few Rows:     This lets you see the column names and a sample of the data to make sure it looks as expected.</p> <p><pre><code># Display the first 5 rows of the DataFrame\nprint(housing_df.head())\n</code></pre> Look at the output. Do the column names match our data dictionary? Do the data values look reasonable at first glance?</p> </li> <li> <p><code>.schema</code> - Understand Data Types:     This attribute shows each column's name and its inferred data type. This is crucial because the data type affects how you can operate on a column (e.g., you can't perform mathematical calculations on a string type without conversion).</p> <p><pre><code>print(housing_df.schema)\n</code></pre> Pay close attention to the data types. For example:</p> <ul> <li>Is <code>price</code> a numerical type (like <code>pl.Float64</code> or <code>pl.Int64</code>)?</li> <li>Is <code>date</code> loaded as a string (<code>pl.Utf8</code>) or has it been automatically parsed into a date/datetime type? (Often, dates in custom formats like \"20141013T000000\" will be read as strings initially.)</li> <li>Are <code>bedrooms</code>, <code>bathrooms</code>, <code>floors</code> numerical?</li> </ul> </li> <li> <p><code>.describe()</code> - Get Summary Statistics:     This method provides descriptive statistics for numerical columns, such as count, mean, standard deviation, min, max, and quartiles. It's a great way to get a quick overview of the distribution and range of your numerical data and spot potential anomalies.</p> <p><pre><code>print(housing_df.describe())\n</code></pre> What can you learn from this?</p> <ul> <li>For <code>price</code>: What are the min, max, and mean prices? Does the standard deviation suggest a wide spread?</li> <li>For <code>bedrooms</code> or <code>bathrooms</code>: Do the min/max values make sense? (e.g., a house with 0 bedrooms, or an unusually high number).</li> <li>For <code>sqft_living</code>, <code>sqft_lot</code>: What are the typical sizes? Are there extreme values?</li> <li>For <code>yr_built</code>: What's the range of construction years?</li> <li>Look for columns with a <code>null_count</code> greater than 0 in the <code>describe</code> output; this indicates missing values.</li> </ul> </li> <li> <p><code>.shape</code> - DataFrame Dimensions:     This attribute returns a tuple <code>(height, width)</code> representing the number of rows and columns.</p> <pre><code>print(f\"\\nThe dataset has {housing_df.height} rows and {housing_df.width} columns.\")\n</code></pre> </li> </ol> <p>This initial inspection is vital. It helps you confirm the data integrity at a high level and identify areas that might need attention before you can effectively visualize or analyze it. For instance, if <code>price</code> was loaded as a string, you'd need to convert it to a number before creating a price distribution histogram.</p> \ud83d\udca1 Activity: Initial Data Check <ol> <li>Run the code snippets above to load and inspect the <code>kc_house_data.csv</code> dataset.</li> <li>Review the output of <code>.head()</code>, <code>.schema</code>, and <code>.describe()</code>.</li> <li>Jot down 2-3 initial observations or potential data issues you notice. For example:<ul> <li>Is the <code>date</code> column a string that will need parsing?</li> <li>Are there any surprising min/max values in <code>describe()</code>?</li> <li>Does the <code>schema</code> show any unexpected data types for key variables you plan to use from your Visualization Task List?</li> </ul> </li> </ol>"},{"location":"eda/data-storytelling/ingest-explore/#performing-a-quick-data-smoke-test","title":"Performing a Quick Data \"Smoke Test\" \ud83d\udd25\ud83e\uddea","text":"<p>Before we even attempt our first chart, it's wise to perform a quick \"smoke test\" on the data. This isn't an exhaustive validation, but rather a set of simple checks to catch glaring issues that would immediately derail any analysis or visualization attempts. Think of it as asking: \"Does this data even remotely make sense at a high level?\"</p> <p>Here are some common smoke tests you can perform with Polars, building on what you've already seen:</p> <ol> <li> <p>Check for Unexpected Nulls in Critical Columns:     While <code>.describe()</code> gives you <code>null_count</code>, you might want to specifically check columns crucial for your initial visualizations (from your Step 3 task list). For example, if <code>price</code> or <code>sqft_living</code> are key to your first few charts:</p> <pre><code>critical_cols = ['price', 'sqft_living', 'bedrooms', 'bathrooms']\nhousing_df.select(\n    pl.col(critical_cols).is_null().sum().name.suffix(\"_nulls\")\n)\n</code></pre> </li> <li> <p>Basic Sanity Check on Numerical Ranges:     Does <code>price</code> contain negative values or zeros where it shouldn't? Are there impossible values for <code>bedrooms</code> (e.g., 0 if that's unexpected, or an extremely high number like 50)?</p> <pre><code># Example: Check for non-positive prices if they are not expected\nnon_positive_prices = housing_df.filter(pl.col('price') &lt;= 0).height\nif non_positive_prices &gt; 0:\n    print(f\"Warning: Found {non_positive_prices} records with non-positive prices.\")\n\n# Example: Check for an unusually high number of bedrooms\nunusual_bedrooms = housing_df.filter(pl.col('bedrooms') &gt; 20).height # Assuming &gt;20 is unusual\nif unusual_bedrooms &gt; 0:\n    print(f\"Warning: Found {unusual_bedrooms} records with more than 20 bedrooms.\")\n</code></pre> </li> <li> <p>Check Unique Values for Key Categorical/Boolean-like Columns:     For columns like <code>waterfront</code> (expected to be 0 or 1) or <code>condition</code> (expected 1-5), quickly check their unique values.</p> <pre><code>unique_waterfront = housing_df['waterfront'].unique().sort()\nprint(f\"\\nUnique values in 'waterfront': {unique_waterfront.to_list()}\")\n# Expected: [0, 1] or similar based on data dictionary\n\nunique_condition = housing_df['condition'].unique().sort()\nprint(f\"Unique values in 'condition': {unique_condition.to_list()}\")\n    # Expected: [1, 2, 3, 4, 5]\n</code></pre> </li> <li> <p>Sufficient Data Volume:     Is the DataFrame completely empty or has an unexpectedly low number of rows after loading? (<code>housing_df.height</code> check we've been using).</p> </li> </ol> <p>If these basic smoke tests reveal significant problems (e.g., a critical column is entirely missing, <code>price</code> is all nulls, <code>waterfront</code> has values like 'Yes'/'No' instead of 0/1 when 0/1 is expected by downstream code), you'd likely need to revisit the data source or ingestion script before proceeding.</p> <p>Passing these smoke tests doesn't guarantee the data is perfect, but it gives you reasonable confidence that the fundamentals are in place for you to start the more detailed work of visualization and iterative cleaning.</p> \ud83d\udca1 Activity: Run Your Smoke Test <ol> <li>Adapt and run some of the smoke test code snippets above on the housing dataset.</li> <li>Focus on variables that are key to the first 1-2 visualizations you planned in your Step 3 Task List.</li> <li>Did you find any immediate \"red flags\" that would stop you from proceeding directly to visualization?</li> </ol> <p>If your data passes this initial smoke screen, you're ready to try building your first visual!</p>"},{"location":"eda/data-storytelling/ingest-explore/#step-5-first-pass-visualization-the-reality-check","title":"Step 5: First Pass Visualization &amp; The \"Reality Check\" \ud83d\udcc9\ud83d\udd0e","text":"<p>This is where your planning meets practice. You'll take one of the visualization tasks from your list (from Step 3) and attempt to implement it using Python and Altair. This first attempt is incredibly valuable, not because it's expected to be perfect, but because it often provides a stark \"reality check\" about your data or your initial visual design.</p> <p>The Goal: To create one of your planned visualizations and, more importantly, to observe what happens \u2013 what works, what doesn't, and what surprises the data throws at you.</p> <p>Let's Try an Example: Suppose from our Step 3 Visualization Task List, we had the following task, aimed at answering Sarah the Savvy Homebuyer's question about the relationship between living area and price:</p> Research Question Data Variables Mark Type Key Encodings (X, Y, Color, Size, Tooltip, etc.) Anticipated Chart Type How does <code>sqft_living</code> relate to <code>price</code>? <code>price</code>, <code>sqft_living</code> <code>point</code> <code>x: sqft_living (Q)</code>, <code>y: price (Q)</code>, <code>tooltip: [price, sqft_living, bedrooms, bathrooms]</code> Scatter Plot <p>Now, let's try to code this in Altair using our <code>housing_df</code> Polars DataFrame:</p> <pre><code>import altair as alt\n# Assuming housing_df is your Polars DataFrame loaded in Step 4\n\nalt.data_transformers.disable_max_rows()\n# By default, Altair has a transformer that limits the number of rows it will process for performance reasons.\n# The .disable_max_rows() method turns off this default behavior.\n# This is useful when you have a large dataset and want to ensure all data points are considered for your visualization, although it might impact performance.\n\nscatter_plot = alt.Chart(housing_df).mark_point().encode(\n    x=alt.X('sqft_living:Q', title='Living Area (sq ft)'), # :Q denotes Quantitative\n    y=alt.Y('price:Q', title='Sale Price (USD)', axis=alt.Axis(format='$,.0f')),\n    tooltip=['price', 'sqft_living', 'bedrooms', 'bathrooms']\n).properties(\n    title='Price vs. Living Area',\n    width=600,\n    height=400\n).interactive() # Adds panning and zooming\n\n# To display the chart in many environments (like a Jupyter notebook or an IDE with an Altair renderer):\nscatter_plot.show()\n# In MkDocs, you might save it to a JSON file and embed it, or use a Python-to-JS bridge.\n# For now, we'll assume you can display it or inspect its structure.\nprint(\"Scatter plot generated (or attempted).\")\n</code></pre> <p>The \"Uh-Oh\" Moment: What Could Go Wrong or Look... Not Quite Right? \ud83d\ude32</p> <p>You run the code. What might you see? This is where the \"reality check\" happens. Here are some common scenarios:</p> <ol> <li> <p>Error Messages:</p> <ul> <li>Incorrect Data Types: If <code>price</code> or <code>sqft_living</code> were accidentally loaded as strings (<code>pl.Utf8</code>) and you didn't convert them to numerical types (<code>pl.Float64</code> or <code>pl.Int64</code>), Altair (or the underlying Vega-Lite) would likely complain or produce an incorrect chart.<ul> <li>Uh-Oh Example: Your <code>price</code> axis might show alphabetical sorting if it's a string, or the plot might fail entirely.</li> </ul> </li> <li>Column Not Found: A typo in a column name (<code>'sqft_liv'</code> instead of <code>'sqft_living'</code>) will cause an error.</li> </ul> </li> <li> <p>The \"Blob\" / Overplotting:</p> <ul> <li>With many thousands of houses in our dataset, a simple scatter plot might just look like a dense, unreadable blob of points, especially in areas with many similarly priced/sized homes. You can't distinguish individual points or see the density clearly.<ul> <li>Uh-Oh Example: Our <code>price</code> vs. <code>sqft_living</code> plot will almost certainly suffer from this.</li> </ul> </li> </ul> </li> <li> <p>Skewed Distributions &amp; Outliers:</p> <ul> <li>The <code>price</code> variable, in particular, is often right-skewed (many houses at lower-to-mid prices, and a few very expensive ones). These outliers can \"squish\" the majority of data points into a small area of the chart, making it hard to see patterns among them.<ul> <li>Uh-Oh Example: Most points on your scatter plot might be clustered in the bottom-left, with a few far-flung points dominating the axes. Your axes scales might be automatically set to accommodate these outliers, hiding detail in the denser regions.</li> </ul> </li> </ul> </li> <li> <p>Ineffective Encodings:</p> <ul> <li>Perhaps your initial choice of encodings doesn't effectively answer the question. Maybe a linear scale for <code>price</code> isn't as revealing as a logarithmic scale would be, given the skew. Or perhaps adding a <code>color</code> encoding for <code>grade</code> would make the scatter plot much more insightful.</li> </ul> </li> <li> <p>Date/Time Issues:</p> <ul> <li>If your question involved the <code>date</code> column (e.g., \"How do prices change over time?\") and you haven't converted the \"20141013T000000\" string format into a proper datetime object, Altair won't be able to create a meaningful time-series plot. It would treat the x-axis as simple strings or numbers.<ul> <li>Uh-Oh Example: Your \"time\" axis might be sorted incorrectly or not show a continuous flow of time.</li> </ul> </li> </ul> </li> </ol> <p>This is Normal and Productive! If your first plot looks messy, is uninformative, or throws errors \u2013 don't worry! This is a completely normal and incredibly valuable part of the data storytelling workflow. These \"Uh-Oh\" moments are not failures; they are discoveries. They tell you:</p> <ul> <li>What data cleaning or transformation is needed (e.g., convert data types, handle outliers, parse dates).</li> <li>How your visualization strategy might need to be refined (e.g., use opacity for overplotting, try log scales, add different encodings, choose a different mark type).</li> </ul> <p>These observations directly feed into the next phase: Iterative Cleaning and Visualization. The \"problems\" you see now will create the task list for data cleaning.</p> <p>This is Normal and Productive! Revisit Your Plan.</p> <p>If your first plot looks messy, is uninformative, or throws errors \u2013 don't worry! This is a completely normal and incredibly valuable part of the data storytelling workflow. These \"Uh-Oh\" moments are not failures; they are discoveries. They tell you:</p> <ul> <li>What data cleaning or transformation is needed (e.g., convert data types, handle outliers, parse dates).</li> <li>How your visualization strategy might need to be refined (e.g., use opacity for overplotting, try log scales, add different encodings, choose a different mark type).</li> </ul> <p>Crucially, this is the point to look back at your Data Visualization Task List from Step 3.</p> <ul> <li>How did reality compare to your initial plan for this specific visualization?</li> <li>Was the mark type appropriate, or does the \"Uh-Oh\" moment suggest a different one?</li> <li>Were your initial encoding ideas effective, or do they need adjustment based on what you see (or don't see) in the plot?</li> <li>What assumptions did you make in your initial plan that the data has now challenged?</li> </ul> <p>These observations directly feed into the next phase: Iterative Cleaning and Visualization. The \"problems\" you see now, and the reflections on your initial plan, will help you create a more informed data cleaning task list and refine your visualization task list (creating a V2).</p> \ud83d\udca1 Activity: Your First Visualization Attempt, Reality Check &amp; Plan Reflection <ol> <li>Choose one visualization task from the list you created at the end of Step 3.</li> <li>Write the Python and Altair code to generate this visualization using the <code>housing_df</code>.</li> <li>Run the code and observe the output (or any errors).</li> <li>Document your observations:<ul> <li>Did the chart generate successfully? If not, what was the error?</li> <li>If it did generate, how does it look? Does it effectively answer the analytical question you had in mind?</li> <li>Note any \"Uh-Oh\" moments:<ul> <li>Is there overplotting?</li> <li>Are outliers skewing the view?</li> <li>Are the scales appropriate?</li> <li>Do the data types seem correct for the encodings used?</li> <li>Does it look how you expected? What surprised you?</li> </ul> </li> </ul> </li> <li>Reflect and Annotate Your Task List:<ul> <li>Go back to your Data Visualization Task List (V1).</li> <li>For the visualization you just attempted, make notes directly on your list. What worked? What didn't? What data issues did you uncover? What potential changes to the visual specification (mark, encodings, transformations) might be needed?</li> </ul> </li> </ol> <p>This reflection is crucial. The issues you spot here, and your thoughts on refining your plan, will become your immediate to-do list for data cleaning and visual refinement in the next phase.</p>"},{"location":"eda/data-storytelling/iterative-execution/","title":"Iterative Cleaning and Visualization","text":"<p>Welcome to the heart of the iterative process! In this phase, we'll address the data issues uncovered during our initial visualization attempts. The key idea here is that data cleaning is often responsive \u2013 driven by the specific needs of your analysis and visualization goals. We clean with a purpose.</p>"},{"location":"eda/data-storytelling/iterative-execution/#step-6-responsive-data-cleaning-task-list","title":"Step 6: Responsive Data Cleaning Task List \ud83e\uddfc\ud83d\udcdd","text":"<p>Your experience in Step 5 \u2013 the errors, the messy charts, the skewed distributions \u2013 is now the direct input for creating a Data Cleaning Task List. Instead of trying to clean everything imaginable in the dataset upfront (which can be inefficient), we'll prioritize tasks that address the problems blocking us from creating clear, insightful visualizations that answer our analytical questions (which we defined in Step 2).</p> <p>Why \"Responsive\" Cleaning? Stay Focused on Your Goals!</p> <ul> <li>Focus and Efficiency: You tackle problems that actually matter for your current goals, rather than spending time on cleaning aspects of the data you may not even use. Refer back to your persona and analytical questions from Step 2. If a data imperfection doesn't hinder your ability to answer those specific questions or communicate effectively to that persona, it might be a lower priority or not necessary to fix at all for this data story.</li> <li>Contextual Understanding: Understanding why a piece of data is problematic (e.g., \"this string needs to be a date for my time-series plot that answers Sarah's question about market trends\") makes the cleaning process more meaningful.</li> <li>Avoiding Unnecessary Work: The goal is not to create a \"perfect\" dataset in isolation, but a dataset that is fit for the purpose of your current data story. This helps avoid scope creep in your cleaning efforts.</li> <li>Iterative Improvement: As you clean and re-visualize, you might uncover deeper issues or realize that your initial cleaning approach wasn't quite right. This iterative loop is key.</li> </ul> <p>Common Cleaning Tasks Triggered by Visualization \"Uh-Ohs\":</p> <p>Based on the kinds of issues we discussed in Step 5, here are typical cleaning tasks you might identify for our housing dataset. We'll use Polars to perform these tasks in the next step.</p> <ol> <li> <p>Handling Missing Values (Nulls):</p> <ul> <li>Observation (Step 5): A chart fails or looks strange because a key variable used in an encoding (<code>x</code>, <code>y</code>, <code>color</code>) has missing values. Or, <code>describe()</code> showed <code>null_count &gt; 0</code> for an important column.</li> <li>Potential Cleaning Tasks:<ul> <li>Imputation: Fill missing values with a sensible placeholder (e.g., mean, median for numerical; mode or a special category like \"Unknown\" for categorical). Example: If <code>bathrooms</code> had a few nulls, we might fill with the median.</li> <li>Removal: If only a very small percentage of rows have missing values in a critical column and imputation is problematic, you might consider removing those rows (use with caution).</li> <li>Flagging: Create a new boolean column indicating if the original value was missing.</li> </ul> </li> <li>Polars Hint: <code>.fill_null()</code>, <code>.drop_nulls()</code>.</li> </ul> </li> <li> <p>Correcting Data Types:</p> <ul> <li>Observation (Step 5): A plot behaves unexpectedly because a numerical variable is treated as a string, or a date variable isn't recognized as such. Your <code>.schema</code> check might have hinted at this.</li> <li>Potential Cleaning Tasks:<ul> <li>Convert to Numerical: Change string representations of numbers to <code>integer</code> or <code>float</code>. Example: If <code>price</code> was accidentally read as a string.</li> <li>Parse Dates/Times: Convert string representations of dates/times into proper datetime objects. This is crucial for time-series analysis or correct sorting.<ul> <li>Example: Our <code>date</code> column (e.g., \"20141013T000000\") definitely needs parsing from string to a datetime type if we want to analyze trends over time.</li> </ul> </li> <li>Convert to Categorical: Ensure variables intended as categorical are treated as such (e.g., <code>zipcode</code> should often be <code>pl.Categorical</code> or <code>pl.Utf8</code>, not a number you'd average).</li> </ul> </li> <li>Polars Hint: <code>.cast()</code>, <code>pl.to_datetime()</code> (often with a <code>format</code> string).</li> </ul> </li> <li> <p>Feature Engineering (Creating New Variables from Existing Ones):</p> <ul> <li>Observation (Step 5): An existing variable isn't quite in the form you need, or a new derived metric could be more insightful.</li> <li>Potential Cleaning Tasks:<ul> <li>Extraction: Pull out parts of a variable. Example: Extract <code>year</code> and <code>month</code> from our parsed <code>date</code> column to analyze seasonality or yearly trends.</li> <li>Calculation: Create new features from existing ones. Example: Calculate <code>price_per_sqft</code> (<code>price</code> / <code>sqft_living</code>) to normalize for size.</li> <li>Binning/Discretization: Convert a continuous numerical variable into a categorical one. Example: Bin <code>yr_built</code> into decades like \"1950-1959\", \"1960-1969\", etc., to see price trends by age category.</li> <li>Creating Flags: Make boolean indicators. Example: Create an <code>is_renovated</code> column (<code>1</code> if <code>yr_renov &gt; 0</code>, else <code>0</code>).</li> </ul> </li> <li>Polars Hint: <code>.with_columns()</code>, string functions (<code>.str.extract()</code>, <code>.str.strptime()</code>), date functions (<code>.dt.year()</code>, <code>.dt.month()</code>), mathematical operations, <code>pl.cut()</code> for binning.</li> </ul> </li> <li> <p>Handling Outliers:</p> <ul> <li>Observation (Step 5): A few extreme values in a numerical column (e.g., <code>price</code>, <code>sqft_living</code>) are \"squishing\" your plot, making it hard to see patterns in the majority of the data.</li> <li>Potential Cleaning Tasks (use with caution and transparency):<ul> <li>Identification: Use statistical methods (e.g., Z-score, IQR) or visualization (box plots) to identify outliers.</li> <li>Capping/Winsorizing: Limit extreme values to a certain percentile (e.g., cap all values above the 99th percentile at the 99th percentile value).</li> <li>Filtering (for visualization): Temporarily exclude extreme outliers when generating a specific plot to get a better view of the bulk of the data (ensure you note this exclusion).</li> <li>Transformation: Apply mathematical transformations like a log transform (<code>pl.log()</code>) to variables like <code>price</code>, which can compress the range and make skewed distributions more symmetrical for visualization.</li> </ul> </li> <li>Polars Hint: <code>.quantile()</code>, filtering expressions, mathematical functions.</li> </ul> </li> <li> <p>String Manipulation / Categorical Variable Cleanup:</p> <ul> <li>Observation (Step 5): Categorical labels are inconsistent (e.g., \"King County\", \"king county\"), have leading/trailing whitespace, or need simplification.</li> <li>Potential Cleaning Tasks:<ul> <li>Standardize case (e.g., all lowercase).</li> <li>Trim whitespace.</li> <li>Replace or map values to create consistent categories.</li> </ul> </li> <li>Polars Hint: <code>.str.to_lowercase()</code>, <code>.str.strip_chars()</code>, <code>.replace()</code>, <code>.map_dict()</code>.</li> </ul> </li> </ol> <p>Creating Your Data Cleaning Task List:</p> <p>Similar to your visualization task list, document the cleaning tasks you identify. This helps you stay organized and track your progress.</p> <p>Example Data Cleaning Task List Entry (with goal alignment in mind):</p> \"Uh-Oh\" / Observation (from Step 5) Data Variable(s) Proposed Cleaning Task Rationale / Why is this needed? (Link to Step 2 Goal) Polars Function(s) (Hint) Priority <code>date</code> column is string (\"20141013T000000\"), cannot plot trend <code>date</code> Parse to datetime objects. Extract <code>year</code> and <code>month</code>. Enable time-series analysis for Sarah's question about price changes over time. <code>pl.to_datetime()</code>, <code>.dt.year()</code> High Scatter plot of <code>price</code> vs <code>sqft_living</code> is a blob. <code>price</code>, <code>sqft_living</code> Try log transformation on <code>price</code> and possibly <code>sqft_living</code>. Reduce skew for better visibility, helping Sarah understand the <code>price</code>/<code>sqft_living</code> relationship. <code>pl.col().log()</code> High <code>yr_renov</code> has <code>0</code> for no renovation. Want a clear flag. <code>yr_renov</code> Create <code>is_renovated</code> (1 if <code>yr_renov</code> &gt; 0, else 0). Easier for Sarah to see if renovations impact value when grouping or coloring. <code>.is_greater()</code>, <code>.cast()</code> Medium <code>bedrooms</code> has some high outliers (e.g. 33). <code>bedrooms</code> Investigate these outliers. For initial viz, filter to &lt; N. Prevent extreme outliers from skewing plots or summary stats for typical house comparisons. <code>.filter()</code> Medium <code>notes_column</code> (hypothetical) has messy text. <code>notes_column</code> No action for now. This column is not related to any of Sarah's current questions or our planned visuals. N/A Low/None \ud83d\udca1 Activity: Draft Your Data Cleaning Task List (Goal-Oriented) <p>Based on your experiences and observations from Step 5 (Your First Visualization Attempt &amp; Reality Check):</p> <ol> <li>Revisit your Step 2 artifacts: Your audience persona and your list of analytical questions.</li> <li>Identify 2-4 specific data cleaning or transformation tasks that are directly necessary to improve your visualizations or enable you to answer your analytical questions more effectively.</li> <li>For each task, note:<ul> <li>The \"Uh-Oh\" moment or observation that triggered it.</li> <li>The data variable(s) involved.</li> <li>A brief description of the proposed cleaning task.</li> <li>How this task aligns with your Step 2 goals or helps your target persona.</li> <li>(Optional, if you know it) Any Polars functions you think might be useful.</li> </ul> </li> <li>Organize these into a list or table similar to the example above, paying attention to priority based on your analytical goals. This focused list will guide your hands-on data manipulation work in the very next step, ensuring you're investing effort where it counts most for your data story.</li> </ol>"},{"location":"eda/data-storytelling/iterative-execution/#step-7-executing-cleaning-and-updating-visualizations-iterative-cycle","title":"Step 7: Executing Cleaning and Updating Visualizations (Iterative Cycle) \ud83d\udd04\u2728","text":"<p>This step is where the \"iterative\" part of the \"Iterative Cleaning and Visualization\" phase truly comes alive. You'll move through a cycle:</p> <ol> <li>Clean: Implement a cleaning task from your list using Polars.</li> <li>Visualize: Re-generate or create a new visualization with the cleaned data using Altair.</li> <li>Assess: Evaluate the impact of the cleaning and the effectiveness of the visual. Does it answer your question better? Is it clearer? Has it revealed new insights or issues?</li> <li>Refine: Update your data cleaning task list (you might identify new cleaning needs) and your data visualization task list (your visual idea might evolve).</li> </ol> <p>You might go through this cycle multiple times for a single visualization or a set of related ones.</p>"},{"location":"eda/data-storytelling/iterative-execution/#1-executing-cleaning-tasks-with-polars","title":"1. Executing Cleaning Tasks with Polars","text":"<p>Let's take some common tasks from the \"Example Data Cleaning Task List\" in Step 6 and see how we might implement them in Polars. You should adapt these to the specific tasks on your list.</p> <p>Example Cleaning Task 1: Parse <code>date</code> and Extract <code>year</code> and <code>month</code></p> <ul> <li>Rationale: Needed for time-series analysis or grouping by year/month. Our <code>date</code> column is a string like \"20141013T000000\".</li> </ul> <pre><code># Assuming housing_df is your Polars DataFrame and  \n# you have imported Polars as pl\n\n# Make sure the 'date' column exists\n# Parse the date string. The \"T000000\" part is effectively ignored by %Y%m%d\n# if it's not included in the format string.\n# Polars' strptime is robust.\n\nhousing_df = housing_df.with_columns(\n    pl.col('date').str.strptime(pl.Date, format=\"%Y%m%dT%H%M%S\").alias('parsed_date')\n)\n\n# Now extract year and month from the new 'parsed_date' column\n\nhousing_df = housing_df.with_columns([\n    pl.col('parsed_date').dt.year().alias('sale_year'),\n    pl.col('parsed_date').dt.month().alias('sale_month')\n])\n\nhousing_df.select(['date', 'parsed_date', 'sale_year', 'sale_month']).head()\n</code></pre> <p>Example Cleaning Task 2: Log Transform <code>price</code> * Rationale: The <code>price</code> distribution is likely skewed, making visualizations hard to interpret. A log transform can help normalize it for visual purposes.</p> <p><pre><code># Make sure the 'price' column exists and is numeric\nhousing_df = housing_df.with_columns(\n    pl.col('price').log().alias('price_log')\n)\n</code></pre> Self-correction during cleaning: If <code>price</code> contains 0 or negative values, <code>log()</code> will produce nulls or errors. You might need to handle those first (e.g., <code>pl.when(pl.col('price') &gt; 0).then(pl.col('price').log()).otherwise(None).alias('price_log')</code>) or filter them out if appropriate for your analysis. This is part of the iterative process!</p> <p>Example Cleaning Task 3: Create <code>price_per_sqft</code></p> <ul> <li>Rationale: A useful metric for comparing value, normalizing for house size.</li> </ul> <pre><code># Ensure 'price' and 'sqft_living' exist and are numeric\nhousing_df = housing_df.with_columns(\n    (pl.col('price') / pl.col('sqft_living')).alias('price_per_sqft')\n)\n# Handle cases where sqft_living might be 0 to avoid division by zero\nhousing_df = housing_df.with_columns(\n    pl.when(pl.col('sqft_living') &gt; 0)\n    .then(pl.col('price') / pl.col('sqft_living'))\n    .otherwise(None) # Or some other appropriate fill value\n    .alias('price_per_sqft')\n)\n</code></pre>"},{"location":"eda/data-storytelling/iterative-execution/#2-vertical-visual-development-with-altair-post-cleaning","title":"2. Vertical Visual Development with Altair (Post-Cleaning)","text":"<p>Once you've performed a relevant cleaning or transformation task, revisit the visualization that prompted it. Now, you can also focus on vertical visual development: building up a single chart layer by layer to enhance its clarity and communicative power.</p> <p>Let's refine our <code>price</code> vs. <code>sqft_living</code> scatter plot from Step 5, assuming we've now created <code>price_log</code>.</p> <pre><code># Assuming housing_df now contains 'price_log' and other cleaned columns\n# and you have imported Altair as alt and disabled max_rows safeguard. \n\n\n# Layer 1: Basic scatter plot with transformed data\nbase_scatter = alt.Chart(housing_df).mark_point(opacity=0.3).encode( # Added opacity for overplotting\n    x=alt.X('sqft_living:Q', title='Living Area (sq ft)'),\n    y=alt.Y('price_log:Q', title='Log of Sale Price (USD)') # Using log-transformed price\n)\n\n# Layer 2: Add tooltips for interactivity\nbase_scatter_with_tooltips = base_scatter.encode(\n    tooltip=[\n        alt.Tooltip('price:Q', title='Price', format='$,.0f'), # Show original price in tooltip\n        'sqft_living:Q',\n        'bedrooms:O', # :O for Ordinal or discrete numeric\n        'bathrooms:Q'\n    ]\n)\n\n# Layer 3: Add a title and adjust properties\nfinal_scatter_plot = base_scatter_with_tooltips.properties(\n    title='Log Price vs. Living Area',\n    width=600,\n    height=400\n).interactive() # Enable panning and zooming\n\n# Layer 4 (Optional): Add a regression line to see the trend\n# Note: Depnding on the software version installed,\n# Polars DataFrames need to be converted to pandas for transform_regression\n# or you'd pre-calculate regression line data.\n# For simplicity in this example, we'll skip adding a live regression line\n# but you could add a pre-calculated one or use a loess line.\n# Example: final_scatter_plot + final_scatter_plot.transform_loess('sqft_living', 'price_log').mark_line(color='red')\n\n\n# Display the chart\nfinal_scatter_plot.show()\nprint(\"Refined scatter plot generated.\")\n</code></pre> <p>Elements of Vertical Development to Consider:</p> <ul> <li>Base Marks and Encodings: Start with your core data mapping.</li> <li>Transformations: Apply transformations directly in Altair (e.g., <code>bin</code>, <code>aggregate</code>, <code>log</code> scales on axes) or use pre-transformed data from Polars.</li> <li>Interactivity: Add <code>tooltip</code>s, make charts <code>.interactive()</code>.</li> <li>Aesthetics &amp; Clarity: Adjust <code>opacity</code> for overplotting, choose clear <code>color</code> schemes, refine <code>size</code> and <code>shape</code> encodings.</li> <li>Labels and Titles: Ensure axes have informative titles (<code>title=</code>), charts have main titles (<code>.properties(title=)</code>), and units are clear (e.g., <code>axis=alt.Axis(format='$,.0f')</code>).</li> <li>Annotations: Add text or line marks to highlight specific points or trends (e.g., <code>mark_text()</code>, <code>mark_rule()</code>).</li> <li>Legends: Customize legend titles and appearance for clarity.</li> </ul>"},{"location":"eda/data-storytelling/iterative-execution/#3-updating-your-data-visualization-task-list-v2-v3","title":"3. Updating Your Data Visualization Task List (V2, V3, ...)","text":"<p>As you clean your data and refine your visuals, your understanding deepens. This often leads to changes in your visualization plan:</p> <ul> <li>Refine Existing Ideas: The scatter plot example above was a refinement. You might update your task list to note \"use log scale for price,\" \"add opacity.\"</li> <li>Add New Ideas: Cleaner data or an insightful plot might spark ideas for new visualizations you hadn't considered. Add them to your list!</li> <li>Discard Unfruitful Ideas: Sometimes, even after cleaning, a planned visualization doesn't yield useful insights or effectively answer your question. It's okay to remove it from your active list (perhaps with a note why).</li> </ul> <p>Your Data Visualization Task List is a dynamic document. Keep it updated to reflect your evolving strategy.</p> \ud83d\udca1 Activity: Clean, Visualize, Assess, Refine! <ol> <li>Select 1-2 tasks from your Data Cleaning Task List (from Step 6).</li> <li>Execute the Cleaning: Write and run the Polars code to perform these cleaning/transformation tasks on your <code>housing_df</code>. Verify the changes (e.g., using <code>.head()</code>, <code>.schema</code>, or plotting a quick histogram of a transformed column).</li> <li>Re-Visualize / Develop Vertically:<ul> <li>Revisit the Altair visualization that was affected by the cleaning you just performed.</li> <li>Re-generate it using the cleaned data.</li> <li>Practice vertical development: Try adding or refining at least two layers to your chart (e.g., improve tooltips, add a title, adjust opacity, change axis formatting, add a color encoding based on another variable).</li> </ul> </li> <li>Assess:<ul> <li>How did the cleaning impact the visual? Is it clearer? More insightful?</li> <li>Does the vertically developed chart better guide attention to the key information?</li> </ul> </li> <li>Refine Your Task Lists:<ul> <li>Update your Data Cleaning Task List: Mark tasks as complete, or add new ones if your latest visualization attempt revealed further data issues.</li> <li>Update your Data Visualization Task List (V2): Note the refinements made to your chart. Did this spark any new visualization ideas? Should any old ones be modified or removed? This cycle is the engine of practical data analysis and storytelling.</li> </ul> </li> </ol> <p>This step provides concrete examples and guides students through the core iterative loop. It emphasizes both the data manipulation (Polars) and visualization refinement (Altair) aspects.</p> <p>Next is Step 8: \"Creating a Cohesive Set of Visuals (The 'Comic Strip'),\" which moves into the \"horizontal\" development of the story. Okay, we've spent considerable time in the iterative loop of cleaning data and refining individual visualizations (vertical development). You should now have a few well-crafted charts that begin to answer your analytical questions.</p> <p>Now, let's zoom out a bit. A single chart, no matter how well designed, rarely tells the whole story. The next step is to weave these individual visual pieces into a coherent narrative. This is where \"horizontal visual development\" comes in.</p>"},{"location":"eda/data-storytelling/narrative-development/","title":"Building the Narrative - Horizontal Development","text":"<p>Before we dive into arranging multiple visuals into a cohesive story, let's take a moment for a quick checkpoint. You've come a long way, and it's good to ensure your \"virtual workspace\" is organized and you're ready for this next stage of narrative construction.</p>"},{"location":"eda/data-storytelling/narrative-development/#checkpoint-am-i-ready-to-build-the-narrative","title":"Checkpoint: Am I Ready to Build the Narrative? \ud83d\udccd","text":"<p>You've journeyed through understanding your data, defining analytical goals, making initial visualization plans, encountering the \"reality checks\" of data issues, and then iteratively cleaning your data and refining individual visualizations. This is a significant amount of work!</p> <p>What Your \"Virtual Workspace\" Might Look Like Now:</p> <p>By this point, you should ideally have:</p> <ol> <li>Cleaned(er) Data: A version of your Polars DataFrame (<code>housing_df</code>) that reflects the cleaning and transformation tasks you deemed necessary in Step 6 and executed in Step 7 (e.g., parsed dates, transformed variables like <code>price_log</code>, newly engineered features like <code>price_per_sqft</code>).</li> <li>Refined Individual Visualizations: Python scripts or notebook cells with Altair code for several individually developed and refined charts. These are the \"panels\" you've been working on, likely improved through the iterative cycle of Step 7.</li> <li>Updated Task Lists:<ul> <li>Your Data Cleaning Task List should show completed tasks.</li> <li>Your Data Visualization Task List is likely at V2 or V3, reflecting the refinements made to your initial ideas, and perhaps even some new visual ideas sparked during exploration.</li> </ul> </li> <li>Clear Analytical Questions: Your guiding analytical questions (from Step 2) for your persona should still be front-of-mind, as these are what your story aims to answer.</li> </ol> <p>Self-Assessment - \"Am I Doing Okay?\"</p> <p>Ask yourself:</p> <ul> <li>\"Do I have 2-4 refined visualizations whose code I understand and that I believe are now reasonably effective at showing a specific aspect of the data (e.g., a distribution, a relationship, a comparison)?\"</li> <li>\"Does the data feeding these visuals now seem appropriate for their purpose, thanks to the cleaning I've done?\"</li> <li>\"Can I look at my refined individual charts and clearly articulate which analytical question (from Step 2) each one helps to answer?\"</li> <li>\"Do I feel the 'Uh-Oh' moments from Step 5 have been largely addressed for these key visuals?\"</li> </ul> <p>If you can answer \"yes\" or \"mostly yes\" to these, you're in great shape to start thinking about how to weave these visuals together into a story. If some areas still feel a bit shaky, that's okay too! This workflow is iterative. You can always revisit earlier steps. For now, proceed with the understanding that your current set of refined visuals will be the building blocks for your narrative.</p> <p>This checkpoint serves to ensure you're building your story on a solid foundation of well-understood, individually refined components. Now, let's explore how to arrange these into a compelling sequence.</p>"},{"location":"eda/data-storytelling/narrative-development/#step-8-creating-a-cohesive-set-of-visuals-the-comic-strip","title":"Step 8: Creating a Cohesive Set of Visuals (The \"Comic Strip\") \ud83d\uddbc\ufe0f\u27a1\ufe0f\ud83d\uddbc\ufe0f\u27a1\ufe0f\ud83d\uddbc\ufe0f","text":"<p>Horizontal visual development is about thoughtfully selecting and sequencing multiple visualizations to guide your audience through your findings, building understanding layer by layer, much like a comic strip tells a story panel by panel.</p> <p>Key Principles of Horizontal Development:</p> <ol> <li>Logical Flow: The sequence of visuals should follow a logical progression. This might mean:<ul> <li>Overview first, then details: Start with a high-level summary (e.g., overall distribution of prices) and then drill down into specifics (e.g., price drivers, comparisons).</li> <li>Problem/Question, then evidence/answer: Pose a question implicitly or explicitly with one visual, then use subsequent visuals to explore and answer it.</li> <li>Building complexity: Start with simpler relationships and introduce more variables or facets gradually.</li> </ul> </li> <li>Complementary Information: Each visual in the sequence should add a new piece of information or a different perspective, building upon the previous ones. Avoid redundancy unless it's for intentional emphasis.</li> <li>Clear Transitions: While we'll discuss narrative text later (Step 9), the visual transition itself should feel natural. The audience should understand why they are seeing the next chart in the sequence.</li> <li>Audience Focus (Revisit Your Persona!): The chosen sequence should directly address the key questions and goals of your audience persona (from Step 2). What order of information would make the most sense to them?</li> <li>Consistent Aesthetics (Where Appropriate): Using consistent color palettes for the same categories across different charts, similar font styles, and a generally unified design language can make the \"story\" feel more professional and easier to follow. However, variations can be used intentionally to highlight changes in focus.</li> </ol> <p>Example: Building a Mini-Story for \"Sarah the Savvy Homebuyer\"</p> <p>Let's imagine Sarah wants to understand the King County housing market, focusing on price, size, and location. Here's a possible sequence of 2-3 visuals (using concepts from our cleaned data):</p> <p>Visual 1: Overall Price Landscape</p> <ul> <li>Chart Type: Histogram of <code>price_log</code> (log-transformed prices).</li> <li>Purpose: Give Sarah an understanding of the general distribution of house prices \u2013 what's typical, the range, and the shape of the market.</li> <li> <p>Key Insight: Most houses fall within a certain price band, but there's a long tail of more expensive properties.</p> <pre><code># Incomplete code listing.\n# skips several data cleaning /tranformation steps.\nprice_histogram = alt.Chart(housing_cleaned_df).mark_bar().encode(\n    x=alt.X('price_log:Q').bin(maxbins=30).title('Sale Price (log USD)'),\n    y=alt.Y('count()').title('Number of Houses'),\n    tooltip=[\n        alt.Tooltip('price', title=\"Price USD\"),\n        alt.Tooltip('count()', title='Number of Houses'), ]\n).properties(\n    title='Distribution of House Prices (Log Scale)',\n    width=500,\n    height=350\n)\n\nbed_colored = price_histogram.encode(\n    color=alt.Color('bed_cat:N', title='Number of Bedrooms')\n).transform_filter(\nalt.FieldOneOfPredicate(field='bed_cat', oneOf=\"1+ 2+ 3+ 4+\".split())\n)\n\nbath_colored = price_histogram.encode(\n    color=alt.Color('bath_cat:N', title='Number of Bathrooms')\n).transform_filter(\nalt.FieldOneOfPredicate(field='bath_cat', oneOf=\"1+ 2+ 3+\".split())\n)\n\ngrade_colored = price_histogram.encode(\n    color=alt.Color('grade_cat:N', title='Construction Grade')\n    ).transform_filter(\nalt.FieldOneOfPredicate(field='grade_cat', oneOf=\"6 7 8 9\".split())\n)\n</code></pre> </li> </ul> Price Distribution: Using Colors and Facets to Communicate More Context <p>Visual 2: Price vs. Key Feature (Living Area)</p> <ul> <li>Chart Type: Scatter plot of <code>price_log</code> vs. <code>sqft_living</code>, perhaps with <code>grade</code> encoded by color.</li> <li>Purpose: Show Sarah how a primary feature (living area) relates to price, and if construction grade plays a role.</li> <li> <p>Key Insight: Generally, as living area increases, price increases. Higher grade houses tend to command higher prices for similar sizes.</p> <pre><code># Incomplete code listing.\n# skips several data cleaning /tranformation steps.\npoints = alt.Chart(housing_cleaned_df).mark_point(opacity=0.4).encode(\n    x=alt.X('sqft_living_log', title='Living Area Sq.Ft (Log Scale)', scale=alt.Scale(zero=False)),\n    y=alt.Y('price_log', title='Sale Price $ (Log Scale)', scale=alt.Scale(zero=False)),\n    tooltip=[\n        alt.Tooltip('price:Q', title='Price', format='$,.0f'),\n        alt.Tooltip('sqft_living:Q', title='SqFt Living'),\n        alt.Tooltip('bedrooms:Q', title='Bedrooms'),\n        alt.Tooltip('bathrooms:Q', title='Bathrooms'),\n        alt.Tooltip('grade:O', title='Grade')\n    ]\n)\n\nline = points.transform_loess(\n        \"sqft_living_log\",\n        \"price_log\").mark_line( color='red')\n\nprice_size = (points + line).encode(\n    color=alt.Color('bath_cat:N', title='Bathrooms'),\n        )\nprice_size.facet(\nalt.Facet(\"grade_cat:N\", title=\"Construction Grade\")\n).transform_filter(\n    alt.FieldOneOfPredicate(field='bed_cat', oneOf=\"1+ 2+ 3+ 4+\".split()),\n    alt.FieldOneOfPredicate(field='bath_cat', oneOf=\"1+ 2+ 3+\".split()),\n    alt.FieldOneOfPredicate(field='grade_cat', oneOf=\"6 7 8 9\".split())\n)\n</code></pre> </li> </ul> Price vs Size Faceted by Construction Grade <p></p> <p>Visual 3: Price Variation by Location (Zip Code)</p> <ul> <li>Chart Type: Bar chart (or box plot) showing the distribution of <code>price_log</code> (or median <code>price</code>) for several key <code>zipcode</code>s. Alternatively, a choropleth map if geo-data for zip codes is available and joined.</li> <li>Purpose: Help Sarah understand how prices vary geographically.</li> <li> <p>Key Insight: There are noticeable price differences across zip codes, with some areas being significantly more expensive than others.</p> <pre><code># Incomplete code listing.\n# skips several data cleaning /tranformation steps.\n# uses GeoPandas DataFrame\nbase = alt.Chart(kczip_cleaned).mark_geoshape(\n# filled=False,\nstrokeWidth=1.5\n)\n\nprice = base.encode(\n    alt.Color('price_log:Q').scale(scheme=\"redblue\").legend(title=\"Median Price\"),\n    tooltip=[\n        alt.Tooltip('price', title='Median Price'),\n        alt.Tooltip('zipcode', title='Zipcode')\n    ]\n).properties(title=\"Price\",)\n\nsqft_liv = base.encode(\n    alt.Color('sqft_living_log:Q').scale(scheme=\"lightmulti\").legend(title=\"Median Sqft Living\"),\n    tooltip=[\n        alt.Tooltip('sqft_living', title='Median Sqft Living'),\n        alt.Tooltip('zipcode', title='Zipcode')\n    ]\n).properties(title=\"Sq.ft. Living\",)\n\nsqft_lot = base.encode(\n    alt.Color('sqft_lot_log:Q').scale(scheme=\"lightgreyred\").legend(title=\"Median Sqft Lot\"),\n    tooltip=[\n        alt.Tooltip('sqft_lot', title='Median Sqft Lot'),\n        alt.Tooltip('zipcode', title='Zipcode')\n    ]\n).properties(title=\"Sq.ft. Lot\",)\n\n(price | sqft_liv | sqft_lot).resolve_scale(\n    color='independent'\n)\n</code></pre> </li> </ul> Price, Size, and Lot across King County Zip Codes Price, Size, and Lot across King County School Districts <p>This sequence tells a story: \"Here's the overall market (Visual 1), here's how a key feature like size influences price (Visual 2), and here's how location further impacts it (Visual 3).\"</p> Arranging Charts with Altair: Concatenation Operators <p>Altair makes it easy to combine multiple charts into a single visualization object using intuitive operators:</p> <ul> <li><code>chart1 | chart2</code>: Horizontal Concatenation (hconcat). Places <code>chart2</code> to the right of <code>chart1</code>. Perfect for a \"comic strip\" sequence.</li> <li><code>chart1 &amp; chart2</code>: Vertical Concatenation (vconcat). Places <code>chart2</code> below <code>chart1</code>. Useful for stacking related views.</li> <li><code>chart1 + chart2</code>: Layering. Places <code>chart2</code> on top of <code>chart1</code>. Both charts must share the same x and y scales. This is primarily for \"vertical development\" within a single conceptual chart (e.g., adding a regression line to a scatter plot), which we touched upon in Step 7, but it's part of the same family of operators.</li> </ul> <p>These operators allow you to build complex, multi-view displays from simpler components.</p> <p>Slicing, Dicing, and Faceting for Richer Narratives:</p> <p>Part of horizontal development can also involve showing different \"slices\" or \"facets\" of your data to provide deeper insights or comparisons.</p> <ul> <li>Filtering: You might show a general trend, then show the same chart filtered for a specific subgroup (e.g., \"Here's <code>price</code> vs. <code>sqft_living</code> for all houses, and now here it is just for <code>waterfront</code> properties\").</li> <li> <p>Faceting (Small Multiples): Instead of multiple separate charts, you can use faceting (e.g., <code>facet</code> or <code>row</code>/<code>column</code> encodings in Altair) to create a grid of charts that show the same relationship broken down by different categories. For example, you could facet the <code>price</code> vs. <code>sqft_living</code> scatter plot by <code>bedrooms</code>. This allows for easy comparison across categories.</p> <pre><code>points = alt.Chart(housing_cleaned_df).mark_point(opacity=0.4).encode(\n    x=alt.X('sqft_living_log', title='Living Area Sq.Ft (Log Scale)', scale=alt.Scale(zero=False)),\n    y=alt.Y('price_log', title='Sale Price $ (Log Scale)', scale=alt.Scale(zero=False)),\n    tooltip=[\n        alt.Tooltip('price:Q', title='Price', format='$,.0f'),\n        alt.Tooltip('sqft_living:Q', title='SqFt Living'),\n        alt.Tooltip('bedrooms:Q', title='Bedrooms'),\n        alt.Tooltip('bathrooms:Q', title='Bathrooms'),\n        alt.Tooltip('grade:O', title='Grade')\n    ]\n)\n\nline = points.transform_loess(\n        \"sqft_living_log\",\n        \"price_log\").mark_line( color='red')\n\nprice_size = (points + line).encode(\n    color=alt.Color('bath_cat:N', title='Bathrooms'),\n        )\n\ndisplay(price_size)\n\n# Faceting by categorical variable representing number of beds\nprice_size.facet(\nalt.Facet(\"bed_cat:N\", title=\"Number of Beds\")\n).transform_filter(\nalt.FieldOneOfPredicate(field='bed_cat', oneOf=\"1+ 2+ 3+ 4+\".split()),\nalt.FieldOneOfPredicate(field='bath_cat', oneOf=\"1+ 2+ 3+\".split())\n)\n</code></pre> </li> </ul> Before Faceting: Price vs. Size Scatter Plot. After Faceting by Beds Category: Price vs. Size Scatter Plot. <p>The goal is to create a journey for your audience, leading them logically from one piece of evidence to the next, ultimately supporting your main message or answering their key questions.</p> \ud83d\udca1 Activity: Outline Your \"Comic Strip\" <ol> <li>Review your analytical questions (Step 2) for your chosen persona and your updated Data Visualization Task List (from Step 7).</li> <li> <p>Select 2-3 refined visualizations from your list that you believe can be sequenced to tell a mini-story or answer a more complex question for   ur persona.</p> </li> <li> <p>Outline this sequence:</p> <ul> <li>For each visual in the sequence:<ul> <li>Briefly describe the chart (e.g., \"Histogram of log prices,\" \"Scatter of price vs. sqft_living colored by grade\").</li> <li>What is the key insight or piece of information this specific visual contributes to the mini-story?</li> <li>How does it connect to the previous visual (if any) and lead to the next?</li> </ul> </li> </ul> </li> <li> <p>(Optional) Think about whether filtering or faceting could enhance part of your mini-story.</p> </li> </ol> <p>You don't need to code these all perfectly right now, but sketch out the narrative flow. This plan will guide the final assembly of your data story.</p>"},{"location":"eda/data-storytelling/narrative-development/#step-9-adding-narrative-text-and-annotations","title":"Step 9: Adding Narrative Text and Annotations \ud83d\udde3\ufe0f\u270d\ufe0f","text":"<p>This step is about weaving your visuals together with words to create a complete and understandable data story. Well-chosen text and carefully placed annotations bridge the gap between what the data shows and what the audience understands.</p> <p>The Role of Narrative Text: Why Words Matter</p> <ul> <li>Contextualization: Text provides background, explains what the audience is looking at, and sets the stage for the insights revealed in the visuals.</li> <li>Explanation of Insights: While a chart might show a pattern, text explicitly calls out the key takeaways, answers the \"so what?\" question, and clarifies complex points.</li> <li>Guidance and Flow: Words act as signposts, guiding the audience's attention from one visual to the next, ensuring a smooth narrative flow.</li> <li>Reinforcement: Key messages can be reinforced by stating them in text alongside their visual representation.</li> <li>Addressing Nuance: Text can address subtleties, limitations of the data, or alternative interpretations that might not be obvious from the visuals alone.</li> </ul> <p>Types of Narrative Text:</p> <ol> <li> <p>Overall Story Title: A compelling title for your entire data story that grabs attention and summarizes the main theme.</p> <ul> <li>Example: \"Navigating King County's Housing Market: Insights for First-Time Buyers\"</li> </ul> </li> <li> <p>Chart Titles: Every chart needs a clear, descriptive title that immediately tells the audience what the chart is about.</p> <ul> <li>Bad Title: \"Price Data\"</li> <li>Good Title (for Visual 1 in our Step 8 example): \"Most Homes Cluster in Mid-Range Prices, With a Tail of Luxury Properties\" or \"Distribution of King County House Prices (Log Scale)\"</li> </ul> </li> <li> <p>Captions or Introductory/Explanatory Text: Short paragraphs or sentences accompanying each visual or a sequence of visuals.</p> <ul> <li>What to include:<ul> <li>Briefly state the main insight the visual is intended to convey.</li> <li>Explain any important features or encodings if they aren't immediately obvious.</li> <li>Point out what the audience should be looking for.</li> </ul> </li> <li>Example (for Visual 2: Price vs. Living Area by Grade):     \"As expected, larger homes generally command higher prices. The color-coding by construction grade further reveals that higher-grade homes (shown in brighter yellows) tend to be priced above lower-grade homes of similar size, indicating a premium for quality.\"</li> </ul> </li> <li> <p>Transitional Text: Words or phrases that create a smooth link from one visual or point to the next.</p> <ul> <li>Examples: \"Having seen the overall price landscape, let's now explore what drives these prices...\"     \"Beyond size and grade, location also plays a significant role...\"     \"This leads us to another important factor: ...\"</li> </ul> </li> </ol> <p>Annotations: Highlighting Directly on Your Charts</p> <p>Annotations are textual or graphical elements placed directly onto a chart to highlight specific data points, trends, or areas of interest. They are powerful tools for guiding audience attention precisely where you want it.</p> <ul> <li> <p>Why Annotate?</p> <ul> <li>Draw attention to key outliers, specific values, or important thresholds.</li> <li>Explain an anomaly or a significant event within the data.</li> <li>Add context directly where it's needed.</li> </ul> </li> <li> <p>Simple Altair Annotation Examples:     Altair allows for annotations, often by layering additional marks like <code>mark_text()</code> or <code>mark_rule()</code>.</p> <ul> <li> <p>Highlighting a specific point with <code>mark_text()</code>:     Imagine you wanted to label a particularly interesting house in your <code>price_log</code> vs. <code>sqft_living</code> scatter plot. You might create a small DataFrame with the details of that point and layer a <code>mark_text</code> chart.</p> <pre><code># (Conceptual - assume 'final_scatter_plot' from Step 7 and housing_df exists)\n# highlight_data = pl.DataFrame({\n#     'sqft_living': [3000],  # Sqft of the house to highlight\n#     'price_log': [housing_df.filter(pl.col('sqft_living') == 3000)['price_log'].mean()], # Example Y\n#     'text': ['Example High Value Home']\n# })\n#\n# annotation_text = alt.Chart(highlight_data).mark_text(\n#     align='left',\n#     baseline='middle',\n#     dx=7  # Offset text slightly to the right\n# ).encode(\n#     x='sqft_living:Q',\n#     y='price_log:Q',\n#     text='text:N'\n# )\n#\n# annotated_scatter_plot = final_scatter_plot + annotation_text\n# annotated_scatter_plot.show()\n</code></pre> </li> <li> <p>Adding a reference line with <code>mark_rule()</code>:     If you wanted to show the median <code>price_log</code> on your histogram.</p> <p><pre><code># (Conceptual - assume 'price_histogram' and housing_df with 'price_log' exists)\n# median_price_log = housing_df['price_log'].median()\n#\n# median_rule = alt.Chart(pl.DataFrame({'median_val': [median_price_log]})).mark_rule(color='red', strokeDash=[3,3]).encode(\n#     y='median_val:Q'\n# )\n#\n# histogram_with_median_line = price_histogram + median_rule # Layering the rule\n# histogram_with_median_line.show()\n</code></pre> Keep annotations sparse and purposeful. Too many can clutter the visual.</p> </li> </ul> </li> </ul> <p>Key Considerations for Text and Annotations:</p> <ul> <li>Audience and Purpose (Again!): Use language your persona understands. Focus on what's relevant to their goals and your analytical questions (Step 2).</li> <li>Conciseness: Be brief and to the point. Avoid jargon where possible. Every word should add value.</li> <li>Clarity: Ensure your explanations are unambiguous.</li> <li>Integration: Text and visuals should work together seamlessly. The text should refer to what's in the visual, and the visual should support the text.</li> <li>Placement: Position text and annotations thoughtfully so they are easy to read and don't obscure important data.</li> </ul> \ud83d\udca1 Activity: Draft Your Narrative and Annotations <ol> <li>Take the \"Comic Strip\" outline (sequence of 2-3 visuals) you created in Step 8.</li> <li>For your sequence, draft the following narrative text:<ul> <li>A compelling overall title for this mini-story.</li> <li>A clear, descriptive title for each visual in your sequence.</li> <li>A short caption or introductory sentence for each visual, explaining its main takeaway or what the audience should focus on.</li> <li>Brief transitional phrases or sentences to link one visual to the next.</li> </ul> </li> <li>Identify Annotation Opportunities:<ul> <li>For at least one of your visuals, identify 1-2 specific annotations that would help guide attention or clarify an important point (e.g., highlighting an outlier, marking an average, labeling a specific segment).</li> <li>Briefly describe what the annotation would say or show. You don't need to code it in Altair for this activity unless you're comfortable doing so, but think about its purpose and placement.</li> </ul> </li> </ol> <p>Write this down. This brings you one step closer to a fully formed data story!</p>"},{"location":"eda/data-storytelling/review-validation/","title":"Review and Validation","text":"<p>You've put in the hard work of planning, exploring, cleaning, visualizing, and narrating. This final phase is about ensuring your efforts culminate in a data story that is clear, accurate, and impactful for your intended audience.</p>"},{"location":"eda/data-storytelling/review-validation/#step-10-checking-against-goals","title":"Step 10: Checking Against Goals \u2705\ud83c\udfaf","text":"<p>The primary purpose of this step is to objectively assess your data story against the initial objectives and analytical questions you defined way back in Step 2. It's easy to get caught up in the technical details of data wrangling and chart creation, so this is your chance to ensure the final product aligns with your original intent.</p> <p>Key Questions for Your Review:</p> <p>Ask yourself these questions as you review your sequence of visuals (from Step 8) and the accompanying narrative text and annotations (from Step 9):</p> <ol> <li> <p>Answers the Analytical Questions?</p> <ul> <li>Go back to the specific analytical questions you drafted in Step 2.</li> <li>Does your data story, as a whole, directly and clearly answer each of these questions?</li> <li>Is the evidence provided in your visuals and text sufficient to support these answers?</li> </ul> </li> <li> <p>Meets Audience (Persona) Needs?</p> <ul> <li>Revisit the audience persona you created in Step 2.</li> <li>Is the story tailored to their level of understanding, their interests, and their goals?</li> <li>Will they find it relevant and valuable?</li> <li>Is the language used appropriate for them?</li> <li>Are the visuals clear and accessible from their perspective?</li> </ul> </li> <li> <p>Clear Key Message?</p> <ul> <li>What is the single most important message or takeaway you want your audience to get from this data story?</li> <li>Is this message prominent and easy to understand?</li> <li>Do all parts of your story (visuals, text, sequence) work together to support this key message?</li> </ul> </li> <li> <p>Logical and Coherent Narrative?</p> <ul> <li>Does the story flow logically from one point to the next?</li> <li>Are the transitions between visuals and ideas smooth and easy to follow?</li> <li>Is the \"comic strip\" (your sequence of visuals) telling a coherent tale, or does it feel disjointed?</li> </ul> </li> <li> <p>Supported by Data?</p> <ul> <li>Are all claims and interpretations accurately supported by the data presented in your visuals?</li> <li>Have you avoided overstating your findings or drawing conclusions that go beyond what the data can support?</li> <li>Are your visualizations an honest representation of the data?</li> </ul> </li> <li> <p>Clarity and Conciseness?</p> <ul> <li>Is the story free of jargon that your audience might not understand (unless it's explicitly defined)?</li> <li>Is all the text necessary, or can some be trimmed for brevity without losing meaning?</li> <li>Are the visuals uncluttered and easy to interpret? (This relates back to effective vertical development in Step 7).</li> </ul> </li> </ol> <p>The Goal: Honesty, Constructive Self-Criticism, and the MVP Mindset</p> <p>Be honest with yourself during this review. It's better to identify areas for improvement now than for your audience to be confused or unconvinced later. This step might lead you to identify several potential refinements.</p> <p>However, it's also important to adopt a Minimum Viable Product (MVP) mindset, or in our case, a \"Minimum Viable Story.\" Your primary goal is to ensure your data story effectively answers the core analytical questions for your target audience and delivers the key intended message.</p> <ul> <li>Focus on Core Effectiveness First: Does your story, in its current state, achieve its main purpose? Is it clear, accurate, and does it address the crucial questions? This is your MVP.</li> <li>Prioritize Refinements: When you identify areas for improvement:<ul> <li>Distinguish between \"must-have\" changes (those essential for clarity, accuracy, or answering the core questions) and \"nice-to-have\" refinements (e.g., more advanced visual embellishments, deeper dives into secondary questions, stylistic polishing that doesn't alter the core message).</li> <li>Address the \"must-haves\" to ensure your MVP is solid.</li> <li>List the \"nice-to-haves\" as potential future iterations. In a professional setting, you often deliver the MVP and then iterate based on feedback or available time.</li> </ul> </li> </ul> <p>This approach helps you manage your time effectively and ensures you deliver value even if deadlines are tight. Your review might lead you to:</p> <ul> <li>Refine your narrative text (Must-have if unclear): Clarify explanations, strengthen transitions, or sharpen your key messages to ensure the MVP is effective.</li> <li>Tweak your visuals (Must-have if misleading or confusing): Make small adjustments to titles, labels, colors, or annotations for better clarity.</li> <li>Re-order visuals (Must-have if flow is illogical): If the flow is a barrier to understanding the core message, adjust it.</li> <li>Add or remove a visual (Consider for MVP): If a visual is critical to answering a core question, it's part of the MVP. If it's tangential, it might be a \"nice-to-have.\"</li> <li>(In rare cases) Revisit earlier steps: If a major flaw undermines the MVP (e.g., a critical data error affecting your main conclusion), then addressing it is essential.</li> </ul> <p>This review ensures your data story doesn't just present data, but effectively communicates the intended insights to the intended audience, achieving the purpose you set out to fulfill, starting with a strong, viable core message.</p> \ud83d\udca1 Activity: Review Your Data Story Draft (MVP Focus) <p>Take your assembled \"mini-story\" \u2013 the sequence of 2-3 visuals you outlined in Step 8, along with the narrative text and annotation ideas you drafted in Step 9.</p> <ol> <li>Re-read your analytical questions and persona description from Step 2. Keep them clearly in mind.</li> <li>Critically review your mini-story using the \"Key Questions for Your Review\" listed above.</li> <li>Jot down your honest answers and observations.<ul> <li>What works well?</li> <li>Where are there potential weaknesses or areas for improvement?</li> </ul> </li> <li>Prioritize with an MVP Mindset:<ul> <li>Identify the \"must-have\" changes needed for your story to clearly and accurately answer your core analytical questions for your persona (this forms your \"Minimum Viable Story\").</li> <li>List any \"nice-to-have\" refinements that would enhance the story further but are not critical to its core message.</li> </ul> </li> <li>Based on your review, list 1-2 specific, actionable \"must-have\" refinements you would make to ensure your mini-story is a solid MVP.</li> </ol> <p>This structured self-review, with an eye towards delivering a viable core story, is a vital skill for any data professional.</p>"},{"location":"eda/data-storytelling/review-validation/#step-11-final-polish-and-presentation-considerations","title":"Step 11: Final Polish and Presentation Considerations \u2728","text":"<p>This step is about taking that last look to ensure your data story is presented in the most clear, professional, and effective manner possible. Think of it as the final proofread and visual tidy-up. While major content issues should have been addressed in Step 10, this is where you catch the small things that can make a big difference to the audience's experience.</p> <p>Key Areas for Final Polish:</p> <ol> <li> <p>Simplicity and Clarity (Revisit \"Chart Junk\"):</p> <ul> <li>Edward Tufte's principle of maximizing the \"data-ink ratio\" is relevant here: ensure every element on your visuals (ink) is there to convey data information.</li> <li>Remove any unnecessary gridlines, borders, backgrounds, or decorative elements that don't add to understanding (often called \"chart junk\").</li> <li>Is every visual as simple and direct as it can be while still conveying the necessary information?</li> </ul> </li> <li> <p>Consistency:</p> <ul> <li>Formatting: Are fonts, font sizes, and colors used consistently across your visuals and text?</li> <li>Terminology: Are you using the same terms for the same concepts throughout your story?</li> <li>Visual Style: Do your charts have a reasonably consistent look and feel (unless variations are intentional for emphasis)? This creates a more professional and cohesive experience.</li> </ul> </li> <li> <p>Accuracy (Final Double-Check):</p> <ul> <li>Numbers and Labels: Quickly verify that all numbers displayed in your charts, tables, or text are accurate. Check axis labels, tick mark values, and any specific data points you've highlighted.</li> <li>Interpretations: One last read-through: do your textual interpretations still accurately reflect what the visuals show?</li> </ul> </li> <li> <p>Proofreading for Typos and Grammar:</p> <ul> <li>Carefully read all titles, captions, narrative text, and annotations for any spelling errors, grammatical mistakes, or awkward phrasing.</li> <li>It can be helpful to read it aloud or ask a colleague to glance over it, as fresh eyes often catch things you might miss.</li> </ul> </li> </ol> <p>Presentation Medium Considerations (e.g., MkDocs):</p> <p>How and where your audience will consume your data story can influence some final presentation choices. Since you're preparing content for an MkDocs course site:</p> <ul> <li>Layout: How will your visuals and text be arranged on the page? Will charts be displayed side-by-side (as you might have planned with concatenation in Step 8), or one after another with text in between? Ensure the flow is logical on the webpage.</li> <li>Interactivity: Altair charts can be interactive (tooltips, zooming, panning). Ensure these features work as intended when embedded. If you're saving charts as static images (e.g., PNGs) for MkDocs, make sure all necessary information is visible without interaction (e.g., clear labels instead of relying solely on tooltips).</li> <li>Responsiveness: If your audience might view the content on different screen sizes, consider how your visuals will scale or adapt (though this is a more advanced topic).</li> <li>Accessibility: Consider aspects like color contrast and clear, resizable fonts for broader accessibility (another advanced, but important, topic for future learning).</li> </ul> <p>The primary goal here is to ensure that nothing in the final presentation distracts from or undermines the clarity and credibility of your data story.</p> \ud83d\udca1 Activity: Your Final Polish Checklist <p>Before you consider your \"mini-story\" ready for sharing (even hypothetically), run through this quick final checklist:</p> <ol> <li>Simplicity: Is there anything I can remove from my charts to make them simpler without losing information?</li> <li>Consistency: Are my titles, labels, and color schemes reasonably consistent?</li> <li>Accuracy: Have I double-checked key numbers and labels in my visuals and text?</li> <li>Proofreading: Have I read all text for typos or grammatical errors?</li> <li>Flow (for MkDocs): If I were to put this on a webpage, does the sequence of text and visuals make sense?</li> </ol> <p>Make any quick fixes identified. This is about ensuring a professional finish.</p> <p>And with that final polish, you've completed the core data storytelling workflow for your mini-story! You've taken raw data, formulated questions, explored, cleaned, visualized, built a narrative, and reviewed your work.</p>"},{"location":"eda/data-storytelling/storytelling-intro/","title":"What is Data Storytelling and Why a Workflow?","text":"<p>Welcome to the Data Storytelling Workflow! In your journey as data professionals, you've already started acquiring powerful skills in Python, data wrangling with Polars, and creating expressive visualizations with Altair. Now, it's time to weave these skills together into a cohesive process that transforms raw data into compelling narratives that drive understanding and action.</p>"},{"location":"eda/data-storytelling/storytelling-intro/#the-goal-beyond-the-chart-communicating-insights-and-guiding-attention","title":"The Goal: Beyond the Chart - Communicating Insights and Guiding Attention","text":"<p>We've all seen charts. Some are clear and insightful; others are confusing, overwhelming, or simply miss the mark. The difference often lies not just in the technical execution of the chart, but in the story it tells and, critically, how it guides the audience's attention to the core message.</p> <ul> <li>Data Storytelling is the art and science of:<ul> <li>Identifying significant insights and patterns within data.</li> <li>Crafting a narrative around these insights.</li> <li>Designing visualizations that intentionally direct the viewer's focus to key elements, making complex information accessible.</li> <li>Communicating this narrative effectively to a specific audience using data visualizations and explanatory text.</li> </ul> </li> </ul> <p>It's not just about presenting data; it's about conveying meaning and ensuring your audience sees what you need them to see. Think of yourself not just as an analyst, but as a guide, leading your audience through the data\u2014and the visual\u2014to a clear understanding or a well-supported conclusion.</p>"},{"location":"eda/data-storytelling/storytelling-intro/#the-challenge-bridging-data-analysis-and-audience-understanding","title":"The Challenge: Bridging Data, Analysis, and Audience Understanding","text":"<p>As working professionals, you know that data in its raw form is rarely self-explanatory. Several challenges stand between a dataset and a clear, actionable insight for your stakeholders:</p> <ul> <li>Information Overload: Datasets can be vast and complex. How do you decide what's important and what's just noise? How do you prevent your audience from getting lost?</li> <li>Misdirected Focus: Even with a visually appealing chart, an audience might focus on less important aspects or miss the main point if their attention isn't properly guided through design choices (like color, size, annotations).</li> <li>The \"So What?\" Question: You might find a statistically significant pattern, but can you explain why it matters to your audience and make them see its significance quickly?</li> <li>Audience Gap: Your audience (e.g., managers, clients, colleagues from other departments) may not have the same technical background or familiarity with the data as you do. How do you bridge this gap and ensure they interpret the visuals correctly?</li> <li>Unclear Objectives: Without a clear understanding of what you're trying to achieve or what questions you're trying to answer, your analysis can become unfocused and your visualizations aimless, failing to direct attention effectively.</li> </ul> <p>Simply producing a technically correct chart or a table of numbers often isn't enough to overcome these challenges. You need to design for understanding and attention.</p>"},{"location":"eda/data-storytelling/storytelling-intro/#the-solution-a-structured-iterative-workflow","title":"The Solution: A Structured, Iterative Workflow","text":"<p>To navigate these challenges effectively, we need a systematic approach: a Data Storytelling Workflow. This workflow provides a roadmap from the initial exploration of data to the final communication of insights, with a constant eye on how to best guide your audience.</p> <p>Why is a workflow so important?</p> <ul> <li>Clarity &amp; Focus: It forces you to define your objectives, understand your audience, and articulate the key messages you want to convey before you get lost in the weeds of data manipulation and chart generation. This clarity helps in deciding where to direct audience attention.</li> <li>Efficiency: A structured process helps you make deliberate choices about both analysis and visual design, reducing wasted effort on analyses or visualizations that don't serve your core purpose or effectively guide the eye.</li> <li>Stakeholder Alignment: By involving stakeholders (or at least considering their perspective) early in defining questions and goals, you ensure your final story is relevant, impactful, and visually intuitive for them.</li> <li>Avoiding \"Data Dumps\": A workflow guides you towards a curated narrative, preventing the common pitfall of overwhelming your audience. It's about strategic reveals and visuals that highlight, not hide, the key takeaways.</li> <li>Iterative Improvement: Good data storytelling is rarely a linear process. A workflow embraces iteration\u2014allowing you to revisit assumptions, refine questions, and improve visualizations (including how they direct attention) as you learn more from the data.</li> <li>Project Management: Explicitly defining tasks helps manage scope, track progress, and ensure that your efforts are directed towards achieving the desired outcome \u2013 crucial in a professional setting.</li> </ul>"},{"location":"eda/data-storytelling/storytelling-intro/#lesson-roadmap-our-journey-today","title":"Lesson Roadmap: Our Journey Today","text":"<p>In this lesson, we'll walk through a simulated data storytelling workflow using a real-world dataset. We'll cover these key phases, always considering how our choices impact audience understanding and attention:</p> <ol> <li>Understanding the Landscape &amp; Defining Goals: Starting with the data dictionary, articulating analytical questions, and applying the grammar of graphics to outline initial visualizations that will effectively focus attention.</li> <li>Data Ingestion and Initial Exploration: Loading data with Polars and taking a first look, often leading to new questions about what to highlight.</li> <li>Iterative Cleaning and Visualization: How visualization attempts inform data cleaning needs, and how we build charts \"vertically\" by layering information to guide the viewer's eye.</li> <li>Building the Narrative - Horizontal Development: Creating a cohesive sequence of visuals that tell a story, like a \"comic strip,\" ensuring smooth transitions in focus.</li> <li>Review and Validation: Ensuring our story effectively answers our questions, meets our goals, and clearly directs attention to the intended insights.</li> </ol> <p>By the end of this lesson, you'll have a practical framework you can adapt for your own data storytelling projects, enabling you to more effectively turn data into actionable insights that capture and hold your audience's attention.</p>"},{"location":"eda/data-visualization/","title":"Data Visualization with Altair","text":"<ul> <li> Starter Colab Notebook</li> <li> Introduction to Data Visualization </li> <li> Your First Plots with Altair </li> <li> Enhancing Your Visualizations </li> <li> Interactive Visualization </li> <li> Data Transformation </li> <li> Best Practices &amp; Data Storytelling Workflow </li> <li> Wrap-up &amp; Next Steps </li> </ul>"},{"location":"eda/data-visualization/best-practices/","title":"Best Practices & Data Storytelling Workflow","text":"<p>Creating visualizations is both an art and a science. Beyond knowing the syntax of a library like Altair, effective data visualization involves critical thinking, understanding your audience, and following established best practices to tell a clear and compelling story with your data.</p>"},{"location":"eda/data-visualization/best-practices/#choosing-the-right-chart-matching-visuals-to-data-and-questions","title":"Choosing the Right Chart: Matching Visuals to Data and Questions","text":"<p>The type of chart you choose should primarily be driven by:</p> <ol> <li>The question you are trying to answer or the insight you want to convey.</li> <li>The type of data you are working with (quantitative, categorical, temporal, etc.).</li> </ol> <p>Here's a quick recap of common chart types and their typical uses:</p> <ul> <li>Scatter Plot:<ul> <li>Purpose: To show the relationship between two quantitative variables. Ideal for identifying correlations, clusters, and outliers.</li> <li>Data: Two quantitative variables. Can incorporate a third categorical variable for color/shape encoding.</li> </ul> </li> <li>Bar Chart:<ul> <li>Purpose: To compare a quantitative measure across different discrete categories, or to show frequencies.</li> <li>Data: One categorical variable (for the bars) and one quantitative variable (for the length of the bars), or just a categorical variable for counts.</li> </ul> </li> <li>Line Chart:<ul> <li>Purpose: To display trends of a quantitative variable over a continuous interval or sequence, most commonly time.</li> <li>Data: One quantitative variable and one ordered variable (often temporal, but can be any ordered sequence). Can use color to distinguish multiple series.</li> </ul> </li> <li>Histogram:<ul> <li>Purpose: To visualize the distribution of a single quantitative variable, showing the frequency of values within specific bins.</li> <li>Data: One quantitative variable.</li> </ul> </li> <li>Box Plot:<ul> <li>Purpose: To summarize the distribution of a quantitative variable, highlighting median, quartiles, and potential outliers. Useful for comparing distributions across categories.</li> <li>Data: One quantitative variable, often grouped by a categorical variable.</li> </ul> </li> </ul> <p>Key Question: Before making a chart, ask yourself: \"What do I want my audience to learn from this?\"</p>"},{"location":"eda/data-visualization/best-practices/#clarity-and-simplicity-less-is-more","title":"Clarity and Simplicity: \"Less is More\"","text":"<p>The primary goal of a visualization is to communicate information clearly and efficiently.</p> <ul> <li>Avoid Chart Junk: Coined by Edward Tufte, \"chart junk\" refers to unnecessary visual elements that don't add information (e.g., excessive gridlines, distracting background colors, 3D effects on 2D data). Strive for a high data-ink ratio.</li> <li>Clear Labeling: Ensure titles, axis labels, and legends are descriptive and unambiguous. Units should be clearly indicated.</li> <li>Logical Ordering: For categorical data, consider ordering bars or segments meaningfully (e.g., by size, alphabetically if no other logic applies).</li> <li>Appropriate Scales: Ensure your axes start at a meaningful point (often zero for bar charts showing magnitude, but not always for line or scatter plots showing trends/relationships). Avoid distorting the data.</li> <li>Strategic Use of Color: Use color purposefully to highlight, distinguish categories, or show intensity. Avoid using too many colors, which can be overwhelming. Be mindful of color-blindness (use color-blind friendly palettes where possible).</li> </ul>"},{"location":"eda/data-visualization/best-practices/#audience-awareness-who-are-you-talking-to","title":"Audience Awareness: Who Are You Talking To?","text":"<p>Tailor your visualizations to your audience's level of expertise and their information needs.</p> <ul> <li>Executive Summary vs. Technical Deep Dive: An executive might need a high-level summary chart, while an analyst might need a more detailed, interactive visualization.</li> <li>Familiarity with Data: If your audience is unfamiliar with the data, you may need to provide more context or simpler charts.</li> <li>Key Message: Ensure your visualization clearly delivers the key message relevant to that specific audience.</li> </ul>"},{"location":"eda/data-visualization/best-practices/#iteration-and-experimentation-the-path-to-insight","title":"Iteration and Experimentation: The Path to Insight","text":"<p>Your first attempt at a visualization is rarely your best or final one.</p> <ul> <li>Embrace Iteration: Start with a basic chart and progressively refine it. Try different chart types, encodings, and aggregations.</li> <li>Experiment: Don't be afraid to try unconventional approaches if they might reveal new insights (while still adhering to principles of clarity).</li> <li>Seek Feedback: If possible, get feedback from peers or your intended audience. Does the chart make sense to them? Does it answer their questions?</li> </ul> <p>This iterative process is a core part of Exploratory Data Analysis (EDA).</p>"},{"location":"eda/data-visualization/best-practices/#evaluating-good-enough-for-presentation","title":"Evaluating \"Good Enough\" for Presentation","text":"<p>When is your visualization ready for your audience? Consider these points:</p> <ul> <li>Accuracy: Does it correctly represent the underlying data?</li> <li>Clarity: Is the message immediately understandable? Can the audience quickly grasp the main takeaway?</li> <li>Completeness: Does it provide sufficient context without being overwhelming?</li> <li>Honesty &amp; Ethics:<ul> <li>Does it avoid misleading interpretations? (e.g., truncated y-axis that exaggerates differences, inappropriate scales, cherry-picking data).</li> <li>Is it transparent about data sources and potential limitations?</li> <li>A brief discussion on how visualizations can be used to mislead reinforces the importance of ethical data representation.</li> </ul> </li> <li>Purpose-Driven: Does it effectively answer the question it set out to address or support the point it aims to make?</li> </ul>"},{"location":"eda/data-visualization/best-practices/#visualization-in-the-broader-data-analysis-workflow","title":"Visualization in the Broader Data Analysis Workflow","text":"<p>Visualization is not an isolated step; it's integrated throughout the data analysis lifecycle:</p> <ol> <li>Data Acquisition &amp; Cleaning: Visualizing data can help identify errors, outliers, and missing values during the cleaning phase.</li> <li>Exploratory Data Analysis (EDA): This is where visualization shines \u2013 uncovering patterns, testing assumptions, generating hypotheses.</li> <li>Model Building (Machine Learning): Visualizing feature distributions, relationships, model performance metrics (e.g., residuals, confusion matrices), and results.</li> <li>Communication &amp; Storytelling: Presenting findings, insights, and recommendations to stakeholders using clear and impactful visualizations.</li> </ol>"},{"location":"eda/data-visualization/best-practices/#strategic-use-of-resources-for-continuous-learning","title":"Strategic Use of Resources for Continuous Learning","text":"<p>The field of data visualization is vast. To continue your learning and problem-solving journey:</p> <ul> <li>Leverage Library Documentation: The Altair documentation website is comprehensive and full of examples. Similarly for Polars and scikit-learn.</li> <li>Online Resources: Blogs, tutorials, and forums (like Stack Overflow) are invaluable for specific questions and new techniques.</li> <li>Generative AI Tools: Tools like ChatGPT can be strategic aids for generating code snippets, explaining concepts, or brainstorming approaches, but always critically evaluate their output.</li> <li>Practice: The more you practice creating visualizations with different datasets and for different purposes, the more your skills and intuition will develop.</li> <li>Translate Problems to Steps: A key skill is translating loosely defined business problems into concrete data analysis steps, including which visualizations might be appropriate.</li> </ul> <p>By combining technical skills with these best practices, you can create visualizations that not only look good but also drive understanding and effective decision-making. ```</p>"},{"location":"eda/data-visualization/data-transformations/","title":"Data Transformations for Visualization","text":"<p>Often, your raw data isn't in the perfect shape or format for the visualization you have in mind. You might need to filter it, create new categories, or calculate summary statistics before you can effectively plot it. Both Polars (your primary data manipulation tool) and Altair offer ways to perform these transformations.</p>"},{"location":"eda/data-visualization/data-transformations/#why-transform-data-for-visualization","title":"Why Transform Data for Visualization?","text":"<p>Common reasons to transform data before or during visualization include:</p> <ul> <li>Filtering: Focusing on a specific subset of your data (e.g., data for a particular year, region, or category).</li> <li>Deriving New Features (Feature Engineering): Creating new columns from existing ones that are more suitable for plotting (e.g., extracting the year from a date, creating bins for a continuous variable, calculating ratios).</li> <li>Aggregation: Summarizing data to a higher level (e.g., calculating averages, counts, or sums per category) before plotting. This is essential for bar charts showing averages, summary line charts, etc.</li> <li>Reshaping Data (Pivoting/Melting): Sometimes your data's structure (wide vs. long format) needs to be changed to suit certain plot types, though this is a more advanced topic we'll touch upon lightly if needed.</li> </ul>"},{"location":"eda/data-visualization/data-transformations/#primary-approach-polars-for-pre-processing","title":"Primary Approach: Polars for Pre-processing","text":"<p>For this course, we emphasize using Polars for most of your data pre-processing needs before you even pass the data to Altair. You've already started learning Polars for high-performance data manipulation.</p> <p>Advantages of using Polars for pre-transformation:</p> <ul> <li>Power and Flexibility: Polars offers a rich and highly optimized API for a wide range of complex data manipulations.</li> <li>Clarity of Workflow: Separating data manipulation steps (in Polars) from visualization steps (in Altair) can make your overall workflow clearer and easier to debug.</li> <li>Performance: Polars is designed for speed, especially with larger datasets. Performing transformations efficiently upfront can be beneficial.</li> <li>Reusability: The transformed Polars DataFrame can be used for multiple different visualizations or other analytical tasks.</li> </ul> <p>Common Polars operations before plotting:</p> <ol> <li>Filtering Rows:     <pre><code># Assuming 'data_pl' is a Polars DataFrame\nfiltered_data = data_pl.filter(pl.col('Year') == 2023)\nfiltered_data = data_pl.filter(pl.col('Sales') &gt; 1000)\n</code></pre></li> <li>Creating New Columns (Feature Engineering):     <pre><code># Extracting year from a date column\ndata_with_year = data_pl.with_columns(\n    pl.col('Order_Date').dt.year().alias('Order_Year')\n)\n# Creating a new category based on a condition\ndata_with_category = data_pl.with_columns(\n    pl.when(pl.col('Value') &gt; 50)\n    .then(pl.lit('High'))\n    .otherwise(pl.lit('Low'))\n    .alias('Value_Category')\n)\n</code></pre></li> <li>Aggregating Data (<code>group_by().agg()</code>): This is very common for creating summary charts.     <pre><code>summary_data = data_pl.group_by('Product_Category').agg(\n    pl.mean('Sales').alias('Average_Sales'),\n    pl.sum('Quantity').alias('Total_Quantity')\n)\n</code></pre>     The resulting <code>summary_data</code> DataFrame is then passed to <code>alt.Chart()</code>.</li> </ol>"},{"location":"eda/data-visualization/data-transformations/#altairs-built-in-transformations","title":"Altair's Built-in Transformations","text":"<p>Altair also provides its own set of transformations that can be applied directly within the charting pipeline. These are convenient for simpler, on-the-fly adjustments.</p> <ul> <li> <p><code>transform_calculate</code>: Create new fields (columns) based on an expression.     <pre><code># chart.transform_calculate(\n#    new_field_name = \"datum.existing_field * 2\" # Vega expression syntax\n# )\n</code></pre>     The expression uses Vega expression syntax, which can feel different from Python/Polars syntax.</p> </li> <li> <p><code>transform_filter</code>: Filter data based on a condition (often used with selections, as seen in the previous section, or with Vega expressions).     <pre><code># chart.transform_filter(\n#    'datum.some_field &gt; 100' # Vega expression syntax for filtering\n# )\n</code></pre></p> </li> <li> <p><code>transform_aggregate</code>: Perform aggregations. This is implicitly used when you specify aggregations in encodings (e.g., <code>y='average(Sales):Q'</code>). You can also define it explicitly.     <pre><code># chart.transform_aggregate(\n#    aggregated_sales='average(Sales)',\n#    groupby=['Category']\n# )\n</code></pre></p> </li> <li> <p><code>transform_bin</code>: Bin data, often used for histograms. This is implicitly used with <code>bin=True</code> in encodings (e.g., <code>alt.X('Horsepower:Q', bin=True)</code>).     <pre><code># chart.transform_bin(\n#    'binned_horsepower',\n#    field='Horsepower',\n#    bin=alt.Bin(maxbins=10)\n# )\n</code></pre></p> </li> </ul> <p>When to use Altair transformations?</p> <ul> <li>For very simple calculations or filters that are tightly coupled with a specific chart's logic.</li> <li>When interactivity drives the transformation (e.g., filtering based on a selection).</li> <li>For convenience with standard operations like binning or simple aggregation directly in encodings.</li> </ul> <p>General Recommendation: For clarity, complex logic, and consistency with your data manipulation skills, prefer Polars for significant data transformations. Use Altair's transformations for minor adjustments specific to a single chart or for enabling interactivity. This aligns with the course goal to interpret API design choices of major data science libraries like Polars and Altair to use them more effectively. ```</p>"},{"location":"eda/data-visualization/decorations/","title":"Enhancing Your Visualizations","text":"<p>Creating a basic chart is just the first step. To effectively communicate insights, your visualizations need to be clear, well-annotated, and visually appealing. Altair provides extensive options for customizing almost every aspect of your chart.</p>"},{"location":"eda/data-visualization/decorations/#chart-properties-titles-sizing-and-more","title":"Chart Properties: Titles, Sizing, and More","text":"<p>The <code>.properties()</code> method allows you to set various chart-level attributes.</p> <ul> <li>Titles: Every chart should have a descriptive title.     <pre><code>.properties(title='My Awesome Chart Title')\n</code></pre></li> <li>Sizing: You can control the width and height of your chart.     <pre><code>.properties(width=600, height=400)\n</code></pre>     For responsive sizing that fills the available width, you can set <code>width='container'</code>. However, be mindful that this might not work perfectly in all rendering environments or when saving static images.</li> </ul>"},{"location":"eda/data-visualization/decorations/#customizing-axes-labels-formatting-and-scales","title":"Customizing Axes: Labels, Formatting, and Scales","text":"<p>Clear axis labels are crucial for understanding a chart. You can customize axes by passing an <code>alt.Axis()</code> object to the <code>axis</code> argument within an encoding channel definition (e.g., <code>alt.X()</code>, <code>alt.Y()</code>).</p> <ul> <li>Axis Titles: Change the default column name to something more descriptive.     <pre><code>alt.X('Horsepower:Q', axis=alt.Axis(title='Vehicle Horsepower (HP)'))\nalt.Y('Miles_per_Gallon:Q', axis=alt.Axis(title='Fuel Efficiency (MPG)'))\n</code></pre></li> <li>Label Formatting: Control the appearance of axis tick labels (e.g., number formats, date formats).     <pre><code>alt.Y('some_value:Q', axis=alt.Axis(format='~s')) # SI unit prefix (k, M, G)\nalt.X('date_column:T', axis=alt.Axis(format='%Y-%m-%d', title='Date')) # Date format\n</code></pre></li> <li>Disabling Axes/Gridlines: Sometimes you might want to remove axes or gridlines for a cleaner look.     <pre><code>alt.X('field:Q', axis=None) # Remove X-axis completely\nalt.Y('field:Q', axis=alt.Axis(grid=False)) # Remove Y-axis gridlines\n</code></pre></li> <li>Scale Properties: While axis deals with the visual representation of the scale (ticks, labels), <code>alt.Scale()</code> within an encoding channel definition configures the mapping from data values to visual values.     <pre><code>alt.Y('value:Q', scale=alt.Scale(domain=[0, 100])) # Set Y-axis domain\nalt.Color('category:N', scale=alt.Scale(scheme='tableau10')) # Set color scheme\n</code></pre></li> </ul>"},{"location":"eda/data-visualization/decorations/#working-with-color-schemes-and-scales","title":"Working with Color: Schemes and Scales","text":"<p>Color is a powerful visual encoding but should be used thoughtfully.</p> <ul> <li>Categorical Data (Nominal/Ordinal): Use qualitative color schemes where colors are distinct. Altair provides many Vega-Lite schemes.     <pre><code>alt.Color('Origin:N', scale=alt.Scale(scheme='category10')) # Or 'accent', 'paired', 'tableau20' etc.\n</code></pre></li> <li>Quantitative Data: Use sequential (for ordered data, e.g., light to dark) or diverging (when data has a meaningful midpoint, e.g., negative to positive) color schemes.     <pre><code>alt.Color('Temperature:Q', scale=alt.Scale(scheme='viridis')) # Sequential\nalt.Color('ProfitRatio:Q', scale=alt.Scale(scheme='redblue', domainMid=0)) # Diverging\n</code></pre></li> <li>Specifying a Fixed Color: If you don't want color to encode a data field but simply want all marks to be a specific color, use <code>alt.value()</code>.     <pre><code>.mark_point(color='steelblue')\n# or within encode if other encodings are present\n# .encode(..., color=alt.value('steelblue'))\n</code></pre></li> </ul>"},{"location":"eda/data-visualization/decorations/#informative-tooltips","title":"Informative Tooltips","text":"<p>Tooltips provide details when a user hovers over a data point. They significantly enhance interactivity and allow you to convey more information without cluttering the main chart.</p> <p><pre><code>.encode(\n    # ... other encodings\n    tooltip=[\n        'Name:N',\n        alt.Tooltip('Horsepower:Q', title='HP'), # Custom title in tooltip\n        alt.Tooltip('Miles_per_Gallon:Q', title='MPG', format='.1f') # Format number\n    ]\n)\n</code></pre> You can specify a list of column names or use <code>alt.Tooltip()</code> for more control over titles and formatting within the tooltip.</p>"},{"location":"eda/data-visualization/decorations/#saving-your-charts","title":"Saving Your Charts","text":"<p>Once you've created and refined your visualization, you'll likely want to save it.</p> <ul> <li>HTML: Saves the chart as an interactive HTML file.     <pre><code>my_chart.save('my_chart.html')\n</code></pre></li> <li>PNG or SVG (Static Images): For static images, you might need an additional engine like <code>altair_viewer</code> or <code>vl-convert</code>. This setup can sometimes be tricky depending on your environment.     <pre><code># Requires altair_viewer or vl-convert installed\n# my_chart.save('my_chart.png')\n# my_chart.save('my_chart.svg')\n</code></pre>     For Colab, saving as HTML is straightforward. For PNG/SVG, if direct saving isn't working easily, you can often right-click the displayed chart in the output cell and save the image from there (though this might have limitations).</li> </ul>"},{"location":"eda/data-visualization/decorations/#incremental-development-exercise-recap","title":"Incremental Development Exercise (Recap)","text":"<p>As emphasized before, building a polished chart is an iterative process: 1.  Core Plot: Start with the basic data, mark, and essential x/y encodings. 2.  Add Layers: Introduce color, size, or faceting if needed. 3.  Refine Tooltips: Ensure hover information is useful. 4.  Customize Aesthetics: Adjust titles, axis labels, formatting, and color schemes. 5.  Review and Iterate: Check if the chart clearly communicates the intended message.</p> <p>This approach makes the process manageable and helps in pinpointing issues if they arise. ```</p>"},{"location":"eda/data-visualization/first-plots/","title":"Your First Plots with Altair","text":"<p>In the previous section, we saw why visualization is important and got a glimpse of Altair's basic structure. Now, let's dive into creating common chart types. We'll focus on understanding how to map your data to visual marks and encodings.</p>"},{"location":"eda/data-visualization/first-plots/#loading-and-preparing-data","title":"Loading and Preparing Data","text":"<p>Altair works seamlessly with Pandas DataFrames and can also handle Polars DataFrames, often directly or with a simple conversion. For our examples, we'll primarily use datasets from the <code>vega_datasets</code> package and convert them to Polars DataFrames to align with the data manipulation tools you've learned.</p> <p>Loading a Sample Dataset (<code>cars</code> dataset): The <code>vega_datasets</code> library provides a variety of sample datasets. Let's consider the <code>cars</code> dataset.</p> <pre><code>from vega_datasets import data as vega_data\nimport polars as pl\n\n# Load as Pandas DataFrame first\ncars_pd_df = vega_data.cars()\n\n# Convert to Polars DataFrame\ncars_pl_df = pl.from_pandas(cars_pd_df)\n\n# Always a good idea to inspect your data\nprint(cars_pl_df.head())\nprint(cars_pl_df.shape)\nprint(cars_pl_df.null_count()) # Check for missing values\n</code></pre> <p>Quick Polars Recap for Pre-Visualization:</p> <p>Before plotting, you might need to select specific columns, filter rows, or handle missing data using Polars. For example: <pre><code># Use the columns 'Name', 'Miles_per_Gallon', 'Horsepower' from cars_pl_df DataFrame object\ncars_pl_df.select(pl.col('Name', 'Miles_per_Gallon', 'Horsepower') #\n\n# filter rows: get rows where \"Origin\" column has the value \"USA\"\ncars_pl_df.filter(pl.col('Origin') == 'USA') #\n\n# Drop any rows with null value in one or more columns\ncars_pl_df.drop_nulls(\n    subset=['Cylinders', 'Displacement']\n)\n# alternative syntax \ncars_pl_df.filter(\n    ~pl.any_horizontal(\n        pl.col('Cylinders', 'Displacement').is_null()\n    )\n)\n</code></pre></p>"},{"location":"eda/data-visualization/first-plots/#basic-chart-types-and-encodings","title":"Basic Chart Types and Encodings","text":"<p>The core of creating any plot in Altair involves:</p> <ol> <li>Initializing a <code>Chart</code> object with your data.</li> <li>Choosing a <code>mark</code> type (e.g., <code>mark_point()</code>, <code>mark_bar()</code>, <code>mark_line()</code>).</li> <li>Defining <code>encode()</code>ings to map data fields to visual properties.</li> </ol> <p>Altair Data Types: When specifying fields in encodings, Altair can often infer the data type. However, it's good practice to be explicit using shorthands:</p> <ul> <li><code>:Q</code> for Quantitative (numerical values that can be measured, e.g., temperature, horsepower).</li> <li><code>:N</code> for Nominal (categorical data with no inherent order, e.g., car origin, movie genre).</li> <li><code>:O</code> for Ordinal (categorical data with a meaningful order, e.g., t-shirt size S/M/L, ratings).</li> <li><code>:T</code> for Temporal (date/time values).</li> </ul>"},{"location":"eda/data-visualization/first-plots/#1-scatter-plot","title":"1. Scatter Plot","text":"<p>Scatter plots are used to visualize the relationship between two quantitative variables. Each point represents an observation.</p> <ul> <li>Mark: <code>mark_point()</code> or <code>mark_circle()</code></li> <li>Key Encodings:<ul> <li><code>x='field_name:Q'</code>: Maps a quantitative field to the x-axis.</li> <li><code>y='field_name:Q'</code>: Maps a quantitative field to the y-axis.</li> <li><code>color='field_name:N'</code>: Colors points by a nominal field (category).</li> <li><code>size='field_name:Q'</code>: Varies point size by a quantitative field.</li> <li><code>tooltip=['field1', 'field2']</code>: Shows specified fields on hover.</li> </ul> </li> </ul> <p>Example: <pre><code>import altair as alt\n\n# Assuming cars_pl_df is your Polars DataFrame\nscatter_plot = alt.Chart(cars_pl_df.drop_nulls(subset=['Horsepower', 'Miles_per_Gallon'])).mark_circle(size=60).encode(\n    x='Horsepower:Q',\n    y='Miles_per_Gallon:Q',\n    color='Origin:N',  # Color by country of origin\n    tooltip=['Name:N', 'Horsepower:Q', 'Miles_per_Gallon:Q']\n).properties(\n    title='Horsepower vs. Miles per Gallon'\n)\n# scatter_plot.show() # To display\n</code></pre></p>"},{"location":"eda/data-visualization/first-plots/#2-bar-chart","title":"2. Bar Chart","text":"<p>Bar charts are excellent for comparing a quantitative measure across different categories or showing counts.</p> <ul> <li>Mark: <code>mark_bar()</code></li> <li>Key Encodings:<ul> <li><code>x='category_field:N'</code>: Nominal field for categories on the x-axis.</li> <li><code>y='quantitative_field:Q'</code>: Quantitative field for bar height (or <code>y='count():Q'</code> for frequencies).</li> <li>Or, <code>x='quantitative_field:Q'</code>, <code>y='category_field:N'</code> for horizontal bars.</li> <li><code>color='category_field:N'</code>: Colors bars by category (often redundant if categories are already on an axis, but useful for stacked/grouped bars).</li> </ul> </li> </ul> <p>Example (Average MPG per Origin): Altair can perform simple aggregations. <pre><code>avg_mpg_bar_chart = alt.Chart(cars_pl_df.drop_nulls(subset=['Miles_per_Gallon', 'Origin'])).mark_bar().encode(\n    x='Origin:N',\n    y='average(Miles_per_Gallon):Q', # Altair aggregation\n    color='Origin:N',\n    tooltip=['Origin:N', 'average(Miles_per_Gallon):Q']\n).properties(\n    title='Average Miles per Gallon by Origin'\n)\n</code></pre> Alternatively, you can pre-aggregate with Polars: <pre><code># Polars aggregation\navg_mpg_origin_pl = cars_pl_df.drop_nulls(subset=['Miles_per_Gallon', 'Origin']) \\\n    .group_by('Origin') \\\n    .agg(pl.mean('Miles_per_Gallon').alias('avg_MPG'))\n\navg_mpg_bar_chart_polars = alt.Chart(avg_mpg_origin_pl).mark_bar().encode(\n    x='Origin:N',\n    y='avg_MPG:Q',\n    color='Origin:N',\n    tooltip=['Origin:N', 'avg_MPG:Q']\n).properties(\n    title='Average Miles per Gallon by Origin (Polars Pre-aggregated)'\n)\n</code></pre></p>"},{"location":"eda/data-visualization/first-plots/#3-line-chart","title":"3. Line Chart","text":"<p>Line charts are ideal for showing trends over a continuous or ordered sequence, typically time.</p> <ul> <li>Mark: <code>mark_line()</code></li> <li>Key Encodings:<ul> <li><code>x='ordered_field:T'</code> (Temporal) or <code>:O</code> (Ordinal) or <code>:Q</code> (if a continuous quantitative sequence).</li> <li><code>y='quantitative_field:Q'</code>.</li> <li><code>color='category_field:N'</code>: For plotting multiple lines from different categories.</li> </ul> </li> </ul> <p>Example (Seattle Weather - Max Temperature Over Time): We'll use the <code>seattle-weather</code> dataset. <pre><code># Load seattle_weather data\nweather_pd_df = vega_data.seattle_weather()\nweather_pl_df = pl.from_pandas(weather_pd_df)\n\nline_chart = alt.Chart(weather_pl_df).mark_line().encode(\n    x='date:T',  # 'T' for Temporal data\n    y='temp_max:Q',\n    color='weather:N', # Show different weather conditions as separate lines\n    tooltip=['date:T', 'temp_max:Q', 'weather:N']\n).properties(\n    title='Maximum Daily Temperature in Seattle Over Time by Weather Condition',\n    width=600\n)\n</code></pre></p>"},{"location":"eda/data-visualization/first-plots/#4-histogram","title":"4. Histogram","text":"<p>Histograms visualize the distribution of a single quantitative variable by dividing the data range into bins and showing the frequency of observations in each bin.</p> <ul> <li>Mark: <code>mark_bar()</code></li> <li>Key Encodings:<ul> <li><code>x='quantitative_field:Q'</code> with <code>bin=True</code> or <code>alt.X('quantitative_field:Q', bin=alt.Bin(maxbins=20))</code> for more control.</li> <li><code>y='count():Q'</code> (Altair automatically computes the count for histograms).</li> </ul> </li> </ul> <p>Example (Distribution of Horsepower):</p> <pre><code>alt.Chart(\n    cars_pl_df.drop_nulls(subset=['Horsepower'])\n    ).mark_bar().encode(\n    alt.X(\n        'Horsepower:Q', \n        bin=alt.Bin(maxbins=20), \n        title='Horsepower Bins',\n        ), \n    alt.Y('count():Q'),\n    tooltip=[alt.Tooltip('Horsepower:Q', bin=True), 'count():Q']\n).properties(\n    title='Distribution of Car Horsepower',\n    width=400\n)\n</code></pre>"},{"location":"eda/data-visualization/first-plots/#workflow-tip-incremental-development","title":"Workflow Tip: Incremental Development","text":"<p>When building visualizations, especially complex ones:</p> <ol> <li>Start Simple: Get your data loaded and create the most basic version of your chart (e.g., <code>alt.Chart(data).mark_point().encode(x='col_A', y='col_B')</code>).</li> <li>Add Encodings Gradually: Introduce color, size, tooltips, etc., one by one. Check the output at each step.</li> <li>Refine Properties: Adjust titles, labels, sizes, and scales last.</li> <li>Tweak DataFrame: Tweak the DataFrame using Polars if you want to adjust properties that are strongly coupled to datapoints from the dataset. For example, the legend labels for categorical variables are easier to change in Polars than in Altair. This iterative approach makes debugging easier and helps you build intuition for how each component affects the final visualization.</li> </ol>"},{"location":"eda/data-visualization/interactive-visualization/","title":"Interactive Visualizations with Altair","text":"<p>Static charts are informative, but interactive charts can transform how users explore and understand data. Altair provides a powerful way to add interactivity through selections. Selections allow users to highlight, filter, or otherwise interact with the data points directly on the chart.</p>"},{"location":"eda/data-visualization/interactive-visualization/#the-concept-of-selections","title":"The Concept of Selections","text":"<p>A selection in Altair defines how the user can interact with the chart. There are several types, but we'll focus on two common ones:</p> <ul> <li><code>alt.selection_single()</code>: Allows the user to select a single data point (or a group if <code>fields</code> or <code>encodings</code> are specified) by clicking.<ul> <li><code>fields</code>: Specify data fields that define the scope of the selection. Clicking a point selects all other points with the same values in these fields.</li> <li><code>encodings</code>: Similar to <code>fields</code>, but uses encoding channels (e.g., 'x', 'color').</li> <li><code>on</code>: Event to trigger selection (e.g., 'click', 'mouseover'). Defaults to 'click'.</li> <li><code>empty</code>: How to treat the selection when no items are selected ('all' or 'none'). Defaults to 'all'.</li> </ul> </li> <li><code>alt.selection_interval()</code>: Allows the user to select a range of data points by clicking and dragging to draw a rectangular brush.<ul> <li><code>encodings</code>: Specify encoding channels (typically 'x' and/or 'y') to define the selection interval.</li> <li><code>empty</code>: Defaults to 'all'.</li> </ul> </li> <li><code>alt.selection_point()</code> (formerly <code>selection_multi</code>): Allows selection of multiple discrete points, typically by shift-clicking.</li> </ul> <p>You first define a selection and give it a name, then you can use this named selection to control aspects of your chart.</p> <pre><code># Define a single-click selection\nclick_selection = alt.selection_single(\n    fields=['Category'], # Selects all items of the same category on click\n    empty='none' # No items selected initially means nothing is \"selected\"\n)\n\n# Define an interval selection (brushing)\ninterval_brush = alt.selection_interval(\n    encodings=['x'], # Brush along the x-axis\n    empty='all' # Initially, all data is considered selected\n)\n</code></pre>"},{"location":"eda/data-visualization/interactive-visualization/#using-selections","title":"Using Selections","text":"<p>Once a selection is defined and added to a chart using <code>.add_params()</code>, its state (e.g., which data points are selected) can be used to:</p> <ol> <li>Conditionally Encode Properties: Change visual properties (like color, size, opacity) of marks based on whether they are part of the selection. This is done using <code>alt.condition()</code>.</li> <li>Filter Data: Display only the selected data in the current chart or in other linked charts.</li> </ol>"},{"location":"eda/data-visualization/interactive-visualization/#1-conditional-encodings","title":"1. Conditional Encodings","text":"<p><code>alt.condition(selection, value_if_selected, value_if_not_selected)</code> is the key here.</p> <p>Example: Highlighting selected points Change the color and opacity of points based on a single click selection.</p> <p><pre><code>selection_name = alt.selection_single(empty='none')\n\nalt.Chart(data).mark_point().encode(\n    x='x_field:Q',\n    y='y_field:Q',\n    color=alt.condition(selection_name, 'highlight_color:N', alt.value('lightgray')),\n    opacity=alt.condition(selection_name, alt.value(1.0), alt.value(0.3))\n).add_params(\n    selection_name\n)\n</code></pre> In this example, points included in <code>selection_name</code> will get <code>highlight_color</code> and full opacity, while others will be light gray and semi-transparent.</p>"},{"location":"eda/data-visualization/interactive-visualization/#2-filtering-data-linked-views-basic","title":"2. Filtering Data (Linked Views - Basic)","text":"<p>Selections can also be used to filter data. The <code>.transform_filter(selection_name)</code> method is used for this. This is powerful for creating linked views where interacting with one chart affects another.</p> <p>Example: A scatter plot filters a bar chart (conceptual) Imagine a scatter plot where you select points. A nearby bar chart could then update to show data only related to those selected points.</p> <p><pre><code>brush = alt.selection_interval() # Define a brush selection\n\nscatter = alt.Chart(data).mark_point().encode(\nx='X:Q', y='Y:Q'\n).add_params(brush)\n\nbars = alt.Chart(data).mark_bar().encode(\nx='Category:N', y='average(Value):Q'\n).transform_filter(\nbrush # Filter bars based on the brush selection in the scatter plot\n)\n\nscatter &amp; bars # Display side-by-side\n</code></pre> For an introductory session, we'll focus primarily on conditional encodings within a single chart, as linked views can add complexity quickly.</p>"},{"location":"eda/data-visualization/interactive-visualization/#focus-on-simple-meaningful-interactions","title":"Focus on Simple, Meaningful Interactions","text":"<p>The goal of adding interactivity is to make the data exploration process more intuitive and insightful.</p> <ul> <li>Start with simple interactions like highlighting.</li> <li>Ensure the interaction has a clear purpose and helps answer a question or reveal a pattern.</li> <li>Avoid overly complex interactions that might confuse the user.</li> </ul> <p>Interactive charts are particularly powerful for:</p> <ul> <li>Drilling down into specific data points.</li> <li>Comparing subsets of data dynamically.</li> <li>Exploring relationships that might not be obvious in a static view. ```</li> </ul>"},{"location":"eda/data-visualization/intro-to-data-viz/","title":"Introduction to Data Visualization with Altair","text":""},{"location":"eda/data-visualization/intro-to-data-viz/#why-visualize-data-the-tale-of-anscombes-quartet","title":"Why Visualize Data? The Tale of Anscombe's Quartet","text":"<p>In data analysis, summary statistics like mean, variance, and correlation provide a concise numerical overview of our datasets. However, relying solely on these statistics can sometimes be misleading.</p> <p>Consider Anscombe's Quartet, a set of four distinct datasets created by statistician Francis Anscombe in 1973. Let's look at their primary summary statistics:</p> Statistic Dataset I Dataset II Dataset III Dataset IV Mean of X 9.0 9.0 9.0 9.0 Mean of Y 7.50 7.50 7.50 7.50 Variance of X 11.0 11.0 11.0 11.0 Variance of Y 4.125 4.123 4.123 4.123 Correlation X,Y 0.816 0.816 0.816 0.817 Linear Regression y=3+0.5x y=3+0.5x y=3+0.5x y=3+0.5x <p>Based on these numbers, the datasets appear remarkably similar! However, visualization tells a dramatically different story.</p> Anscombe's Quartet visualized. Each dataset, while having nearly identical summary statistics, exhibits a unique visual pattern. <p>This powerful example underscores why data visualization is crucial. It helps us to: * Identify patterns, trends, anomalies, and relationships within datasets to guide further analysis. * Understand data distributions. * Communicate findings effectively. * Build intuition about the data that numbers alone cannot provide.</p> <p>In this course, particularly for business decision-making, turning raw data into actionable insights often hinges on effective visualization.</p>"},{"location":"eda/data-visualization/intro-to-data-viz/#what-is-altair","title":"What is Altair?","text":"<p>Altair is a declarative statistical visualization library for Python. But what does \"declarative\" mean in this context?</p> <ul> <li>Declarative Visualization: You describe what you want to see, specifying the data and how different data features should map to visual properties. Altair then handles the \"how\" of rendering the plot. This contrasts with imperative plotting tools where you often build plots step-by-step (e.g., draw axis, draw points, add labels).</li> <li>Based on the Grammar of Graphics: Altair's design is rooted in the principles of the Grammar of Graphics (specifically, Vega and Vega-Lite). This grammar provides a strong theoretical foundation for building a wide array of statistical graphics by combining a few key components:<ul> <li>Data: The dataset you are visualizing (typically a table-like structure).</li> <li>Marks: Geometric objects that represent your data (e.g., points, bars, lines, areas).</li> <li>Encodings: Rules that map data fields (columns) to the visual properties of marks (e.g., x-position, y-position, color, size, shape, opacity).</li> </ul> </li> </ul> <p>By understanding these core concepts, you gain a deeper, more adaptable understanding that transcends rote memorization of specific chart recipes.</p>"},{"location":"eda/data-visualization/intro-to-data-viz/#altairs-core-idea-the-chart-object-and-encodings","title":"Altair's Core Idea: The <code>Chart</code> Object and Encodings","text":"<p>The fundamental structure of an Altair visualization is surprisingly simple and consistent:</p> <pre><code>import altair as alt\n\n# Assuming 'my_data' is a Polars (or Pandas) DataFrame\nchart = alt.Chart(my_data).mark_point().encode(\n    x='data_column_for_x_axis',\n    y='data_column_for_y_axis',\n    color='data_column_for_color'\n    # ... other encoding channels\n)\n</code></pre> <p>Let's break this down:</p> <ol> <li><code>alt.Chart(my_data)</code>: This creates a <code>Chart</code> object. It takes your dataset (e.g., a Polars DataFrame) as the primary argument.</li> <li><code>.mark_type()</code>: This method specifies the type of geometric mark you want to use to represent your data. Examples include <code>mark_point()</code>, <code>mark_bar()</code>, <code>mark_line()</code>, <code>mark_circle()</code>, <code>mark_rect()</code>, etc.</li> <li><code>.encode(...)</code>: This crucial method defines the encoding channels. Here, you map columns from your dataset to visual properties of the marks. Common encodings include:<ul> <li><code>x</code>: Position on the x-axis.</li> <li><code>y</code>: Position on the y-axis.</li> <li><code>color</code>: Mark color.</li> <li><code>size</code>: Mark size.</li> <li><code>shape</code>: Mark shape (for point marks).</li> <li><code>opacity</code>: Mark transparency.</li> <li><code>tooltip</code>: Information to display on hover.</li> </ul> </li> </ol>"},{"location":"eda/data-visualization/intro-to-data-viz/#setting-up-altair-in-your-environment","title":"Setting up Altair in Your Environment","text":"<p>To use Altair, you'll need to install it and its dependencies. For this course, we'll also use <code>vega_datasets</code> for sample data and <code>polars</code>.</p> <p>Installation: This step is not required for Google Colab. All three packages are preinstalled. If you are using a different coding environment, you may need to execute this command. <pre><code>pip install altair vega_datasets polars\n</code></pre></p> <p>Importing into your Python script or notebook: <pre><code>import altair as alt\nimport polars as pl\nfrom vega_datasets import data as vega_data # For loading sample datasets\n</code></pre></p> <p>Enabling the Renderer: Altair needs to know how to display charts in your specific environment (Jupyter Notebook, JupyterLab, Google Colab, etc.). <pre><code># For Google Colab\nalt.renderers.enable('colab')\n\n# For JupyterLab\n# alt.renderers.enable('jupyterlab')\n\n# For classic Jupyter Notebook\n# alt.renderers.enable('notebook')\n\n# Default renderer (often works well across environments)\n# alt.renderers.enable('default')\n</code></pre> In this course, we'll primarily use Google Colab, so <code>alt.renderers.enable('colab')</code> will be our standard.</p>"},{"location":"eda/data-visualization/wrap-up/","title":"Wrap-up & Next Steps","text":"<p>Congratulations on completing this introduction to data visualization with Altair! You've learned the fundamental concepts and practical skills to start turning data into insightful visuals.</p>"},{"location":"eda/data-visualization/wrap-up/#key-concepts-recap","title":"Key Concepts Recap","text":"<p>Throughout this session, we've covered:</p> <ul> <li>The \"Why\" of Visualization: Understanding the importance of visual exploration, as exemplified by Anscombe's Quartet.</li> <li>Altair's Declarative Grammar:<ul> <li>The core structure: <code>alt.Chart(data).mark_type().encode(...)</code>.</li> <li>Understanding marks (geometric representations) and encodings (mapping data to visual properties like <code>x</code>, <code>y</code>, <code>color</code>, <code>size</code>, <code>tooltip</code>).</li> <li>The importance of data type hinting (<code>:Q</code>, <code>:N</code>, <code>:O</code>, <code>:T</code>).</li> </ul> </li> <li>Basic Chart Types: Creating scatter plots, bar charts, line charts, and histograms to answer different types of questions.</li> <li>Enhancing Visualizations: Customizing charts with titles, axis labels, color schemes, and informative tooltips to improve clarity and impact.</li> <li>Interactive Visualizations: Adding basic interactivity using selections (<code>selection_single</code>, <code>selection_interval</code>) and conditional encodings to allow for dynamic data exploration.</li> <li>Data Transformations:<ul> <li>Primarily using Polars for robust pre-processing (filtering, deriving columns, aggregating) before visualization.</li> <li>Awareness of Altair's built-in transformations for simpler, chart-specific tasks.</li> </ul> </li> <li>Best Practices: Choosing appropriate charts, aiming for clarity and simplicity, knowing your audience, iterating on your designs, and ethical considerations.</li> </ul>"},{"location":"eda/data-visualization/wrap-up/#connection-to-future-topics-machine-learning","title":"Connection to Future Topics: Machine Learning","text":"<p>The visualization skills you've developed are foundational for upcoming topics in the course, particularly Machine Learning. You'll find that visualizing your data is crucial for:</p> <ul> <li>Exploratory Data Analysis (EDA) before model building: Understanding feature distributions, identifying correlations, and detecting potential issues like outliers.</li> <li>Feature Engineering: Visualizing the impact of newly created features.</li> <li>Model Evaluation: Plotting results from common supervised learning models for regression and classification tasks, such as visualizing prediction errors (residuals), confusion matrices, or ROC curves.</li> <li>Unsupervised Learning: Visualizing clusters or the results of dimensionality reduction techniques.</li> </ul> <p>Being able to \"see\" your data and model outputs will greatly enhance your understanding and ability to build effective machine learning models.</p>"},{"location":"eda/data-visualization/wrap-up/#things-to-try-continue-your-learning-journey","title":"Things to Try: Continue Your Learning Journey!","text":"<p>The best way to solidify your understanding and build confidence is through practice. The starter Jupyter Notebook contains a list of \"Things to Try\" exercises. These challenges encourage you to:</p> <ul> <li>Explore new datasets (See GitHub repo of <code>vega-datasets</code>, or Kaggle for practice datasets).</li> <li>Recreate charts you find in the wild.</li> <li>Experiment deeply with customizations and interactivity.</li> <li>Combine your Polars and Altair skills.</li> <li>Practice data storytelling.</li> </ul> <p>I strongly encourage you to work through these activities. Don't hesitate to consult the Altair documentation, online resources, or leverage generative AI tools strategically to help you along the way. Remember, the goal is to become comfortable applying these powerful data programming techniques to your own professional challenges and personal projects. ```</p>"},{"location":"eda/data-wrangling/","title":"Data Wrangling with Polars","text":"<ul> <li> Starter Colab Notebook</li> <li> Introduction to Data Wrangling </li> <li> Data Exploration </li> <li> Core Polars Concepts </li> <li> Data Transformation </li> <li> Data Aggregation </li> <li> Stitching &amp; Saving Data </li> </ul>"},{"location":"eda/data-wrangling/core-polars-concepts/","title":"Core Polars Concepts","text":"<p>Core Polars Concepts - The Building Blocks</p> <p>Having learned how to load and perform initial inspections on your data, we now focus on the core data structures and the expression API that form the foundation of Polars.</p> Prerequisite <pre><code># Prerequisite: Ensure Polars is imported.\nimport polars as pl\n\n# We will use customers_df and orders_df from previous modules.\n# For stand-alone execution of this module's examples,\n# ensure these DataFrames are loaded or use placeholders.\ntry:\n    _ = customers_df.shape # Check if exists\nexcept NameError:\n    print(\"Placeholder: customers_df not loaded. Creating a minimal example.\")\n    customers_df = pl.DataFrame({\n        \"customer_id\": [101, 102, 103, 104],\n        \"name\": [\"Alice Wonderland\", \"Bob The Builder\", \"Charlie Brown\", \"Diana Prince\"],\n        \"city\": [\"New York\", \"London\", \"Paris\", \"New York\"],\n        \"age\": [28, 35, 45, 28] # Added more variety for filtering\n    })\n\ntry:\n    _ = orders_df.shape # Check if exists\nexcept NameError:\n    print(\"Placeholder: orders_df not loaded. Creating a slightly more diverse example for context demonstration.\")\n    orders_df = pl.DataFrame({\n        \"order_id\": [201, 202, 203, 204, 205, 206],\n        \"customer_id\": [101, 102, 101, 103, 102, 104],\n        \"product_category\": [\"Books\", \"Tools\", \"Books\", \"Home Goods\", \"Books\", \"Electronics\"],\n        \"quantity\": [2, 1, 3, 1, 1, 2],\n        \"unit_price\": [15.99, 199.50, 10.00, 25.00, 12.00, 79.99]\n    })\n</code></pre>"},{"location":"eda/data-wrangling/core-polars-concepts/#polars-dataframe-revisited","title":"Polars DataFrame Revisited","text":"<p>You've already been working with Polars DataFrames (e.g., <code>customers_df</code>, <code>orders_df</code>). Let's formally define it:</p> <ul> <li> <p>Definition: A Polars DataFrame is a 2-dimensional, in-memory, tabular data structure. It consists of an ordered collection of named columns, where each column can have a different data type (e.g., integer, string, float). Think of it as a more powerful version of a spreadsheet or an SQL table within your Python environment.</p> </li> <li> <p>Creation: While DataFrames are commonly created by reading files, you can also construct them directly, for instance, from a Python dictionary where keys are column names and values are lists (or Polars Series) representing the column data.</p> <pre><code># Example: Creating a DataFrame from scratch\ndata_dict = {\n    'StudentID': [1001, 1002, 1003],\n    'Course': ['Finance', 'Marketing', 'Finance'],\n    'MidtermScore': [85, 92, 78]\n}\nscores_df = pl.DataFrame(data_dict)\nprint(\"DataFrame created from scratch:\")\nprint(scores_df)\n</code></pre> </li> </ul>"},{"location":"eda/data-wrangling/core-polars-concepts/#polars-series","title":"Polars Series","text":"<ul> <li> <p>Definition: A Polars Series is a 1-dimensional array-like object that represents a single column within a DataFrame. Every column in a DataFrame is, in fact, a Series. A Series has a name (usually inherited from its column name) and a single data type for all its elements.</p> </li> <li> <p>Accessing a Series: You can extract a column as a Series from a DataFrame.</p> <ul> <li>Using <code>DataFrame.get_column(\"column_name\")</code>: This method directly returns the specified column as a Series.</li> <li>Using <code>DataFrame[\"column_name\"]</code>: This is a common shorthand to select a column, returning a Series.</li> </ul> <pre><code># Extracting the 'city' column from customers_df as a Series\ncity_series = customers_df.get_column('city')\nprint(\"\\n'city' column extracted as a Series:\")\nprint(city_series)\nprint(f\"Type of city_series: {type(city_series)}\")\nprint(f\"Name of city_series: {city_series.name}\")\nprint(f\"DataType of city_series: {city_series.dtype}\")\n\n# Alternative shorthand (produces the same Series)\n# age_series = customers_df['age']\n# print(\"\\n'age' column extracted as a Series (shorthand):\")\n# print(age_series)\n</code></pre> <p>While <code>df.select(pl.col('column_name'))</code> also selects a column, it returns a new single-column DataFrame. To get a Series from it, you'd use <code>df.select(pl.col('column_name')).to_series()</code>. Understanding Series is important as many expressions operate on them implicitly.</p> </li> </ul>"},{"location":"eda/data-wrangling/core-polars-concepts/#polars-expressions-the-core-engine","title":"Polars Expressions: The Core Engine","text":"<p>Expressions are the cornerstone of Polars' power and flexibility. They allow you to define computations or transformations on your data in a declarative way.</p> <ul> <li>What are Expressions? Think of expressions as blueprints or recipes for operations you want to perform. They describe what you want to do, not how to do it step-by-step. Polars' query optimizer can then figure out the most efficient way to execute these blueprints.</li> <li>Lazy Evaluation (in many contexts): Expressions themselves are typically not executed immediately. They are evaluated when used within an \"execution context\" \u2013 a DataFrame method like <code>select()</code>, <code>filter()</code>, or <code>with_columns()</code>.</li> </ul> <p>Let's explore the fundamental components for building expressions:</p> <ul> <li> <p>Referring to Columns: <code>pl.col()</code>     The primary way to refer to one or more columns within an expression is using <code>pl.col()</code>.</p> <ul> <li><code>pl.col(\"column_name\")</code>: Refers to a single column by its name.</li> <li><code>pl.col([\"colA\", \"colB\"])</code>: Refers to multiple specified columns.</li> <li><code>pl.col(\"*\")</code> or <code>pl.all()</code>: Refers to all columns.  </li> <li>You can also use regular expressions or data types with <code>pl.col()</code> to select columns.</li> </ul> <pre><code># An expression referring to the 'age' column in customers_df\nage_expression = pl.col(\"age\")\nprint(f\"\\nA Polars expression for the 'age' column: {age_expression}\")\n# Note: Printing the expression itself just shows its structure, it doesn't compute anything yet.\n</code></pre> </li> <li> <p>Literal Values: <code>pl.lit()</code>     When you want to use a constant value (like a number, string, or boolean) within an expression, you should wrap it with <code>pl.lit()</code>. This tells Polars that the value is a literal, not a column name or another expression.</p> <ul> <li><code>pl.lit(value)</code>: Creates a literal expression from the given value.</li> </ul> <pre><code># An expression representing the literal numeric value 10\nliteral_ten_expr = pl.lit(10)\nprint(f\"A Polars expression for the literal value 10: {literal_ten_expr}\")\n\n# An expression representing a literal string\nliteral_string_expr = pl.lit(\"Approved\")\nprint(f\"A Polars expression for a literal string: {literal_string_expr}\")\n</code></pre> </li> <li> <p>Basic Operations with Expressions:     Expressions can be combined using standard arithmetic, comparison, and logical operators to define more complex computations.</p> <pre><code># Example 1: Arithmetic operation - Calculate discounted price\n#\n# Assumes 'unit_price' and a hypothetical 'discount_rate' column (or use a literal for discount)\n# Let's use orders_df and assume we want to apply a fixed 0.05 discount if not specified\n# For illustration, let's define an expression for 'unit_price' * (1 - 0.05)\noriginal_price_expr = pl.col(\"unit_price\")\ndiscount_rate_lit_expr = pl.lit(0.05) # 5% discount\ndiscounted_price_expr = original_price_expr * (pl.lit(1) - discount_rate_lit_expr)\nprint(f\"\\nExpression for discounted price: {discounted_price_expr}\")\n\n# Example 2: Comparison operation - Check if age is greater than 30\nage_col_expr = pl.col(\"age\")\nthreshold_age_lit_expr = pl.lit(30)\nis_older_than_30_expr = age_col_expr &gt; threshold_age_lit_expr\nprint(f\"Expression for 'age &gt; 30': {is_older_than_30_expr}\")\n</code></pre> </li> </ul>"},{"location":"eda/data-wrangling/core-polars-concepts/#expressions-are-executed-in-contexts","title":"Expressions are Executed in Contexts:","text":"<p>As emphasized, expressions are plans. They come to life when used within a DataFrame method that provides an \"execution context.\" Different contexts use expressions for different purposes. Let's briefly preview some common contexts with simple examples. Modules 5 and 6 will cover these in detail.</p> <ul> <li><code>select</code> Context: Used to choose, derive, or rename columns, creating a new DataFrame with only the specified columns. Expressions in <code>select</code> typically define what data each column in the new DataFrame will contain.</li> </ul> <pre><code># Example: Selecting customer name and deriving a 'is_minor' status\nprint(\"\\nPreview: Expression in `select` context\")\ncustomer_status_df = customers_df.select([\n    pl.col(\"name\"),\n    pl.col(\"age\"),\n    (pl.col(\"age\") &lt; pl.lit(18)).alias(\"is_minor\")\n])\nprint(customer_status_df.head())\n</code></pre> <p>Observation: The expression <code>(pl.col(\"age\") &lt; pl.lit(18))</code> creates a new boolean column.</p> <ul> <li><code>with_columns</code> Context: Used to add new columns or modify existing ones in a DataFrame. The DataFrame returned will contain all original columns plus any new or altered ones defined by the expressions.</li> </ul> <pre><code># Example: Adding a 'total_price' column to orders_df (quantity * unit_price)\nprint(\"\\nPreview: Expression in `with_columns` context\")\norders_with_total = orders_df.with_columns(\n    (pl.col(\"quantity\") * pl.col(\"unit_price\")).alias(\"total_price\")\n)\nprint(orders_with_total.head())\n</code></pre> <p>Observation: The <code>total_price</code> column is added to the existing columns of <code>orders_df</code>.</p> <ul> <li><code>filter</code> Context: Used to select a subset of rows from a DataFrame based on one or more conditions. Expressions in <code>filter</code> must evaluate to a boolean (True/False) value for each row. Rows where the expression evaluates to <code>True</code> are kept.</li> </ul> <pre><code># Example: Filtering customers who are older than 30\nprint(\"\\nPreview: Expression in `filter` context\")\nadult_customers_df = customers_df.filter(\n    pl.col(\"age\") &gt; pl.lit(30)\n)\nprint(adult_customers_df)\n</code></pre> <p>Observation: The expression <code>pl.col(\"age\") &gt; pl.lit(30)</code> determines which rows are included in the result.</p> <ul> <li><code>group_by().agg()</code> Context: Used for grouping rows that have the same values in specified columns and then performing aggregate calculations (like sum, mean, count) on each group. Expressions within <code>.agg()</code> define these aggregate calculations.</li> </ul> <pre><code># Example: Calculating total quantity sold per product_category\nprint(\"\\nPreview: Expression in `group_by().agg()` context\")\nsales_by_category = orders_df.group_by(\"product_category\").agg([\n    pl.sum(\"quantity\").alias(\"total_quantity_sold\"), # Aggregates the 'quantity' column\n    pl.mean(\"unit_price\").alias(\"average_unit_price\")\n])\n# You can also write pl.col(\"quantity\").sum().alias(...)\nprint(sales_by_category)\n</code></pre> <p>Observation: The <code>pl.sum(\"quantity\")</code> expression calculates a sum for each <code>product_category</code> group.</p> <p>These examples illustrate that the same fundamental building blocks (<code>pl.col()</code>, <code>pl.lit()</code>, operators) are used to construct expressions, but their outcome is determined by the context in which they are executed.</p>"},{"location":"eda/data-wrangling/core-polars-concepts/#recap-and-importance","title":"Recap and Importance","text":"<ul> <li>DataFrame: Your primary multi-columnar data container.</li> <li>Series: Represents a single column within a DataFrame.</li> <li>Expressions (using <code>pl.col()</code>, <code>pl.lit()</code>, and operators): The fundamental way you define data transformations and computations in Polars. Their behavior is realized within execution contexts like <code>select</code>, <code>with_columns</code>, <code>filter</code>, and <code>group_by().agg()</code>.</li> </ul> <p>Mastering the concept of expressions and how they interact with different execution contexts is crucial, as they form the basis for almost all data manipulation tasks you will perform in Polars. Module 5 will heavily expand on the <code>select</code>, <code>with_columns</code>, and <code>filter</code> contexts.</p>"},{"location":"eda/data-wrangling/data-aggregation/","title":"Data Aggregation","text":"<p>Aggregating &amp; Reshaping Data - Summarizing for Insights</p> <p>After cleaning and transforming individual data points, the next step is often to aggregate data to extract higher-level insights and summaries. This module covers grouping data and applying aggregation functions, along with a brief introduction to pivoting and window functions.</p> Prerequisites <pre><code># Prerequisites: Ensure Polars is imported.\nimport polars as pl\n\n# --- Placeholder DataFrames (consistent with those used/modified in Module 5) ---\n# Re-establishing them here for clarity if this module is run standalone.\n# In a continuous notebook, these would carry over.\n\ncustomers_df = pl.DataFrame({\n    \"customer_id\": [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115],\n    \"name\": [\"Alice Wonderland\", \"Bob The Builder\", \"Charlie Brown\", \"Diana Prince\", \"Evan Almighty\", \"Fiona Gallagher\", \"George Jetson\", \"Hannah Montana\", \"Ian Malcolm\", \"Jane Doe\", \"Kevin McCallister\", \"Laura Palmer\", \"Michael Scott\", \"Nancy Drew\", \"Oscar Grouch\"],\n    \"registration_date_str\": [\"2022-01-15\", \"2022-03-22\", \"2022-05-10\", \"2022-07-01\", \"2022-08-19\", \"2023-01-20\", None, \"2023-04-05\", \"2023-06-12\", \"2023-07-21\", \"2023-09-01\", \"2023-10-15\", \"2024-02-10\", \"03/15/2024\", \"2024-05-01\"],\n    \"city\": [\"New York\", \"London\", \"Paris\", \"New York\", \"London\", \"New York\", \"Paris\", \"Berlin\", \"London\", \"New York\", \"Chicago\", \"Twin Peaks\", \"Scranton\", \"River Heights\", \"New York\"],\n    \"age\": [28, 35, 45, 3000, 42, 29, 50, 22, 55, None, 12, 17, 48, 18, 60]\n}).with_columns(\n    pl.when(pl.col(\"age\") &gt; 100).then(None).otherwise(pl.col(\"age\")).cast(pl.Int64, strict=False).alias(\"age_cleaned\") # Cleaned age\n)\n\norders_df = pl.DataFrame({\n    \"order_id\": [201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218],\n    \"customer_id\": [101, 102, 101, 103, 104, 102, 105, 101, 106, 108, 103, 107, 110, 111, 102, 113, 101, 115],\n    \"order_date_str\": [\"2023-01-20 10:30:00\", \"2023-02-15 11:05:30\", \"2023-02-28 14:12:55\", \"2023-03-10 09:00:15\", \"2023-03-12 17:45:00\", \"2023-04-05 12:00:00\", \"2023-04-22 16:20:30\", \"2023-05-01 10:00:00\", \"2023-05-15 08:55:10\", \"2023-06-01 11:30:00\", \"2023-06-20 13:40:00\", \"2023-07-01 00:00:00\", \"2023-07-25 10:10:10\", \"2023-09-10 14:20:30\", \"2023-11-05 19:00:00\", \"2024-02-15 09:30:00\", \"2024-03-01 10:00:00\", \"2024-05-05 12:12:12\"],\n    \"product_category\": [\"Books\", \"Tools\", \"Electronics\", \"Home Goods\", \"Antiques\", \"Books\", \"Electronics\", \"Books\", \"Beauty\", \"Music\", \"Home Goods\", \"Electronics\", \"Clothing\", \"Toys\", \"Tools\", \"Office Supplies\", \"Electronics\", \"Home Goods\"],\n    \"quantity\": [2, 1, 1, 3, 1, 1, None, 3, 2, 5, 1, 1, 2, 3, 1, 10, 1, 1],\n    \"unit_price\": [15.99, 199.50, 799.00, 25.00, 2500.00, 12.50, 99.99, 10.00, 45.75, 9.99, 150.00, 499.50, 75.00, 29.99, 75.00, 4.99, 1200.00, 12.00],\n    \"discount_applied\": [0.05, 0.10, None, 0.0, 0.15, 0.0, 0.05, None, 0.10, 0.0, 0.05, 0.10, None, 0.05, 0.0, 0.0, 0.15, 0.0]\n}).with_columns([\n    pl.col(\"quantity\").cast(pl.Int64, strict=False),\n    pl.col(\"unit_price\").cast(pl.Float64, strict=False),\n    pl.col(\"discount_applied\").cast(pl.Float64, strict=False),\n    pl.col(\"order_date_str\").str.to_datetime(format=\"%Y-%m-%d %H:%M:%S\", strict=False).alias(\"order_datetime\") # Parse date\n])\n</code></pre>"},{"location":"eda/data-wrangling/data-aggregation/#1-group-by-and-aggregations","title":"1. Group By and Aggregations","text":"<p>The \"group by\" operation is a common data manipulation pattern often described as \"split-apply-combine\":</p> <ul> <li>Split: The data is divided into groups based on the unique values in one or more specified key columns.</li> <li>Apply: An aggregation function (e.g., sum, mean, count) is applied to each group independently.</li> <li>Combine: The results of the applications are combined into a new DataFrame.</li> </ul> <p>In Polars, this is primarily achieved using <code>df.group_by(\"key_column\").agg([...])</code>.</p> <ul> <li> <p>Creating a <code>GroupBy</code> Object: <code>df.group_by(\"key_column_name\")</code> or <code>df.group_by([\"col1\", \"col2\"])</code> creates a <code>GroupBy</code> object, which represents the grouped data. No computation happens at this stage.</p> </li> <li> <p>Applying Aggregations with <code>.agg()</code>:     The <code>.agg()</code> method is called on the <code>GroupBy</code> object. It takes a list of Polars expressions that define the aggregations to be performed on each group.</p> </li> <li> <p>Common Aggregation Expressions:</p> <ul> <li><code>pl.sum(\"col_name\")</code> or <code>pl.col(\"col_name\").sum()</code>: Sum of values.</li> <li><code>pl.mean(\"col_name\")</code> or <code>pl.col(\"col_name\").mean()</code>: Average of values.</li> <li><code>pl.median(\"col_name\")</code> or <code>pl.col(\"col_name\").median()</code>: Median of values.</li> <li><code>pl.min(\"col_name\")</code> or <code>pl.col(\"col_name\").min()</code>: Minimum value.</li> <li><code>pl.max(\"col_name\")</code> or <code>pl.col(\"col_name\").max()</code>: Maximum value.</li> <li><code>pl.count()</code>: Number of rows in each group. (Note: <code>pl.col(\"col_name\").count()</code> counts non-null values in that specific column within the group).</li> <li><code>pl.n_unique(\"col_name\")</code> or <code>pl.col(\"col_name\").n_unique()</code>: Number of unique values.</li> <li><code>pl.first(\"col_name\")</code> or <code>pl.col(\"col_name\").first()</code>: First value.</li> <li><code>pl.last(\"col_name\")</code> or <code>pl.col(\"col_name\").last()</code>: Last value.</li> </ul> </li> <li> <p>Examples:</p> <pre><code># Example 1: Sales summary by product_category from orders_df\ncategory_summary_df = orders_df.group_by(\"product_category\").agg([\n    pl.sum(\"quantity\").alias(\"total_quantity_sold\"),\n    pl.mean(\"unit_price\").alias(\"average_unit_price\"),\n    pl.col(\"order_id\").count().alias(\"number_of_orders\"), # Counts non-null order_ids per group\n    pl.n_unique(\"customer_id\").alias(\"unique_customers_in_category\")\n]).sort(\"total_quantity_sold\", descending=True)\n\nprint(\"Sales summary by product category:\")\nprint(category_summary_df)\n\n# Example 2: Customer demographics by city from customers_df\n# Using 'age_cleaned' which handles the outlier and converts to Int64 (allowing nulls for mean calculation)\ncity_demographics_df = customers_df.group_by(\"city\").agg([\n    pl.count().alias(\"number_of_customers\"), # Total customers per city\n    pl.mean(\"age_cleaned\").alias(\"average_age\"),\n    pl.min(\"age_cleaned\").alias(\"min_age\"),\n    pl.max(\"age_cleaned\").alias(\"max_age\")\n]).sort(\"number_of_customers\", descending=True)\n\nprint(\"\\nCustomer demographics by city:\")\nprint(city_demographics_df)\n</code></pre> </li> <li> <p>Grouping by Multiple Columns:     Pass a list of column names to <code>group_by()</code>.</p> <pre><code># Example: Total quantity of products sold by each customer for each product category\ncustomer_category_sales_df = orders_df.group_by([\"customer_id\", \"product_category\"]).agg(\n    pl.sum(\"quantity\").alias(\"total_quantity_per_customer_category\")\n).sort([\"customer_id\", \"product_category\"])\n\nprint(\"\\nTotal quantity by customer and product category:\")\nprint(customer_category_sales_df.head(10))\n</code></pre> </li> </ul>"},{"location":"eda/data-wrangling/data-aggregation/#2-pivoting-data-with-dfpivot","title":"2. Pivoting Data with <code>df.pivot()</code>","text":"<p>Pivoting is a way to reshape data from a \"long\" format to a \"wide\" format, where unique values from one column become new column headers.</p> <ul> <li> <p><code>df.pivot(values=\"values_col\", index=\"index_col\", columns=\"columns_col\", aggregate_function=None)</code>:</p> <ul> <li><code>values</code>: The column whose data will populate the new pivoted columns.</li> <li><code>index</code>: Column(s) that will remain as rows (identifying each row).</li> <li><code>columns</code>: The column whose unique values will become the new column headers.</li> <li><code>aggregate_function</code> (optional): If there are multiple <code>values</code> for a given <code>index</code> and <code>column</code> combination, this function is used to aggregate them (e.g., 'first', 'sum', 'mean'). If not specified or data is already unique for combinations, it takes the first value.</li> </ul> </li> <li> <p>Example: Create a wide table showing total quantity sold per product category for each year.     First, ensure we have an 'order_year' column.</p> <pre><code>orders_with_year_df = orders_df.with_columns(\n    pl.col(\"order_datetime\").dt.year().alias(\"order_year\")\n)\n\n# Aggregate to get total quantity per year and product category (needed for pivot if not 1-to-1)\nyearly_category_sales = orders_with_year_df.group_by([\"order_year\", \"product_category\"]).agg(\n    pl.sum(\"quantity\").alias(\"total_quantity\")\n).sort([\"order_year\", \"product_category\"])\n\nprint(\"\\nYearly sales by category (long format):\")\nprint(yearly_category_sales)\n\n# Pivot to get product categories as columns\npivoted_yearly_sales_df = yearly_category_sales.pivot(\n    values=\"total_quantity\",\n    index=\"order_year\",\n    columns=\"product_category\"\n).fill_null(0) # Fill categories with no sales in a year with 0\n\nprint(\"\\nPivoted yearly sales (product categories as columns):\")\nprint(pivoted_yearly_sales_df)\n</code></pre> <p>Business Context: This wide format can be useful for reports or visualizations where you want to compare categories side-by-side over years.</p> </li> </ul>"},{"location":"eda/data-wrangling/data-aggregation/#3-window-functions","title":"3. Window Functions","text":"<p>Window functions perform calculations across a set of table rows that are somehow related to the current row \u2013 this set is called a \"window.\" Unlike <code>group_by().agg()</code> operations which typically reduce the number of rows, window functions compute a value for each row based on its window.</p> <ul> <li> <p>Concept: Imagine you want to calculate a running total of sales, or rank products within their categories based on sales quantity, or find the difference between a customer's order amount and the average order amount for their city. These require looking at other rows related to the current one.</p> </li> <li> <p>Use Cases: Running totals, moving averages, ranking, calculations relative to a group (e.g., percentage of group total).</p> </li> <li> <p>Polars Syntax (High-Level): Window functions are often applied using the <code>.over(\"grouping_column_or_expression\")</code> method on an expression.     For example, <code>pl.col(\"sales\").sum().over(\"category\")</code> would compute, for each row, the total sales for that row's category (effectively broadcasting the group sum to each member of the group). <code>pl.col(\"sales\").rank().over(\"category\")</code> would rank sales within each category.</p> <pre><code># Illustrative example (syntax can be complex for beginners):\n# Calculate running total of quantity per product category over time\n\n# Ensure data is sorted correctly for a meaningful running total\norders_sorted_for_window = orders_with_year_df.sort([\"product_category\", \"order_datetime\"])\n\norders_with_running_total = orders_sorted_for_window.with_columns(\n    pl.col(\"quantity\").cumsum().over(\"product_category\").alias(\"running_total_quantity_in_category\")\n)\nprint(\"\\nOrders with running total quantity within category (illustrative):\")\nprint(orders_with_running_total.filter(pl.col(\"product_category\") == \"Books\").select([\n    \"order_datetime\", \"product_category\", \"quantity\", \"running_total_quantity_in_category\"\n]))\n</code></pre> <p>Note for Students: Window functions are very powerful but have a steeper learning curve. They are mentioned here for awareness. For many business reporting tasks, standard group-by aggregations are often sufficient.</p> </li> </ul>"},{"location":"eda/data-wrangling/data-exploration/","title":"Data Exploration","text":""},{"location":"eda/data-wrangling/data-exploration/#getting-data-in-data-io","title":"Getting Data In - Data IO","text":"<p>The ability to import data from various sources is often the first step in data analysis. Polars provides efficient functions to read data from common file types directly into its primary data structure, the DataFrame.</p> <p>1. Loading Data into DataFrames</p> <p>Once data is read by Polars, it is typically stored in a DataFrame. A DataFrame is a two-dimensional, labeled data structure, analogous to a table in a database or a sheet in a spreadsheet program. You will assign the loaded data to a variable to work with it.</p> <p>2. Reading Comma-Separated Values (CSV) Files</p> <p>CSV files are one of the most common formats for storing and exchanging tabular data. Each line in a CSV file typically represents a row of data, with values separated by a delimiter, most often a comma.</p> <ul> <li> <p>The <code>pl.read_csv()</code> Function:     The primary function for reading CSV files in Polars is <code>pl.read_csv()</code>. Its most basic usage requires the path to the file.</p> </li> <li> <p>Example: Loading Local CSV Files     We will use the <code>customers.csv</code> and <code>orders.csv</code>  files provided for this course. Assuming these files are in your current working directory (the same directory where your Python script or notebook is running), you can load them as follows:</p> <pre><code>import polars as pl\n\n# Load the customers dataset\ncustomers_df = pl.read_csv('customers.csv')\n\n# Load the orders dataset\norders_df = pl.read_csv('orders.csv')\n\n# You can then display the first few rows to verify\nprint(customers_df.head())\nprint(orders_df.head())\n</code></pre> </li> <li> <p>Common Parameters for <code>pl.read_csv()</code>:</p> <ul> <li><code>source</code>: The first argument, which is the file path or URL.</li> <li><code>separator</code> (or <code>sep</code>): Specifies the delimiter used in the file. While the default is a comma (<code>,</code>), you might encounter files using semicolons (<code>;</code>), tabs (<code>\\t</code>), or other characters. For example: <code>pl.read_csv('data.tsv', separator='\\t')</code>.</li> <li><code>has_header</code>: A boolean indicating whether the first row of the CSV contains column names. This defaults to <code>True</code>. If your file does not have a header row, you would set <code>has_header=False</code>.</li> <li><code>dtypes</code>: Used to specify or override the data types for columns. (More on this in the next section).</li> <li><code>try_parse_dates</code>: If set to <code>True</code>, Polars will attempt to automatically parse columns that look like dates into <code>Date</code> or <code>Datetime</code> types. This can be convenient but requires caution if date formats are inconsistent.</li> <li>There are many other parameters for handling various CSV complexities (e.g.,  <code>null_values</code> for defining strings that should be interpreted as nulls), which you can explore in the Polars documentation as needed.</li> </ul> </li> <li> <p>Specifying or Overriding Data Types (Schema) with <code>dtypes</code></p> What is a schema and why should you care? <p>In the context of data science and business analytics, a schema refers to the formal definition or blueprint of a dataset's structure. Think of it as the architectural plan for your data, outlining what the data looks like and how it's organized.</p> <p>Specifically, a schema typically defines:</p> <ol> <li>Column Names: The labels or identifiers for each piece of information in the dataset (e.g., \"CustomerID\", \"ProductName\", \"TransactionDate\", \"SalesAmount\").</li> <li>Data Types: The nature of the data stored in each column. Common data types include:<ul> <li>Integers: Whole numbers (e.g., for counts like <code>QuantityOrdered</code>).</li> <li>Floating-Point Numbers (Decimals): Numbers with decimal points (e.g., for <code>UnitPrice</code> or <code>Revenue</code>).</li> <li>Strings (Text): Alphanumeric characters (e.g., for <code>CustomerName</code> or <code>ProductDescription</code>).</li> <li>Booleans: True or False values (e.g., for <code>IsActiveCustomer</code>).</li> <li>Dates and Timestamps: Specific points in time (e.g., for <code>OrderDate</code> or <code>RegistrationTimestamp</code>).</li> <li>Categorical/Enum: A fixed set of predefined values (e.g., <code>SalesRegion</code> being \"North,\" \"South,\" \"East,\" or \"West\").</li> </ul> </li> <li>Relationships (in relational databases): How different datasets or tables link together (e.g., how a <code>CustomerID</code> in an orders table relates to a <code>CustomerID</code> in a customer details table).</li> <li>Constraints (sometimes): Rules applied to the data, such as whether a value can be missing (null), if values must be unique (like an <code>OrderID</code>), or if they must fall within a specific range.</li> </ol> <p>Why is Understanding and Defining a Schema Important in a Business Context?</p> <p>A clear and accurate schema is not merely a technical detail; it is fundamental to effective data utilization and decision-making in business for several critical reasons:</p> <ol> <li> <p>Ensuring Data Integrity and Quality:</p> <ul> <li>A schema acts as a first line of defense for data quality. By defining that <code>SalesAmount</code> must be a numerical value, any attempt to input text (e.g., \"One Thousand Dollars\") would be flagged as an error or an inconsistency that needs addressing. This prevents the \"garbage in, garbage out\" problem, leading to more reliable data.</li> <li>It helps identify data collection or entry errors early. If a <code>TransactionDate</code> column, defined as a date type, contains values that cannot be interpreted as valid dates, it signals a problem in the data pipeline.</li> </ul> </li> <li> <p>Enabling Accurate Analysis and Reporting:</p> <ul> <li>Correct data types are essential for any meaningful computation. For example, you cannot accurately calculate total revenue if the <code>SalesAmount</code> column is mistakenly treated as text. Trend analysis based on dates requires the <code>OrderDate</code> column to be a proper date type, not just a string of characters.</li> <li>It ensures that analytical operations like sums, averages, minimums, maximums, and comparisons yield correct and trustworthy results, which are the bedrock of informed business decisions.</li> </ul> </li> <li> <p>Facilitating Effective Data Integration:</p> <ul> <li>Businesses often need to combine data from disparate sources (e.g., CRM systems, sales databases, marketing analytics platforms). A clear understanding of the schema for each source is vital to correctly join or merge these datasets. Mismatched data types or misinterpretation of column meanings (e.g., joining on an ID that has different formats in two tables) can lead to failed integrations or, worse, subtly incorrect combined datasets.</li> </ul> </li> <li> <p>Supporting System Compatibility and Interoperability:</p> <ul> <li>Data frequently moves between different software systems\u2014from operational databases to data warehouses, analytics tools (like Polars), and business intelligence dashboards. A well-defined schema ensures that data is interpreted consistently across these platforms, maintaining its meaning and usability.</li> <li>For regulatory reporting or data exchange with partners, adherence to a predefined schema is often a requirement.</li> </ul> </li> <li> <p>Improving Efficiency in Data Processing:</p> <ul> <li>When data processing tools and databases are aware of the schema, they can optimize data storage and query execution. For example, operations on numerical data are generally faster and more memory-efficient than on text data if the system knows the type in advance.</li> <li>When loading data, explicitly defining the schema can significantly speed up the import process and reduce memory consumption, as the system doesn't need to infer data types.</li> </ul> </li> <li> <p>Promoting Clear Communication and Collaboration:</p> <ul> <li>The schema serves as a common language and a point of reference for all stakeholders involved with the data\u2014from data engineers and analysts to business managers. It provides clear documentation on what each data field represents and its expected format.</li> </ul> </li> <li> <p>Forming a Basis for Data Governance:</p> <ul> <li>Schema definitions are integral to data governance frameworks. These frameworks establish policies for data quality, security, and compliance. A well-understood schema helps enforce these policies and manage data as a valuable organizational asset.</li> </ul> </li> </ol> <p>In essence, the schema provides the necessary structure and rules that allow businesses to transform raw data into reliable information, which in turn supports insightful analysis and strategic decision-making. Neglecting the schema can lead to significant errors, inefficiencies, and ultimately, flawed business intelligence.</p> <p>While Polars does a good job of inferring data types automatically, there are several reasons why you might want to explicitly define the schema when reading a CSV:</p> <ul> <li>Correctness: To ensure columns are interpreted as the intended data type (e.g., an ID column that looks numeric but should be treated as a string, or ensuring a specific precision for floating-point numbers).</li> <li>Performance: Specifying types can sometimes speed up parsing, especially for very large files, as Polars doesn't have to guess.</li> <li>Memory Efficiency: You can choose more memory-efficient types if appropriate (e.g., <code>pl.Int32</code> instead of <code>pl.Int64</code> if the numbers in a column are known to be small enough).</li> <li>Handling Ambiguity: For columns with mixed-format dates or numbers that could be misinterpreted, specifying the type (often as <code>pl.Utf8</code> initially, for later custom parsing) provides control.</li> </ul> <p>The <code>dtypes</code> parameter in <code>pl.read_csv()</code> accepts a dictionary where keys are column names and values are Polars data types.</p> <p>Example: Specifying <code>dtypes</code> for <code>customers.csv</code></p> <p>Let's load <code>customers.csv</code> ([original file: <code>customers.csv</code>]) again, this time specifying data types for some columns:</p> <pre><code>import polars as pl\n\n# Define the desired schema for selected columns\ncustomer_schema = {\n    'customer_id': pl.Int32,        # Specify as 32-bit integer\n    'name': pl.Utf8,                # Explicitly Utf8 (string)\n    'registration_date': pl.Utf8,   # Read as string; parsing will be handled later due to mixed formats\n    'city': pl.Categorical,         # Use Categorical for columns with repetitive string values\n    'age': pl.Float32               # Use Float32 for age (e.g., if non-integer ages were possible or for consistency)\n}\n\n# Load the customers dataset with the specified schema\ncustomers_custom_schema_df = pl.read_csv('customers.csv', dtypes=customer_schema)\n\nprint(customers_custom_schema_df.dtypes)\nprint(customers_custom_schema_df.head())\n</code></pre> <p>Common Polars Data Types for Schema Definition:</p> <ul> <li>Integers: <code>pl.Int8</code>, <code>pl.Int16</code>, <code>pl.Int32</code>, <code>pl.Int64</code> (default for integers)</li> <li>Unsigned Integers: <code>pl.UInt8</code>, <code>pl.UInt16</code>, <code>pl.UInt32</code>, <code>pl.UInt64</code></li> <li>Floats: <code>pl.Float32</code>, <code>pl.Float64</code> (default for floats)</li> <li>Strings: <code>pl.Utf8</code></li> <li>Booleans: <code>pl.Boolean</code></li> <li>Dates/Times: <code>pl.Date</code>, <code>pl.Datetime</code>, <code>pl.Duration</code>, <code>pl.Time</code></li> <li>Categorical: <code>pl.Enum</code>, <code>pl.Categorical</code> (for columns with a limited set of unique string values)</li> </ul> <p>For a comprehensive list, refer to the official Polars documentation on data types. Note that for complex date/time parsing directly within <code>read_csv</code> when formats are inconsistent, it's often more robust to read the column as <code>pl.Utf8</code> and then use Polars' specialized string-to-date conversion functions (covered in Module 5). The <code>try_parse_dates=True</code> parameter can be an alternative for simpler cases.</p> </li> </ul> <p>3. Reading CSV Data from a URL</p> <p>Polars can also read CSV files directly from a URL, which is convenient for accessing publicly available datasets without needing to download them manually.</p> <ul> <li> <p>Example: Loading a CSV from a URL     The <code>pl.read_csv()</code> function seamlessly handles URLs.</p> <pre><code>import polars as pl\n\n# Example URL for a public CSV file (ensure this URL is active and accessible)\n# This is a sample dataset of an airline's monthly passengers.\nairline_passengers_url = 'https://raw.githubusercontent.com/plotly/datasets/master/airline-passengers.csv'\n\n# Load data from the URL\nairline_passengers_df = pl.read_csv(airline_passengers_url)\n\nprint(airline_passengers_df.head())\n</code></pre> <p>Note for students: When working with data from URLs, ensure you have an active internet connection. The availability and content of external URLs can change.</p> </li> </ul> <p>4. Reading JSON Files</p> <p>JSON (JavaScript Object Notation) is another widely used format for data interchange, especially for web-based data and APIs. Polars can read JSON files, typically those structured as a list of records (where each record is an object with key-value pairs).</p> <ul> <li> <p>The <code>pl.read_json()</code> Function:     This function is used to read data from a JSON file. For the common \"list of records\" structure, Polars can infer the schema effectively.</p> </li> <li> <p>Example: Loading a JSON File     Suppose you have a JSON file named <code>product_inventory.json</code> in your working directory with the following content:</p> <pre><code>[\n  {\"product_id\": \"P1001\", \"product_name\": \"Laptop X1\", \"category\": \"Electronics\", \"stock_level\": 150, \"reorder_point\": 50},\n  {\"product_id\": \"P1002\", \"product_name\": \"Wireless Mouse\", \"category\": \"Electronics\", \"stock_level\": 300, \"reorder_point\": 100},\n  {\"product_id\": \"P1003\", \"product_name\": \"Office Chair Pro\", \"category\": \"Furniture\", \"stock_level\": 85, \"reorder_point\": 30},\n  {\"product_id\": \"P1004\", \"product_name\": \"Standing Desk\", \"category\": \"Furniture\", \"stock_level\": 60, \"reorder_point\": 20}\n]\n</code></pre> <p>To load this file into a Polars DataFrame:</p> <pre><code>import polars as pl\n\n# Assuming 'product_inventory.json' with the content above exists in your working directory.\n# If you are following along, you can create this file manually.\ntry:\n    inventory_df = pl.read_json('product_inventory.json')\n    # print(inventory_df.head())\nexcept FileNotFoundError:\n    print(\"Ensure 'product_inventory.json' exists in your working directory with the sample content.\")\nexcept Exception as e: # Polars might raise a different exception if JSON is malformed or empty\n    print(f\"An error occurred: {e}\")\n</code></pre> <p>Note for students: The <code>pl.read_json()</code> function can also read JSON lines format if you specify <code>json_lines=True</code>. JSON structures can vary; <code>pl.read_json()</code> is optimized for array-of-objects or JSON lines formats. For more complex nested JSON, further parsing might be required after initial loading.</p> </li> </ul> <p>5. A Note on Other Data Formats</p> <p>While CSV and JSON are common, Polars supports reading various other file formats, including:</p> <ul> <li>Parquet (<code>pl.read_parquet()</code>): A columnar storage format popular in Big Data ecosystems, known for efficiency.</li> <li>Excel (<code>pl.read_excel()</code>): For reading data from <code>.xlsx</code> or <code>.xls</code> files (often requires an additional engine like <code>xlsx2csv</code> or <code>openpyxl</code>).</li> <li>Database connections: Polars can also read from SQL databases.</li> </ul> <p>These options provide flexibility in accessing data from diverse enterprise sources. For this course, we will primarily focus on CSV and JSON.</p>"},{"location":"eda/data-wrangling/data-exploration/#first-look-initial-data-exploration","title":"First Look - Initial Data Exploration","text":"<p>The data exploration step is where an analyst develops a deeper understanding of the structural characteristics of the ingested data, identifies the sequence of casting, transformation, and cleaning tasks that need to be performed, and implements the identified changes. This section will guide you through the fundamental Polars functions and techniques used to inspect and understand your DataFrames. We will primarily use the <code>customers_df</code> and <code>orders_df</code> DataFrames. Ensure you have these DataFrames available.</p> <pre><code># If you are using a local or non-Colab development environment,\n# you may need to run the code below on your notebook or use a \n# dependency manager to install polars to your environment\n# \n# !pip install polars\n#\n# Prerequisite: Ensure Polars is imported and your DataFrames are loaded.\nimport polars as pl\n\n# Assuming 'customers.csv' and 'orders.csv' are in the current working directory:\ncustomers_df = pl.read_csv('customers.csv')\norders_df = pl.read_csv('orders.csv')\n</code></pre> Error handling code <p>If you stored your data files on the session stroage previously and  encountered <code>NameError</code>, try the code below:</p> <pre><code># For the examples below, we'll assume customers_df and orders_df are already loaded.\n# If you are starting a new session, uncomment and run the lines above.\n# To make examples runnable, let's create placeholder DataFrames if they don't exist.\n# In a real notebook, these would be loaded from files as in Module 2.\ntry:\n    _ = customers_df.shape # Check if exists\nexcept NameError:\n    print(\"Placeholder: customers_df not loaded. Creating a minimal example.\")\n    customers_df = pl.DataFrame({\n        \"customer_id\": [101, 102, 107, 110, 104],\n        \"name\": [\"Alice Wonderland\", \"Bob The Builder\", \"George Jetson\", \"Jane Doe\", \"Diana Prince\"],\n        \"registration_date\": [\"2022-01-15\", \"2022-03-22\", None, \"2023-07-21\", \"2022-07-01\"],\n        \"city\": [\"New York\", \"London\", \"Paris\", \"New York\", \"New York\"],\n        \"age\": [28, 35, 50, None, 3000]\n    }).with_columns(pl.col(\"age\").cast(pl.Int64, strict=False)) # Ensure age is int for describe\n\ntry:\n    _ = orders_df.shape # Check if exists\nexcept NameError:\n    print(\"Placeholder: orders_df not loaded. Creating a minimal example.\")\n    orders_df = pl.DataFrame({\n        \"order_id\": [201, 202, 203, 207, 208],\n        \"customer_id\": [101, 102, 101, 105, 101],\n        \"order_date\": [\"2023-01-20 10:30:00\", \"2023-02-15 11:05:30\", \"2023-02-28 14:12:55\", \"2023-04-22 16:20:30\", \"2023-05-01 10:00:00\"],\n        \"product_category\": [\"Books\", \"Tools\", \"Electronics\", \"Electronics\", \"Books\"],\n        \"quantity\": [2, 1, 1, None, 3],\n        \"unit_price\": [15.99, 199.50, 799.00, 99.99, 10.00],\n        \"discount_applied\": [0.05, 0.10, None, 0.05, None]\n    }).with_columns([\n        pl.col(\"quantity\").cast(pl.Int64, strict=False),\n        pl.col(\"unit_price\").cast(pl.Float64, strict=False),\n        pl.col(\"discount_applied\").cast(pl.Float64, strict=False)\n    ])\n</code></pre> <p>1. Understanding DataFrame Structure: Dimensions</p> <p>The <code>.shape</code> attribute remains the direct way to get dimensions.</p> <ul> <li> <p><code>.shape</code> Attribute:     Returns a tuple representing the dimensions of the DataFrame: <code>(number_of_rows, number_of_columns)</code>.</p> <pre><code># Get the dimensions of the customers DataFrame\ncustomer_dimensions = customers_df.shape\nprint(f\"Customers DataFrame - Rows: {customer_dimensions[0]}, Columns: {customer_dimensions[1]}\")\n\n# Get the dimensions of the orders DataFrame\norder_dimensions = orders_df.shape\nprint(f\"Orders DataFrame - Rows: {order_dimensions[0]}, Columns: {order_dimensions[1]}\")\n</code></pre> </li> </ul> <p>2. Inspecting Data Content: First and Last Rows</p> <p><code>.head()</code> and <code>.tail()</code> are direct DataFrame methods for quick data previews.</p> <ul> <li> <p><code>.head(n)</code> Method:     Displays the first <code>n</code> rows. Defaults to 5.</p> <pre><code>print(\"First 3 rows of customers_df:\")\nprint(customers_df.head(3))\n</code></pre> </li> <li> <p><code>.tail(n)</code> Method:     Displays the last <code>n</code> rows. Defaults to 5.</p> <pre><code>print(\"\\nLast 3 rows of orders_df:\")\nprint(orders_df.tail(3))\n</code></pre> </li> </ul> <p>3. Examining Data Types and Schema</p> <p><code>.dtypes</code> and <code>.schema</code> are attributes for metadata inspection.</p> <ul> <li> <p><code>.dtypes</code> Attribute:     Returns a list of <code>DataType</code> for each column.</p> <pre><code>print(\"Data types for customers_df:\")\nprint(customers_df.dtypes)\n</code></pre> </li> <li> <p><code>.schema</code> Attribute:     Provides a structured view of column names and their <code>DataType</code>.</p> <pre><code>print(\"\\nSchema for orders_df:\")\nprint(orders_df.schema)\n</code></pre> </li> </ul> <p>4. Obtaining Descriptive Statistics and a Quick Overview</p> <p><code>.describe()</code> and <code>.glimpse()</code> are higher-level DataFrame methods for summary.</p> <ul> <li> <p><code>.describe()</code> Method:     Computes summary statistics for numerical columns.</p> <pre><code>print(\"Descriptive statistics for orders_df:\")\nprint(orders_df.describe())\n</code></pre> <p>Self-reflection: Observe the <code>null_count</code> row in the describe output. This is another way to spot missing numerical data.</p> </li> <li> <p><code>.glimpse()</code> Method:     Provides a transposed, concise summary of column names, types, and initial values.</p> <pre><code>print(\"\\nGlimpse of customers_df:\")\ncustomers_df.glimpse()\n\nprint(\"\\nGlimpse of orders_df:\")\norders_df.glimpse()\n</code></pre> </li> </ul> <p>5. Identifying and Quantifying Missing Values using Expressions</p> <p>Missing data is a common issue. Polars expressions allow for flexible ways to identify and count nulls.</p> <ul> <li> <p>Using <code>pl.all().is_null().sum()</code> within <code>select</code>:     To count null values for all columns simultaneously using an explicit expression, you can use <code>pl.all()</code> to refer to all columns, apply <code>is_null()</code> to each, and then <code>sum()</code> the boolean results (where <code>True</code> is 1 and <code>False</code> is 0).</p> <pre><code># Count missing values in each column of customers_df using expressions\nmissing_customers_expr = customers_df.select(\n    pl.all().is_null().sum()\n)\nprint(\"Missing values per column in customers_df (via expression):\")\nprint(missing_customers_expr)\n\n# Count missing values in each column of orders_df using expressions\nmissing_orders_expr = orders_df.select(\n    pl.all().is_null().sum()\n)\nprint(\"\\nMissing values per column in orders_df (via expression):\")\nprint(missing_orders_expr)\n</code></pre> <p>This approach demonstrates how expressions can operate across multiple columns. The result is a new DataFrame where each column represents the sum of nulls for the corresponding column in the original DataFrame.</p> </li> </ul> <p>6. Analyzing Unique Values in Columns using Expressions</p> <p>Understanding unique values helps characterize your data.</p> <ul> <li> <p>Counting Unique Values with <code>pl.col().n_unique()</code>:     To get the number of unique values in one or more specific columns, use the <code>pl.col(\"column_name\").n_unique()</code> expression within a <code>select</code> context.</p> <pre><code># Get the number of unique values in the 'city' column of customers_df\nunique_cities_count_expr = customers_df.select(\n    pl.col('city').n_unique().alias(\"unique_city_count\")\n)\nprint(\"\\nNumber of unique cities in customers_df (via expression):\")\nprint(unique_cities_count_expr)\n\n# Get the number of unique values for 'product_category' and 'customer_id' in orders_df\nunique_counts_orders_expr = orders_df.select([\n    pl.col('product_category').n_unique().alias(\"unique_product_categories\"),\n    pl.col('customer_id').n_unique().alias(\"unique_customer_ids_in_orders\")\n])\nprint(\"\\nNumber of unique product categories and customer IDs in orders_df (via expression):\")\nprint(unique_counts_orders_expr)\n</code></pre> <p>Using <code>.alias()</code> within the expression allows you to name the resulting column in the output DataFrame.</p> </li> <li> <p>Retrieving Unique Values with <code>pl.col().unique()</code>:     To retrieve the actual unique values from a specific column, use the <code>pl.col(\"column_name\").unique()</code> expression.</p> <pre><code># Get the unique values in the 'product_category' column of orders_df\nunique_product_categories_expr = orders_df.select(\n    pl.col('product_category').unique().sort() # .sort() is optional, for consistent order\n)\nprint(\"\\nUnique product categories in orders_df (via expression):\")\nprint(unique_product_categories_expr)\n\n# Get unique cities from customers_df\nunique_cities_expr = customers_df.select(\n    pl.col('city').unique().sort() # .sort() is optional\n)\nprint(\"\\nUnique cities in customers_df (via expression):\")\nprint(unique_cities_expr)\n</code></pre> <p>The result of <code>pl.col().unique()</code> is a column containing only the unique, non-null values.</p> </li> </ul>"},{"location":"eda/data-wrangling/data-transformation/","title":"Data Transformation","text":"<p>We'll use the <code>customers_df</code> and <code>orders_df</code> DataFrames. For realistic examples, especially for date parsing and missing value handling, ensure these DataFrames reflect the structure and \"messiness\" of the original CSV files.</p> Prerequisite <pre><code># Prerequisites: Ensure Polars is imported and selectors if used.\nimport polars as pl\nimport polars.selectors as cs # For using column selectors like cs.numeric()\n\n# --- Placeholder DataFrames (mimicking loaded CSVs for standalone module execution) ---\n# These should reflect the structure from your customers.csv and orders.csv,\n# including mixed date formats, missing values, etc.\n\ncustomers_df = pl.DataFrame({\n    \"customer_id\": [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115],\n    \"name\": [\"Alice Wonderland\", \"Bob The Builder\", \"Charlie Brown\", \"Diana Prince\", \"Evan Almighty\", \"Fiona Gallagher\", \"George Jetson\", \"Hannah Montana\", \"Ian Malcolm\", \"Jane Doe\", \"Kevin McCallister\", \"Laura Palmer\", \"Michael Scott\", \"Nancy Drew\", \"Oscar Grouch\"],\n    \"registration_date_str\": [\"2022-01-15\", \"2022-03-22\", \"2022-05-10\", \"2022-07-01\", \"2022-08-19\", \"2023-01-20\", None, \"2023-04-05\", \"2023-06-12\", \"2023-07-21\", \"2023-09-01\", \"2023-10-15\", \"2024-02-10\", \"03/15/2024\", \"2024-05-01\"],\n    \"city\": [\"New York\", \"London\", \"Paris\", \"New York\", \"London\", \"New York\", \"Paris\", \"Berlin\", \"London\", \"New York\", \"Chicago\", \"Twin Peaks\", \"Scranton\", \"River Heights\", \"New York\"],\n    \"age\": [28, 35, 45, 3000, 42, 29, 50, 22, 55, None, 12, 17, 48, 18, 60]\n}).with_columns(pl.col(\"age\").cast(pl.Int64, strict=False)) # Cast age to Int64, allowing nulls\n\norders_df = pl.DataFrame({\n    \"order_id\": [201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218],\n    \"customer_id\": [101, 102, 101, 103, 104, 102, 105, 101, 106, 108, 103, 107, 110, 111, 102, 113, 101, 115],\n    \"order_date_str\": [\"2023-01-20 10:30:00\", \"2023-02-15 11:05:30\", \"2023-02-28 14:12:55\", \"2023-03-10 09:00:15\", \"2023-03-12 17:45:00\", \"2023-04-05 12:00:00\", \"2023-04-22 16:20:30\", \"2023-05-01 10:00:00\", \"2023-05-15 08:55:10\", \"2023-06-01 11:30:00\", \"2023-06-20 13:40:00\", \"2023-07-01 00:00:00\", \"2023-07-25 10:10:10\", \"2023-09-10 14:20:30\", \"2023-11-05 19:00:00\", \"2024-02-15 09:30:00\", \"2024-03-01 10:00:00\", \"2024-05-05 12:12:12\"],\n    \"product_category\": [\"Books\", \"Tools\", \"Electronics\", \"Home Goods\", \"Antiques\", \"Books\", \"Electronics\", \"Books\", \"Beauty\", \"Music\", \"Home Goods\", \"Electronics\", \"Clothing\", \"Toys\", \"Tools\", \"Office Supplies\", \"Electronics\", \"Home Goods\"],\n    \"quantity\": [2, 1, 1, 3, 1, 1, None, 3, 2, 5, 1, 1, 2, 3, 1, 10, 1, 1],\n    \"unit_price\": [15.99, 199.50, 799.00, 25.00, 2500.00, 12.50, 99.99, 10.00, 45.75, 9.99, 150.00, 499.50, 75.00, 29.99, 75.00, 4.99, 1200.00, 12.00],\n    \"discount_applied\": [0.05, 0.10, None, 0.0, 0.15, 0.0, 0.05, None, 0.10, 0.0, 0.05, 0.10, None, 0.05, 0.0, 0.0, 0.15, 0.0]\n}).with_columns([\n    pl.col(\"quantity\").cast(pl.Int64, strict=False), # Ensure numeric types, allowing nulls\n    pl.col(\"unit_price\").cast(pl.Float64, strict=False),\n    pl.col(\"discount_applied\").cast(pl.Float64, strict=False)\n])\n</code></pre> <p>Transforming Data - Selection, Filtering, and Modification</p> <p>This module covers the primary operations for transforming data: selecting specific columns, creating or modifying columns, filtering rows, changing data types, handling missing values, and sorting data. These operations are fundamental to preparing data for analysis.</p>"},{"location":"eda/data-wrangling/data-transformation/#1-introduction-to-data-transformation-contexts","title":"1. Introduction to Data Transformation Contexts","text":"<p>As previewed in Module 4, Polars expressions are executed within specific DataFrame methods (contexts):</p> <ul> <li><code>select()</code>: For choosing, renaming, or deriving a specific set of columns.</li> <li><code>with_columns()</code>: For adding new columns or modifying existing ones, keeping all other columns.</li> <li><code>filter()</code>: For selecting a subset of rows based on conditions.</li> </ul>"},{"location":"eda/data-wrangling/data-transformation/#2-column-selection-and-manipulation-with-select","title":"2. Column Selection and Manipulation with <code>select</code>","text":"<p>The <code>select()</code> method is used when you want to create a new DataFrame with a specific subset or transformation of columns from an existing DataFrame.</p> <ul> <li> <p>Selecting Specific Columns:     Provide a list of column names (as strings) or <code>pl.col()</code> expressions.</p> <pre><code># Select only name and city from customers_df\ncustomer_locations_df = customers_df.select([\n    pl.col(\"name\"),\n    pl.col(\"city\")\n])\nprint(\"Selected customer names and cities:\")\nprint(customer_locations_df.head())\n</code></pre> </li> <li> <p>Renaming Columns using <code>alias()</code>:     The <code>.alias()</code> method is used within an expression to rename a column in the output.</p> <pre><code># Select customer_id and rename it to 'ID', select name and rename to 'Customer Name'\nrenamed_customers_df = customers_df.select([\n    pl.col(\"customer_id\").alias(\"ID\"),\n    pl.col(\"name\").alias(\"Customer Name\")\n])\nprint(\"\\nCustomers DataFrame with renamed columns:\")\nprint(renamed_customers_df.head())\n</code></pre> </li> <li> <p>Using Polars Selectors:     Polars offers powerful column selectors via the <code>cs</code> module (imported as <code>import polars.selectors as cs</code>). These allow selection based on patterns or data types.</p> <pre><code># Select all string columns from orders_df\nstring_cols_orders_df = orders_df.select(cs.string())\nprint(\"\\nString columns from orders_df:\")\nprint(string_cols_orders_df.head())\n\n# Select all numeric columns from customers_df\nnumeric_cols_customers_df = customers_df.select(cs.numeric())\nprint(\"\\nNumeric columns from customers_df:\")\nprint(numeric_cols_customers_df.head())\n\n# Select columns starting with 'order'\norder_related_cols_df = orders_df.select(cs.starts_with(\"order_\"))\nprint(\"\\nColumns starting with 'order_' from orders_df:\")\nprint(order_related_cols_df.head())\n</code></pre> <p>Selectors are very useful for DataFrames with many columns.</p> </li> </ul>"},{"location":"eda/data-wrangling/data-transformation/#3-adding-or-modifying-columns-with-with_columns","title":"3. Adding or Modifying Columns with <code>with_columns</code>","text":"<p>When you want to add new columns or change existing ones while keeping all other columns, <code>with_columns()</code> is the appropriate method. It takes a list of expressions.</p> <ul> <li> <p>Creating New Columns:     Define an expression and assign it a name using <code>.alias()</code>.</p> <pre><code># Add a 'total_price' column to orders_df (quantity * unit_price)\n# Also, add a column indicating if a discount was applied.\norders_enhanced_df = orders_df.with_columns([\n    (pl.col(\"quantity\") * pl.col(\"unit_price\")).alias(\"total_price\"),\n    pl.col(\"discount_applied\").is_not_null().alias(\"has_discount\") # True if discount_applied is not null\n])\nprint(\"\\nOrders DataFrame with 'total_price' and 'has_discount' columns:\")\nprint(orders_enhanced_df.head())\n</code></pre> </li> <li> <p>Modifying Existing Columns:     If an expression produces a column with an existing name, it replaces that column.</p> <pre><code># Convert customer names to uppercase\ncustomers_upper_name_df = customers_df.with_columns(\n    pl.col(\"name\").str.to_uppercase().alias(\"name\") # .alias(\"name\") ensures it replaces the 'name' column\n)\n# Alternatively, if alias matches existing name, it overwrites:\n# customers_upper_name_df = customers_df.with_columns(\n#     pl.col(\"name\").str.to_uppercase()\n# )\nprint(\"\\nCustomers DataFrame with names in uppercase:\")\nprint(customers_upper_name_df.head())\n</code></pre> </li> </ul>"},{"location":"eda/data-wrangling/data-transformation/#4-filtering-rows-with-filter","title":"4. Filtering Rows with <code>filter</code>","text":"<p>The <code>filter()</code> method is used to select rows that meet certain criteria, defined by one or more boolean expressions.</p> <ul> <li> <p>Filtering Based on Single Conditions:</p> <pre><code># Filter orders for 'Electronics' category\nelectronics_orders_df = orders_df.filter(\n    pl.col(\"product_category\") == pl.lit(\"Electronics\")\n)\nprint(\"\\nElectronics orders:\")\nprint(electronics_orders_df.head())\n\n# Filter customers older than 50 (excluding Diana Prince due to outlier age)\n# First, let's handle the outlier age for a more realistic filter example\ncustomers_cleaned_age_df = customers_df.with_columns(\n    pl.when(pl.col(\"age\") &gt; 100).then(None).otherwise(pl.col(\"age\")).alias(\"age_cleaned\")\n)\nolder_customers_df = customers_cleaned_age_df.filter(\n    pl.col(\"age_cleaned\") &gt; pl.lit(50)\n)\nprint(\"\\nCustomers older than 50 (age cleaned):\")\nprint(older_customers_df.select([\"name\", \"age_cleaned\", \"city\"]))\n</code></pre> </li> <li> <p>Combining Conditions using <code>&amp;</code> (AND) and <code>|</code> (OR):     Wrap individual conditions in parentheses when combining them.</p> <pre><code># Filter orders for 'Books' category with quantity &gt; 1\nspecific_book_orders_df = orders_df.filter(\n    (pl.col(\"product_category\") == pl.lit(\"Books\")) &amp; (pl.col(\"quantity\") &gt; pl.lit(1))\n)\nprint(\"\\nBook orders with quantity &gt; 1:\")\nprint(specific_book_orders_df)\n\n# Filter customers from 'New York' OR 'Paris'\nny_paris_customers_df = customers_df.filter(\n    (pl.col(\"city\") == pl.lit(\"New York\")) | (pl.col(\"city\") == pl.lit(\"Paris\"))\n)\nprint(\"\\nCustomers from New York or Paris:\")\nprint(ny_paris_customers_df.select([\"name\", \"city\"]).head())\n</code></pre> </li> <li> <p>Using <code>is_in()</code> for Multiple Values:     To filter rows where a column's value is one of several specified values.</p> <pre><code># Filter customers from 'London', 'Berlin', or 'Chicago'\nselected_cities = [\"London\", \"Berlin\", \"Chicago\"]\ncustomers_from_selected_cities_df = customers_df.filter(\n    pl.col(\"city\").is_in(selected_cities)\n)\nprint(f\"\\nCustomers from {selected_cities}:\")\nprint(customers_from_selected_cities_df.select([\"name\", \"city\"]))\n</code></pre> </li> </ul>"},{"location":"eda/data-wrangling/data-transformation/#5-data-type-conversion-casting","title":"5. Data Type Conversion (Casting)","text":"<p>Often, data is not in the correct type (e.g., numbers read as strings, dates as strings). Use <code>pl.col().cast(DataType)</code> within <code>with_columns</code> to convert types.</p> <ul> <li> <p>Common Numeric and Boolean Types:</p> <pre><code># Example: Suppose 'customer_id' was read as string and we want it as integer\n# For our placeholder, it's already int. Let's imagine converting age to Float for some reason.\ncustomers_age_float_df = customers_df.with_columns(\n    pl.col(\"age\").cast(pl.Float64).alias(\"age_float\")\n)\nprint(\"\\nCustomers DataFrame with age as Float64:\")\nprint(customers_age_float_df.select([\"name\", \"age_float\"]).head())\n</code></pre> </li> <li> <p>String to <code>pl.Categorical</code>:     Useful for columns with a limited number of unique string values. Can improve performance and memory usage.</p> <pre><code>customers_city_categorical_df = customers_df.with_columns(\n    pl.col(\"city\").cast(pl.Categorical).alias(\"city_categorical\")\n)\nprint(\"\\nCustomers DataFrame with city as Categorical:\")\nprint(customers_city_categorical_df.select([\"name\", \"city_categorical\"]).head())\nprint(f\"Data type of 'city_categorical': {customers_city_categorical_df.get_column('city_categorical').dtype}\")\n</code></pre> </li> <li> <p>String to <code>pl.Enum</code> (for fixed, known categories):     Enums are stricter than Categoricals. You define the set of allowed values. This is useful for data validation.</p> <pre><code># Define the allowed product categories for an Enum\n# Let's say these are the only valid primary categories we expect for a report\nallowed_categories = [\"Books\", \"Electronics\", \"Home Goods\", \"Tools\", \"Clothing\"]\nProductEnum = pl.Enum(allowed_categories)\n\n# Attempt to cast 'product_category' to this Enum.\n# Values not in ProductEnum will become null if strict=False (default), or error if strict=True in cast.\n# For demonstration, let's use with_columns and handle potential nulls or filter them.\norders_enum_df = orders_df.with_columns(\n    pl.col(\"product_category\").cast(ProductEnum, strict=False).alias(\"category_enum\")\n)\nprint(\"\\nOrders DataFrame with product_category as Enum (non-matching become null):\")\nprint(orders_enum_df.select([\"product_category\", \"category_enum\"]).head(10))\nprint(f\"Data type of 'category_enum': {orders_enum_df.get_column('category_enum').dtype}\")\n# You might filter out nulls if they represent unexpected categories:\n# valid_category_orders = orders_enum_df.filter(pl.col(\"category_enum\").is_not_null())\n</code></pre> </li> <li> <p>String to Datetime/Date:     Polars provides powerful string parsing capabilities.</p> <ul> <li> <p>Standard Format (<code>.str.to_datetime()</code>): For <code>orders_df.order_date_str</code> which is in <code>YYYY-MM-DD HH:MM:SS</code>.</p> <pre><code>orders_parsed_dates_df = orders_df.with_columns(\n    pl.col(\"order_date_str\").str.to_datetime(format=\"%Y-%m-%d %H:%M:%S\").alias(\"order_datetime\")\n)\nprint(\"\\nOrders DataFrame with parsed order_datetime:\")\nprint(orders_parsed_dates_df.select([\"order_id\", \"order_datetime\"]).head())\nprint(f\"Data type of 'order_datetime': {orders_parsed_dates_df.get_column('order_datetime').dtype}\")\n</code></pre> </li> <li> <p>Handling Mixed Date Formats (<code>.str.to_date()</code> and <code>pl.coalesce()</code>):     The <code>customers_df.registration_date_str</code> has mixed formats (<code>YYYY-MM-DD</code> and <code>MM/DD/YYYY</code>) and nulls. We can try parsing with multiple formats and use <code>pl.coalesce</code> to pick the first successful parse.</p> <pre><code>customers_parsed_reg_dates_df = customers_df.with_columns(\n    pl.coalesce([\n        pl.col(\"registration_date_str\").str.to_date(format=\"%Y-%m-%d\", strict=False),\n        pl.col(\"registration_date_str\").str.to_date(format=\"%m/%d/%Y\", strict=False)\n    ]).alias(\"registration_date\")\n)\nprint(\"\\nCustomers DataFrame with parsed registration_date (handling mixed formats):\")\nprint(customers_parsed_reg_dates_df.select([\"name\", \"registration_date_str\", \"registration_date\"]))\nprint(f\"Data type of 'registration_date': {customers_parsed_reg_dates_df.get_column('registration_date').dtype}\")\n</code></pre> <p><code>strict=False</code> allows parsing to return <code>null</code> on failure for a given format, letting <code>coalesce</code> try the next one.</p> </li> </ul> </li> </ul>"},{"location":"eda/data-wrangling/data-transformation/#6-handling-missing-values","title":"6. Handling Missing Values","text":"<ul> <li> <p>Filling Nulls with <code>fill_null()</code>:     You can fill missing values using a literal value or a defined strategy.</p> <pre><code># Fill missing 'age' in customers_df with the mean age (after cleaning the outlier)\n# For simplicity here, let's first ensure 'age_cleaned' exists as from filter example\nif \"age_cleaned\" not in customers_cleaned_age_df.columns:\n     customers_cleaned_age_df = customers_cleaned_age_df.with_columns(\n        pl.when(pl.col(\"age\") &gt; 100).then(None).otherwise(pl.col(\"age\")).cast(pl.Float64).alias(\"age_cleaned\") # Cast to float for mean\n    )\n\nmean_age_val = customers_cleaned_age_df.select(pl.col(\"age_cleaned\").mean()).item() # Get the actual mean value\n\ncustomers_filled_age_df = customers_cleaned_age_df.with_columns(\n    pl.col(\"age_cleaned\").fill_null(mean_age_val).alias(\"age_filled_mean\")\n)\n# Alternative using strategy string (Polars will compute the mean internally)\n# customers_filled_age_df = customers_cleaned_age_df.with_columns(\n#     pl.col(\"age_cleaned\").fill_null(strategy=\"mean\").alias(\"age_filled_mean_strategy\")\n# )\nprint(f\"\\nCustomers DataFrame with 'age_cleaned' nulls filled with mean ({mean_age_val:.2f}):\")\nprint(customers_filled_age_df.filter(customers_df[\"age\"].is_null()).select([\"name\", \"age\", \"age_cleaned\", \"age_filled_mean\"]))\n\n\n# Fill missing 'discount_applied' in orders_df with 0\norders_filled_discount_df = orders_df.with_columns(\n    pl.col(\"discount_applied\").fill_null(0.0).alias(\"discount_applied_filled\")\n)\nprint(\"\\nOrders DataFrame with 'discount_applied' nulls filled with 0:\")\nprint(orders_filled_discount_df.filter(pl.col(\"discount_applied\").is_null()).select([\"order_id\", \"discount_applied\", \"discount_applied_filled\"]).head())\n</code></pre> </li> <li> <p>Dropping Rows with Nulls using <code>drop_nulls()</code>:     Use with caution as it involves data loss. The <code>subset</code> parameter can specify columns to consider for nulls.</p> <pre><code># Drop rows from orders_df where 'quantity' is null\norders_dropped_null_quantity_df = orders_df.drop_nulls(subset=[\"quantity\"])\nprint(f\"\\nOriginal orders count: {orders_df.height}, After dropping null quantity: {orders_dropped_null_quantity_df.height}\")\n\n# Drop rows from customers_df if 'registration_date_str' OR 'age' is null\n# customers_dropped_any_df = customers_df.drop_nulls(subset=[\"registration_date_str\", \"age\"])\n# print(f\"\\nOriginal customers count: {customers_df.height}, After dropping if reg_date or age is null: {customers_dropped_any_df.height}\")\n</code></pre> <p>Business Context: Discuss implications: Dropping an order because quantity is missing might be acceptable if quantity is essential for all analyses. Dropping a customer due to a missing registration date might be undesirable if other customer information is valuable.</p> </li> </ul>"},{"location":"eda/data-wrangling/data-transformation/#7-sorting-data-with-sort","title":"7. Sorting Data with <code>sort()</code>","text":"<p>The <code>sort()</code> method orders the DataFrame by one or more columns.</p> <pre><code># Sort customers_df by age in descending order\nsorted_customers_by_age_df = customers_df.sort(\"age\", descending=True)\nprint(\"\\nCustomers sorted by age (descending):\")\nprint(sorted_customers_by_age_df.select([\"name\", \"age\", \"city\"]).head())\n\n# Sort orders_df by 'product_category' (ascending) then by 'total_price' (descending)\n# We need 'total_price' from a previous example\nif \"total_price\" not in orders_enhanced_df.columns: # ensure it exists\n    orders_enhanced_df = orders_enhanced_df.with_columns(\n            (pl.col(\"quantity\") * pl.col(\"unit_price\")).alias(\"total_price\")\n    )\n\nsorted_orders_df = orders_enhanced_df.sort(\n    by=[\"product_category\", \"total_price\"],\n    descending=[False, True] # False for product_category (asc), True for total_price (desc)\n)\nprint(\"\\nOrders sorted by product_category (asc) then total_price (desc):\")\nprint(sorted_orders_df.select([\"product_category\", \"order_id\", \"total_price\"]).head(10))\n</code></pre>"},{"location":"eda/data-wrangling/intro-to-wrangling/","title":"Introduction to Data Wrangling and the Polars Framework","text":"<p>This module introduces the foundational concepts of data wrangling and the Polars library, a powerful tool for this purpose. Effective data wrangling is a critical precursor to sound business analysis and data-driven decision-making.</p>"},{"location":"eda/data-wrangling/intro-to-wrangling/#the-imperative-of-data-wrangling-in-business-analytics","title":"The Imperative of Data Wrangling in Business Analytics","text":"<p>In the context of business analytics, data wrangling refers to the process of transforming and mapping raw data from its original state into a clean, structured, and validated format suitable for analysis. This process is also commonly referred to as data munging or data tidying.</p> <p>The \"80/20 Reality\" in Data Preparation: It is widely recognized within the data science community that data preparation activities, including wrangling, can consume as much as 80% of the total time and effort in an analytical project. The remaining 20% is typically spent on performing the actual analysis and generating insights. This highlights the substantial resources dedicated to ensuring data quality and usability.</p> <p>Data Sources and the Need for Repurposing:     Data utilized for business analytics is often sourced from systems not initially designed for that specific analytical objective. These sources can include:</p> <ul> <li>Online Transaction Processing (OLTP) Systems: These are operational systems that capture day-to-day business transactions. Examples include sales systems, customer relationship management (CRM) platforms, and enterprise resource planning (ERP) systems. Their data structures are optimized for efficient transaction recording and retrieval, not necessarily for complex analytical queries. For instance, sales data might be highly normalized (split into many tables) to ensure data integrity for transactions, requiring restructuring for trend analysis.</li> <li>Online Analytical Processing (OLAP) Systems: While OLAP systems are designed for analysis and often provide summarized or aggregated data (e.g., sales by quarter), the specific analytical questions at hand may require further manipulation, disaggregation, or integration with data not present in the pre-defined OLAP views or cubes.</li> <li>Data Lakes: These are centralized repositories that store vast amounts of raw data in various formats, structured and unstructured. While offering comprehensive data access, the \"raw\" nature means significant wrangling is almost always necessary to extract, clean, and structure relevant data for a particular analytical task.</li> <li>Governance and Regulatory Data Archival Systems: Organizations also maintain data archives for compliance, legal, or historical purposes. The format and structure of this data are dictated by these archival requirements and often require substantial transformation before the data can be leveraged for analytical insights. Because these diverse systems serve different primary functions, the data they produce rarely aligns perfectly with the schema (i.e., the structure and organization) required for a specific analytical investigation. Consequently, data wrangling becomes an essential intermediary step to bridge this gap, ensuring the data is accurate, consistent, and in the correct format for meaningful analysis.</li> </ul> <p>Illustrative Data Challenges:     The provided datasets, <code>customers.csv</code>and <code>orders.csv</code>, exemplify common issues addressed during data wrangling on a small scale:</p> <ul> <li> <p>In <code>customers.csv</code>:</p> <ol> <li>Inconsistent <code>registration_date</code> formats (e.g., <code>YYYY-MM-DD</code> and <code>MM/DD/YYYY</code>) and missing entries hinder time-series analysis.</li> <li>Missing <code>age</code> values and a significant outlier (<code>3000</code>) can distort statistical summaries if not handled.</li> </ol> </li> <li> <p>In <code>orders.csv</code>:</p> <ol> <li>Missing <code>product_category</code> or <code>quantity</code> values can lead to incomplete or biased analytical results.</li> <li>Ambiguity in missing <code>discount_applied</code> values (zero discount vs. unrecorded data) requires clarification or consistent imputation. Addressing such issues is fundamental to the integrity of any subsequent analysis.</li> </ol> </li> </ul>"},{"location":"eda/data-wrangling/intro-to-wrangling/#introduction-to-polars-a-high-performance-dataframe-library","title":"Introduction to Polars: A High-Performance DataFrame Library","text":"<p>For data wrangling tasks in Python, this course will utilize Polars.</p> <p>Overview: Polars is a modern DataFrame library implemented in Rust and designed for Python. It leverages a powerful expression-based API and an optimizing query engine to achieve high performance, particularly with larger datasets. A DataFrame is a two-dimensional, size-mutable, and potentially heterogeneous tabular data structure with labeled rows and columns, conceptually similar to a spreadsheet or an SQL table.</p> <p>Advantages for Business Analytics:</p> <ul> <li>Performance: Polars' design allows for efficient processing of data manipulations, crucial when dealing with enterprise-scale datasets.</li> <li>Expressive Syntax: The API enables clear and concise articulation of complex data transformations.</li> <li>Modern Capabilities: It incorporates contemporary best practices in data processing and is under active development.</li> </ul>"},{"location":"eda/data-wrangling/intro-to-wrangling/#core-polars-concepts-a-high-level-view","title":"Core Polars Concepts: A High-Level View","text":"<p>Understanding two fundamental concepts will facilitate your use of Polars:</p> <p>Expressions: Expressions are central to Polars. They define a sequence of operations to be performed on data. For instance, an expression can select columns, create new columns derived from existing ones, or filter rows based on specified criteria. The <code>pl.col(\"column_name\")</code> syntax is commonly used within expressions to refer to specific data columns.</p> <p>Execution Strategies (Lazy vs. Eager):</p> <p>Polars can apply these expressions using two primary strategies:</p> <ul> <li>Eager Execution: Operations are performed immediately as they are specified. Results are available instantly, which is often intuitive for simpler, interactive tasks.</li> <li>Lazy Execution: Operations are first defined as part of a query plan. Polars can then optimize this entire plan for efficiency before any computation occurs. The final result is computed when explicitly requested (typically via a <code>.collect()</code> method). This approach can offer significant performance advantages for more complex sequences of transformations by allowing Polars to identify and apply optimizations across the entire workflow.</li> </ul> <p>While many initial interactions may appear eager, awareness of Polars' lazy capabilities is beneficial as it underpins much of its performance.</p>"},{"location":"eda/data-wrangling/intro-to-wrangling/#setup-importing-the-polars-library","title":"Setup: Importing the Polars Library","text":"<p>To utilize Polars within a Python environment (such as a Colab or Jupyter notebook), it must first be imported. The standard convention for importing Polars is:</p> <pre><code>import polars as pl\n</code></pre> <p>This statement makes the Polars library accessible and assigns it the alias <code>pl</code>, a common shorthand used when calling Polars functions (e.g., <code>pl.read_csv()</code>).</p>"},{"location":"eda/data-wrangling/stitching-and-saving/","title":"Stiching and Saving Data","text":"<p>Stitching and Saving Data</p> <p>In real-world scenarios, data often resides in multiple tables or files. This module covers how to \"stitch\" these related datasets together using joins. We will also cover how to save your processed DataFrames to files for future use or sharing. Finally, we'll briefly touch upon some workflow concepts.</p> Prerequisites <pre><code># Prerequisites: Ensure Polars is imported.\nimport polars as pl\n\n# --- Placeholder DataFrames (consistent with those used/modified in previous modules) ---\n# Re-establishing them here for clarity if this module is run standalone.\n\ncustomers_df = pl.DataFrame({\n    \"customer_id\": [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115],\n    \"name\": [\"Alice Wonderland\", \"Bob The Builder\", \"Charlie Brown\", \"Diana Prince\", \"Evan Almighty\", \"Fiona Gallagher\", \"George Jetson\", \"Hannah Montana\", \"Ian Malcolm\", \"Jane Doe\", \"Kevin McCallister\", \"Laura Palmer\", \"Michael Scott\", \"Nancy Drew\", \"Oscar Grouch\"],\n    \"registration_date_str\": [\"2022-01-15\", \"2022-03-22\", \"2022-05-10\", \"2022-07-01\", \"2022-08-19\", \"2023-01-20\", None, \"2023-04-05\", \"2023-06-12\", \"2023-07-21\", \"2023-09-01\", \"2023-10-15\", \"2024-02-10\", \"03/15/2024\", \"2024-05-01\"],\n    \"city\": [\"New York\", \"London\", \"Paris\", \"New York\", \"London\", \"New York\", \"Paris\", \"Berlin\", \"London\", \"New York\", \"Chicago\", \"Twin Peaks\", \"Scranton\", \"River Heights\", \"New York\"],\n    \"age\": [28, 35, 45, 3000, 42, 29, 50, 22, 55, None, 12, 17, 48, 18, 60]\n}).with_columns(\n    pl.when(pl.col(\"age\") &gt; 100).then(None).otherwise(pl.col(\"age\")).cast(pl.Int64, strict=False).alias(\"age_cleaned\")\n)\n\norders_df = pl.DataFrame({\n    \"order_id\": [201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218],\n    \"customer_id\": [101, 102, 101, 103, 104, 102, 105, 101, 106, 108, 103, 107, 110, 111, 102, 113, 101, 115], # Includes customer_id 111 not in current customers_df for left join illustration\n    \"order_date_str\": [\"2023-01-20 10:30:00\", \"2023-02-15 11:05:30\", \"2023-02-28 14:12:55\", \"2023-03-10 09:00:15\", \"2023-03-12 17:45:00\", \"2023-04-05 12:00:00\", \"2023-04-22 16:20:30\", \"2023-05-01 10:00:00\", \"2023-05-15 08:55:10\", \"2023-06-01 11:30:00\", \"2023-06-20 13:40:00\", \"2023-07-01 00:00:00\", \"2023-07-25 10:10:10\", \"2023-09-10 14:20:30\", \"2023-11-05 19:00:00\", \"2024-02-15 09:30:00\", \"2024-03-01 10:00:00\", \"2024-05-05 12:12:12\"],\n    \"product_category\": [\"Books\", \"Tools\", \"Electronics\", \"Home Goods\", \"Antiques\", \"Books\", \"Electronics\", \"Books\", \"Beauty\", \"Music\", \"Home Goods\", \"Electronics\", \"Clothing\", \"Toys\", \"Tools\", \"Office Supplies\", \"Electronics\", \"Home Goods\"],\n    \"quantity\": [2, 1, 1, 3, 1, 1, None, 3, 2, 5, 1, 1, 2, 3, 1, 10, 1, 1],\n    \"unit_price\": [15.99, 199.50, 799.00, 25.00, 2500.00, 12.50, 99.99, 10.00, 45.75, 9.99, 150.00, 499.50, 75.00, 29.99, 75.00, 4.99, 1200.00, 12.00]\n}).with_columns([\n    pl.col(\"quantity\").cast(pl.Int64, strict=False),\n    pl.col(\"unit_price\").cast(pl.Float64, strict=False),\n    # For simplicity, assuming discount_applied and order_datetime are handled if needed later\n])\n\n# Let's refine customers_df to ensure all customer_ids in orders_df are present if we want to avoid nulls in inner joins\n# Or, keep it as is to demonstrate how different joins handle mismatches.\n# For this example, let's keep customers_df as is to show how left join differs.\n# Example customer_id in orders_df (e.g., 105) might not be in a shortened customers_df if it was trimmed.\n# The current customer_df includes 101-115. orders_df has customer_ids within this range.\n# Let's adjust orders_df slightly to have a customer_id that *might not* be in customers_df if it were a subset\n# For a clean join demonstration, we will use the full customers_df and orders_df where customer_ids align.\n# The `customers_df` covers 101-115. `orders_df` uses customer_ids within this range.\n# To illustrate left join better where left has unmatched, let's assume an order from customer 999\n# orders_df = orders_df.vstack(pl.DataFrame({\"order_id\": [999], \"customer_id\": [999], ...})) # this is complex for now\n# Instead, we'll focus on cases where customers might not have orders.\n</code></pre>"},{"location":"eda/data-wrangling/stitching-and-saving/#1-stitching-data-joining-dataframes-with-join","title":"1. Stitching Data: Joining DataFrames with <code>.join()</code>","text":"<p>Often, the data you need for a comprehensive analysis is spread across multiple tables or files. For instance, you might have order details in one DataFrame and customer information in another. Joining allows you to combine these DataFrames based on common key columns.</p> <ul> <li> <p>The <code>DataFrame.join()</code> Method:     The primary method for this is <code>df.join(other_df, on=\"key_column_or_columns\", how=\"join_strategy\")</code>.</p> <ul> <li><code>other_df</code>: The DataFrame to join with.</li> <li><code>on</code>: A string specifying the common column name, or a list of strings for multiple key columns. If key column names differ between DataFrames, use <code>left_on=\"left_col_name\", right_on=\"right_col_name\"</code>.</li> <li><code>how</code>: A string defining the type of join. Common types include:<ul> <li><code>inner</code> (default): Keeps only rows where the key(s) exist in both DataFrames.</li> <li><code>left</code>: Keeps all rows from the left DataFrame (the one calling the <code>.join()</code> method) and the matched rows from the right DataFrame. If there's no match in the right DataFrame, columns from the right DataFrame will be filled with <code>null</code>.</li> <li><code>outer</code>: Keeps all rows from both DataFrames. If there's no match for a key in the other DataFrame, missing columns are filled with <code>null</code>.</li> <li>Other types like <code>semi</code>, <code>anti</code>, and <code>cross</code> exist for more specific use cases.</li> </ul> </li> </ul> </li> <li> <p>Example: Combining Orders with Customer Information     Let's join <code>orders_df</code> with <code>customers_df</code> to enrich each order with details about the customer who placed it. The common key is <code>customer_id</code>.</p> <pre><code># Inner Join: Only orders with matching customers, and only customers with matching orders\n# (In this case, all customer_ids in orders_df are in customers_df based on full placeholders)\nmerged_inner_df = orders_df.join(\n    customers_df,\n    on=\"customer_id\",\n    how=\"inner\"\n)\nprint(\"Inner Join - Orders with Customer Details:\")\nprint(merged_inner_df.select([\n    \"order_id\", \"customer_id\", \"name\", \"city\", \"product_category\", \"unit_price\"\n]).head())\nprint(f\"Shape of inner joined df: {merged_inner_df.shape}\")\n\n\n# Left Join: All orders, with customer details if available\n# This is useful if you want to keep all orders, even if some customer details might be missing (though not in our current full dataset).\n# Or, more relevantly, if some customers in customers_df have no orders, they wouldn't appear in an inner join with orders_df as left.\n# Let's demonstrate with customers_df on the left, to see all customers and their orders.\ncustomer_orders_left_df = customers_df.join(\n    orders_df, # Joining with orders data\n    on=\"customer_id\",\n    how=\"left\" # Keep all customers, match with their orders\n)\nprint(\"\\nLeft Join - All Customers with Their Orders (if any):\")\n# Select relevant columns to display\nprint(customer_orders_left_df.select([\n    \"customer_id\", \"name\", \"city\", \"order_id\", \"product_category\"\n]).sort(\"customer_id\").head(10)) # Showing more rows to see customers with and without orders\n# Find customers who might not have orders (order_id would be null)\nprint(\"\\nCustomers with no orders (example from left join):\")\nprint(customer_orders_left_df.filter(pl.col(\"order_id\").is_null()).select([\"customer_id\", \"name\"]))\nprint(f\"Shape of left joined df (customers as left): {customer_orders_left_df.shape}\")\n</code></pre> <p>Business Context: An inner join might be used to analyze only confirmed sales linked to known customers. A left join (e.g., <code>customers_df.join(orders_df, ..., how=\"left\")</code>) is crucial for analyses like identifying customers who haven't placed an order.</p> </li> </ul>"},{"location":"eda/data-wrangling/stitching-and-saving/#2-saving-dataframes-to-files","title":"2. Saving DataFrames to Files","text":"<p>After performing your data wrangling tasks\u2014cleaning, transforming, joining\u2014you'll often want to save the resulting DataFrame. This allows you to persist your work for later use, share it with colleagues, or use it as input for other tools or models.</p> <ul> <li> <p>Saving to CSV (<code>.write_csv()</code>):     Writes the DataFrame to a Comma-Separated Values file.</p> <pre><code># Example: Saving the merged_inner_df (orders with customer details) to a CSV file\n# Let's select a subset of columns for a cleaner output file\noutput_df_for_csv = merged_inner_df.select([\n    \"order_id\", \"customer_id\", \"name\", \"city\", \"product_category\", \"quantity\", \"unit_price\"\n]).sort(\"order_id\")\n\ntry:\n    output_df_for_csv.write_csv(\"enriched_orders.csv\")\n    print(\"\\nSuccessfully saved 'enriched_orders.csv'\")\nexcept Exception as e:\n    print(f\"Error saving CSV: {e}\")\n</code></pre> <p>You can specify other parameters like <code>separator</code> if you don't want a comma.</p> </li> <li> <p>Saving to JSON (<code>.write_json()</code>):     Writes the DataFrame to a JSON file. A common format is a list of records (row-oriented).</p> <pre><code># Example: Saving the same output_df_for_csv to a JSON file\ntry:\n    # row_oriented=True creates a list of JSON objects, one per row.\n    # pretty=True makes the JSON output human-readable (indented).\n    output_df_for_csv.write_json(\"enriched_orders.json\", row_oriented=True, pretty=True)\n    print(\"Successfully saved 'enriched_orders.json' (row-oriented, pretty)\")\nexcept Exception as e:\n    print(f\"Error saving JSON: {e}\")\n</code></pre> <p>Polars' <code>write_json</code> can also produce column-oriented JSON if <code>row_oriented=False</code>.</p> </li> </ul>"},{"location":"eda/data-wrangling/stitching-and-saving/#3-workflow-and-execution-notes","title":"3. Workflow and Execution Notes","text":"<ul> <li> <p>Method Chaining for Cleaner Pipelines:     Polars encourages chaining multiple operations together to create concise and readable data transformation pipelines.</p> <pre><code># Example: Filter electronics orders, calculate total price, select relevant columns, and sort\n# (Assuming orders_df has 'quantity' and 'unit_price')\nfinal_electronics_report_df = (\n    orders_df\n    .filter(pl.col(\"product_category\") == pl.lit(\"Electronics\"))\n    .with_columns(\n        (pl.col(\"quantity\") * pl.col(\"unit_price\")).alias(\"total_sale_amount\")\n    )\n    .select([\"order_id\", \"customer_id\", \"total_sale_amount\"])\n    .sort(\"total_sale_amount\", descending=True)\n)\nprint(\"\\nChained operations for electronics report:\")\nprint(final_electronics_report_df.head())\n# This final_electronics_report_df could then be saved.\n# final_electronics_report_df.write_csv(\"electronics_sales_report.csv\")\n</code></pre> </li> <li> <p>Lazy Execution and <code>.collect()</code> Revisited:</p> <ul> <li>As mentioned before, Polars often builds an optimized plan for your operations (lazy execution) rather than executing each step immediately. This is especially true when you start a chain with <code>df.lazy()</code>.</li> <li>When you have a \"LazyFrame\" (e.g., from <code>df.lazy().some_operation()</code>), the computations are only performed when you explicitly call <code>.collect()</code>.</li> <li>Many \"eager\" operations (like most of what we've used directly on DataFrames like <code>df.filter()</code>) will execute and return a new DataFrame immediately. However, understanding lazy evaluation is key to appreciating Polars' performance with complex queries.</li> </ul> <pre><code># Conceptual example of a lazy query\nlazy_result_plan = (\n    orders_df.lazy() # Start a lazy query\n    .filter(pl.col(\"unit_price\") &gt; pl.lit(100))\n    .group_by(\"product_category\")\n    .agg(pl.sum(\"quantity\").alias(\"total_high_value_quantity\"))\n)\nprint(\"\\nLazy query plan constructed (no execution yet):\")\nprint(lazy_result_plan) # Shows the logical plan\n\n# To execute the plan and get the actual DataFrame:\n# final_high_value_summary_df = lazy_result_plan.collect()\n# print(\"\\nExecuted lazy query result:\")\n# print(final_high_value_summary_df)\n</code></pre> <p>For many operations in an introductory context, Polars feels \"eager,\" but its lazy core is always there for optimization.</p> </li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<ul> <li> <p> Introduction to Colab</p> <p> Getting started</p> </li> <li> <p> Introduction to Python</p> <p> Getting started</p> </li> </ul>"},{"location":"getting-started/colab/","title":"Introduction to Colab","text":"<ul> <li> Starter Colab Notebook</li> <li> Getting Started </li> <li> User Interface </li> <li> Kernel State </li> <li> Data &amp; Files </li> <li> Essential Techniques </li> <li> Going Deeper </li> <li> Best Practice </li> </ul>"},{"location":"getting-started/colab/best-practice/","title":"Best Practices &amp; Getting Help","text":"<p>You've now toured the essential features and concepts for using Google Colab in our \"Data Programming Essentials with Python\" course. To ensure a smooth learning experience, keep these best practices in mind and know where to turn for help.</p>"},{"location":"getting-started/colab/best-practice/#summary-working-effectively-in-colab","title":"Summary: Working Effectively in Colab","text":"<ul> <li>Document with Markdown: Use Text cells and Markdown headings (<code>#</code>, <code>##</code>, etc.) frequently. Explain your code, structure your analysis, and make your notebooks readable (See Section 2.5). Clear documentation is crucial for learning and collaboration.</li> <li>Manage State Consciously: Remember that execution order matters more than cell order (Section 3.1). Use <code>Runtime -&gt; Restart runtime</code> if things behave strangely, and always use <code>Runtime -&gt; Restart and run all</code> before submitting work or considering an analysis complete to ensure reproducibility (Section 3.4).</li> <li>Use Google Drive for Persistence: Mount your Google Drive for all important notebooks and datasets (Section 4.3). Rely on temporary session storage only for quick, disposable file uploads (Section 4.2). Create an organized folder structure within your Drive for the course.</li> <li>Be Version Aware &amp; Consult Docs: Python libraries evolve. Check package versions (<code>library.__version__</code>), pay attention to <code>AttributeError</code>/<code>TypeError</code> and <code>DeprecationWarning</code>s, and always refer to the current official documentation for the libraries you're using (Section 5.1).</li> <li>Leverage Colab Features: Use helpful tools like the File Browser's \"Copy path\" (Section 4.4), Keyboard Shortcuts (Section 6.1), and the integrated AI Assistance (Section 5.2) \u2013 but always focus on understanding the underlying concepts.</li> <li>Debug Systematically: Errors are learning opportunities. Read tracebacks carefully (Section 5.3), use <code>print()</code>, <code>type()</code>, and <code>dir()</code> to inspect the state of your variables (Section 5.3), and try to isolate the problem step-by-step.</li> </ul>"},{"location":"getting-started/colab/best-practice/#leverage-colab-for-learning-from-external-code","title":"Leverage Colab for Learning from External Code","text":"<p>A key skill in programming is learning from code written by others. When you find relevant Python examples online (e.g., on documentation sites, GitHub, Stack Overflow), Colab offers a great environment to deconstruct and understand them:</p> <ol> <li>Isolate the Code: Paste the external code snippet into one or more code cells in a new or relevant Colab notebook.</li> <li>Ask AI for Insight: Select the code and use the built-in Gemini (\u2728) feature (See Section 5.2). Try prompts like:<ul> <li><code>\"Explain this code step-by-step.\"</code></li> <li><code>\"What is the main goal of this function/script?\"</code></li> <li><code>\"Identify the logical sections of this code and explain each.\"</code></li> <li><code>\"Add comments to this Python code.\"</code></li> </ul> </li> <li>Structure with Markdown: Insert Text cells around the code blocks. Use the insights from Gemini (or your own analysis) to create Markdown headings (<code>##</code>, <code>###</code>) that describe each logical part. Add bullet points or descriptive paragraphs in these text cells. Pro Tip: You can even ask Gemini to try generating Markdown documentation directly!</li> <li>Organize &amp; Navigate: Ensure your Markdown headings accurately reflect the code structure. Now, use the Table of Contents pane (See Section 2.5) to get a clear overview and instantly navigate to different parts of the code. You can also collapse sections directly in the notebook using the small triangles next to the Markdown headings to focus on specific areas.</li> <li>Test and Adapt: Run the code, experiment with changes (perhaps in new cells to preserve the original), and add your own notes.</li> </ol> <p>Benefit: This workflow transforms static code examples into interactive, well-documented learning modules within your familiar Colab environment.</p> <p>Caution: Always review AI explanations critically. Ensure you understand why the code works. Test thoroughly, especially if adapting code, and remember to cite your sources appropriately if using significant portions in your own work.</p>"},{"location":"getting-started/colab/best-practice/#official-colab-resources","title":"Official Colab Resources","text":"<p>For more in-depth information directly from Google, check out these official resources:</p> <ul> <li>Welcome to Colaboratory Notebook: https://colab.research.google.com/notebooks/intro.ipynb (A great interactive introduction).</li> <li>Colab Overview: https://research.google.com/colaboratory/ (High-level features).</li> <li>Colab FAQ: https://research.google.com/colaboratory/faq.html (Answers to common questions).</li> </ul>"},{"location":"getting-started/colab/best-practice/#getting-help-in-this-course","title":"Getting Help in This Course","text":"<p>If you've reviewed this guide and the official resources but still have questions about using Colab for our specific course activities:</p> <ul> <li>Check the Course Discussion Forum/Platform: [&lt;- Insert Link Here e.g., Link to Canvas Discussion, Slack Channel, Piazza, etc.] - Post your questions here. It's likely others have similar questions, and peer-to-peer help is encouraged!</li> <li>Attend Teaching Assistant (TA) or Instructor Office Hours: [&lt;- Insert Details Here e.g., Schedule, Location/Link, How to sign up] - This is the best place for personalized help with specific problems you're encountering in labs or the project.</li> </ul> <p>Don't hesitate to reach out when you get stuck \u2013 learning to ask effective questions is part of the process! Good luck, and enjoy using Colab!</p>"},{"location":"getting-started/colab/colab-interface/","title":"The Colab Interface: Your Workspace","text":"<p>Now that you have a fresh notebook open, let's get familiar with the main areas you'll be interacting with. Don't feel you need to memorize everything at once \u2013 we'll focus on the parts you'll use most often.</p>"},{"location":"getting-started/colab/colab-interface/#interface-overview","title":"Interface Overview","text":"<p>Take a look at your Colab screen. Here are the main parts:</p> <p></p> <ul> <li> <code>Menu Bar</code> (Top: File, Edit, View, Insert, Runtime, Tools, Help - Standard menu options)</li> <li> <code>Toolbar</code> (Below Menu: Buttons for common actions like +Code, +Text, file operations, runtime status)</li> <li> <code>Main Cell Area</code> (The large central area: This is where you'll write and run your code and text)</li> <li> <code>Left Sidebar</code> (Contains tabs for: Table of Contents, Find/Replace, Code Snippets, Secrets, and importantly, the File Browser)</li> <li> <code>Notebook Title</code> (Top Left: Where you renamed your notebook)</li> <li> <code>Share Button</code> (Top Right: For sharing your notebook with others)</li> <li> <code>Runtime Status / Connect Button</code> (Top Right near Share: Shows if you're connected to a runtime and resource usage)</li> </ul> <p>You'll spend most of your time writing in the Main Cell Area (C), using the Toolbar (B) and Menu Bar (A) for actions, and the Left Sidebar (D) for navigation and file management.</p>"},{"location":"getting-started/colab/colab-interface/#cells-the-building-blocks","title":"Cells: The Building Blocks","text":"<p>The core components of any Jupyter-style notebook, including Colab, are cells. Think of them as individual containers for your content. There are two primary types:</p> <ol> <li>Code Cells:<ul> <li>This is where you will write and execute your Python code for data analysis, visualization, and modeling.</li> <li>They are usually marked with <code>[ ]:</code> brackets to their left. When you run the code in the cell, a number will appear in the brackets (like <code>[1]:</code>) indicating the order of execution.     (Suggestion: Small screenshot of an empty code cell)</li> </ul> </li> <li>Text Cells:<ul> <li>These cells are for writing descriptive text, notes, explanations, section headings, mathematical formulas \u2013 essentially, any documentation needed to explain your work.</li> <li>They use a simple formatting syntax called Markdown. When you \"run\" a text cell, Colab renders the Markdown into nicely formatted text (bold, italics, lists, headings, etc.). We'll look closer at Markdown in section 2.5.     (Suggestion: Small screenshot of a text cell with some Markdown source, and another showing the rendered output)</li> </ul> </li> </ol> <p>Switching Between Cell Types: You can easily change a cell from Code to Text or vice-versa. When a cell is selected (you'll see a border around it), you can use the command pallette  or keyboard shortcuts </p> <p>Note</p> <ul> <li>To invoke commands via keyboard shortcuts, you need to press the prefix combo before the key for the command.</li> <li>The prefix combo in Windows/Linux/ChromeOS is Ctrl+M and in MacOS it is Cmd+M</li> <li>To turn a cell into a markdown/text cell press prefixthen M. </li> <li>To turn a cell into a code cell press prefix then Y. </li> <li>You can review all keyboard shortcuts available by pressing prefix then H.</li> </ul>"},{"location":"getting-started/colab/colab-interface/#running-cells-bringing-notebooks-to-life","title":"Running Cells: Bringing Notebooks to Life","text":"<p>Simply typing in a cell doesn't do anything until you run (or execute) it. Here are the most common ways to run the currently selected cell:</p> <ul> <li>Shift + Enter <ul> <li>Executes the current cell and automatically selects the next cell below it. If you're at the last cell, it often creates a new code cell. Use this to flow through your notebook.</li> </ul> </li> <li>Ctrl + Enter on Windows and Linux / Cmd + Enter on MacOS <ul> <li>Executes the current cell but keeps the focus on the same cell. Useful if you want to re-run the same code multiple times quickly.</li> </ul> </li> <li>Alt + Enter / Option + Enter <ul> <li>Executes the current cell and inserts a brand new code cell immediately below it. Handy for adding quick tests or explorations.</li> </ul> </li> <li> Play Icon<ul> <li>When you hover your mouse pointer to the left of a code cell, a circular 'play' button icon appears. Clicking this runs only that specific cell. (Suggestion: Small screenshot showing the Play icon next to a code cell)</li> </ul> </li> </ul>"},{"location":"getting-started/colab/colab-interface/#what-happens-when-you-run-a-cell","title":"What happens when you run a cell?","text":"<ul> <li>Code Cell: The Python code is executed by the Colab backend (the \"Kernel\"). Any output generated by the code (e.g., from <code>print()</code> statements, calculation results, error messages) will appear directly beneath the cell. The execution counter in the <code>[ ]:</code> brackets gets updated.</li> <li>Text Cell: The Markdown text is rendered into formatted output, making it readable.</li> </ul> <p>The Cell Toolbar: Quick Actions</p> <p>When you select a cell, or sometimes just hover over it, a small toolbar  usually appears (often in the top right corner of the cell). This provides handy shortcuts for common actions:</p> <p></p> <p>Look for icons like:</p> <ul> <li>Arrows (Up/Down): To move the selected cell up or down in the notebook sequence.</li> <li>Trash Can: To delete the current cell (use with caution!).</li> <li>Link Icon: To get a direct URL link to that specific cell.</li> <li>\u2728 Gemini/AI Icons: Buttons to access integrated AI assistance for generating code, explaining concepts, etc. (More on this in Section 5!).</li> <li>Three Dots ('More actions'): Often reveals other options like clearing the output of a cell.</li> </ul> <p>Get comfortable using this toolbar! It's often faster than navigating the main menus for these frequent tasks.</p>"},{"location":"getting-started/colab/colab-interface/#staying-organized-text-cells-are-key","title":"Staying Organized: Text Cells are Key!","text":"<p>As your analyses become more involved, your notebooks can get quite long. Effective organization is essential for making them understandable. Text cells and Markdown are your best friends here.</p> <p>Use Markdown for Structure: Markdown allows you to easily format your text cells. Some essentials:</p> <ul> <li><code># Heading 1</code> (Largest heading - use for main titles/sections)</li> <li><code>## Heading 2</code> (For sub-sections)</li> <li><code>### Heading 3</code> (For smaller points)</li> <li>Use <code>*asterisks*</code> or <code>_underscores_</code> for italics.</li> <li>Use <code>**double asterisks**</code> or <code>__double underscores__</code> for bold text.</li> <li>Start lines with <code>-</code> or <code>*</code> for bullet points.</li> <li>Start lines with <code>1.</code>, <code>2.</code>, etc., for numbered lists.</li> </ul> <p>Note</p> <p>Review this Markdown Guide from Google for a bite-sized tutorial and a list of references on this topic.</p> <p>Leverage the Table of Contents: Colab automatically creates a navigable Table of Contents based on the Markdown headings in your text cells.</p> <ul> <li>Find the Table of Contents icon (usually looks like three horizontal lines or a bulleted list) in the Left Sidebar. Click it.</li> <li>A pane will appear showing your headings, nested according to their level (<code>#</code>, <code>##</code>, etc.).</li> <li>Click any heading in this pane to jump directly to that section of your notebook. This is incredibly useful for navigation!</li> </ul> <p></p> <p>Recommendation: Make it a habit to use Markdown headings (<code>#</code>, <code>##</code>) to structure all your notebooks (labs, projects). It dramatically improves readability and navigation for yourself and anyone you share your work with.</p>"},{"location":"getting-started/colab/colab-intro/","title":"Colab: Your Coding Environment","text":"<p>Welcome to the world of data programming! Before we dive into Python itself, let's get familiar with the main tool we'll be using throughout this course: Google Colaboratory, or Colab. Think of it as your digital workbench for all things data science in this class.</p>"},{"location":"getting-started/colab/colab-intro/#why-use-an-ide","title":"Why Use an IDE?","text":"<p>Imagine you're trying to build something complex, maybe assemble furniture or cook a gourmet meal. You could try to do it with just a few basic, separate tools scattered around. But isn't it much easier if you have a dedicated workshop or a well-organized kitchen with everything you need integrated and within reach?</p> <p>An IDE, which stands for Integrated Development Environment, is like that well-equipped workshop, but for writing computer code. It brings together all the essential tools you need into one convenient place, making the process of writing, testing, and fixing code much smoother. Typically, an IDE provides:</p> <ul> <li>A specialized editor for writing your code (often with helpful features like syntax highlighting to make code more readable).</li> <li>A way to run or execute your code and see the results immediately.</li> <li>Tools to help organize your files and project components.</li> <li>Features to help you find and fix errors in your code (known as debugging).</li> </ul> <p>Using an IDE helps you be more productive and focus on the logic of your analysis, rather than fighting with basic tools. For this course, Google Colab will be our IDE, specifically tailored for working with data in Python.</p> <p>Aside: The \"Studio\" Connection</p> <p>That idea of an IDE being a programmer's \"workshop\" or creative \"studio\" isn't just an analogy we're using \u2013 it's reflected right in the names of many widely-used development tools! You might recognize names like:</p> <ul> <li>Visual Studio and Visual Studio Code (from Microsoft, for various types of software development)</li> <li>Android Studio (from Google, for building Android mobile apps)</li> <li>RStudio (very popular in data science for working with the R programming language)</li> </ul> <p>The \"Studio\" name emphasizes that these are integrated environments designed for focused, productive coding work, bringing all the necessary tools together in one place.</p>"},{"location":"getting-started/colab/colab-intro/#what-are-jupyter-notebooks","title":"What are Jupyter Notebooks?","text":"<p>Google Colab is built upon a popular open-source project called Jupyter. So, what's a Jupyter Notebook?</p> <p>Imagine a digital document that's more dynamic than a standard text file. In a Jupyter Notebook (which typically has an <code>.ipynb</code> file extension), you can combine:</p> <ul> <li>Live, runnable Python code.</li> <li>The output from running that code (like calculations, tables of data, or messages).</li> <li>Formatted text for explanations, notes, and documentation (using a simple language called Markdown).</li> <li>Mathematical equations.</li> <li>Interactive visualizations like charts and graphs.</li> </ul> <p>This combination makes Jupyter Notebooks incredibly powerful for data science because they allow you to:</p> <ul> <li>Explore data interactively: Write a small piece of code, run it, see the result, tweak the code, and run it again, all within the same document.</li> <li>Explain your work: Document your thought process, methodology, and findings right alongside the code that produced them. This creates a clear narrative of your analysis.</li> <li>Share your results effectively: Package your entire workflow \u2013 code, outputs, visuals, and explanations \u2013 into a single file that others can easily view and even run themselves.</li> </ul> <p>Think of a Jupyter Notebook as a computational story \u2013 you're not just writing code; you're documenting your entire analysis journey.</p>"},{"location":"getting-started/colab/colab-intro/#google-colab-jupyter-in-the-cloud","title":"Google Colab: Jupyter in the Cloud","text":"<p>Now, where does Google Colab fit in? Colab is essentially Google's hosted version of the Jupyter Notebook environment, accessible entirely through your web browser.</p> <p>Here's why it's our choice for this course:</p> <ul> <li>Zero Setup Required: This is a huge advantage! You don't need to install Python or any complex software on your own computer. As long as you have a web browser and a Google account, you're ready to go.</li> <li>Free Access: Colab provides free access to computing resources, including Python environments with many essential data science libraries (like Polars, Altair, and Scikit-learn, which we'll use) pre-installed or easily installable.</li> <li>Seamless Google Drive Integration: Your Colab notebooks are automatically saved to your Google Drive. You can also easily access data files stored in your Drive directly from your Colab notebooks (we'll cover how later!).</li> <li>Easy Collaboration &amp; Sharing: Just like Google Docs or Sheets, you can easily share your Colab notebooks with others for viewing, commenting, or editing.</li> </ul> <p>In short, Colab provides a powerful, accessible, and hassle-free platform for us to learn and apply data programming techniques using Python.</p>"},{"location":"getting-started/colab/colab-intro/#getting-started-accessing-colab","title":"Getting Started: Accessing Colab","text":"<p>Ready to open your first notebook? It's simple:</p> <ol> <li>You Need a Google Account: Since Colab integrates with Google Drive, you must be logged into a Google account (this could be a personal Gmail account or a Google Workspace account).</li> <li>Go to the Colab Website: Open your preferred web browser and navigate to this URL:     https://colab.research.google.com</li> <li> <p>Create a New Notebook:</p> <ul> <li>You might see a welcome pop-up window with options like viewing recent notebooks or examples. You can simply close this for now by clicking <code>CANCEL</code> or the 'X'.</li> </ul> <p></p> <ul> <li>From the menu bar at the top, click File -&gt; New notebook in Drive.</li> </ul> <p></p> </li> <li> <p>Name Your Notebook:</p> <ul> <li>The notebook will open with a default title like <code>Untitled0.ipynb</code> at the very top left of the page.</li> <li>Click directly on this title. A rename dialog will appear. Change the name to something descriptive, like <code>MyFirstColabNotebook.ipynb</code> or <code>W1_Colab_Intro.ipynb</code>, and press Enter.</li> </ul> </li> </ol> <p>Success! You've just created your first Colab notebook. It's automatically saved in a folder named \"Colab Notebooks\" within your Google Drive. You're now looking at the Colab interface, ready for the next step.</p>"},{"location":"getting-started/colab/data-files/","title":"Working with Data &amp; Files","text":"<p>Data analysis naturally starts with data! A common first step is loading data from files (like CSVs, Excel files, etc.) into your programming environment. Colab provides two primary ways to access files within your notebook session. Understanding the difference is key to managing your data effectively.</p>"},{"location":"getting-started/colab/data-files/#colab-storage-explained","title":"Colab Storage Explained","text":"<ol> <li> <p>Session Storage (Temporary):</p> <ul> <li>Think of this as a small, temporary hard drive attached to your Colab notebook only while it's actively running.</li> <li>Pros: Very easy to quickly upload files directly from your computer.</li> <li>Cons: This storage is ephemeral. When your Colab session ends (because you close the tab for an extended period, the connection times out after inactivity, or you manually restart the runtime), this storage is completely wiped, and any files you uploaded directly will be gone.</li> <li>Best Use: Small files needed only for the current working session, quick tests.</li> </ul> </li> <li> <p>Google Drive (Persistent):</p> <ul> <li>You can connect (\"mount\") your personal Google Drive account directly to your Colab session.</li> <li>Pros: Files stored in your Google Drive persist across sessions. This is the standard way to work with datasets you'll reuse, larger files, or any work you want to save reliably. It also allows Colab notebooks to read and write files back to your Drive.</li> <li>Cons: Requires a brief authorization step each time you start a new session.</li> <li>Best Use: Almost all course datasets, project files, any data you need to access repeatedly. This is the recommended method for most work in this course.</li> </ul> </li> </ol> <p>Let's explore how to use each method.</p>"},{"location":"getting-started/colab/data-files/#session-storage","title":"Session Storage","text":"<p>Method 1: Uploading to Session Storage (Temporary Use)</p> <p>This is suitable for a quick, one-time analysis of a small file.</p> <p>How-to:</p> <ol> <li> <p>In the Left Sidebar, click the Folder icon  to open the File Browser.</p> <p></p> </li> <li> <p>At the top of the File Browser pane that appears, click the \"Upload to session storage\" button . It usually looks like a page icon with an arrow pointing upwards.</p> </li> <li> <p>A file selection dialog from your operating system will open. Navigate to the desired file on your local computer (e.g., <code>sample_data.csv</code>) and select it .</p> <p></p> </li> <li> <p>The file will be uploaded to Colab's temporary environment, and you should see its name appear in the File Browser list . Upload speed depends on the file size and your internet connection.</p> <p></p> </li> </ol> <p>Accessing the File in Code: Once uploaded to session storage, you typically refer to the file in your Python code using just its filename as the path, because it's in the \"root\" directory of this temporary environment.</p> <pre><code># Example using Polars (syntax details covered later)\nimport polars as pl\n\ntry:\n  df_temp = pl.read_csv('sample_data.csv')\n  print(\"Successfully read from session storage:\")\n  print(df_temp.head(3))\nexcept Exception as e:\n  print(f\"Error reading file: {e}\")\n  print(\"Did you upload 'sample_data.csv' using the File Browser?\")\n</code></pre> <p>Crucial Reminder: Remember, this storage is temporary. If you restart your runtime or close the notebook for too long, <code>sample_data.csv</code> will disappear, and the code above would fail.</p>"},{"location":"getting-started/colab/data-files/#persistent-storage","title":"Persistent Storage","text":"<p>Method 2: Mounting Google Drive (Recommended)</p> <p>This is the standard and most reliable way to work with your data files for this course.</p> <p>How-to (Mounting Your Drive):</p> <ol> <li> <p>Add a new code cell to your notebook if you don't have one ready .</p> <p></p> </li> <li> <p>Enter the following exact Python code snippet into the cell:     <pre><code>from google.colab import drive\ndrive.mount('/content/drive')\n</code></pre></p> </li> <li> <p>Run this code cell (e.g., Shift+Enter).</p> <p></p> <p>Warning</p> <p>Authorization process is dynamic and sensitive to security settings on your Google profile, personal computer, and the browser. Most likely the instructions here may not match what you see. Follow the instructions on your screen closely. If you have trouble completing the task, please reach out.</p> </li> <li> <p>Authorization Process: The first time you run this in a new session, Colab needs your permission to access your Google Drive. The cell's output will display:</p> <ul> <li>A URL (e.g., <code>https://accounts.google.com/o/oauth2/auth?...)</code>.</li> <li>An input box prompting for an <code>Authorization code</code>.</li> <li>Follow these steps:<ul> <li>Click the URL. A new browser tab will open.</li> <li>Choose the Google Account associated with the Google Drive you want to use (likely the same one you use for Colab).</li> <li>Review the permissions Colab is requesting (it needs access to view/manage files it interacts with) and click \"Allow\" or \"Permit\".</li> <li>Google will provide you with a long authorization code. Copy this entire code.</li> <li>Switch back to your Colab notebook tab.</li> <li>Paste the copied code into the input box in the cell's output area.</li> <li>Press Enter.</li> </ul> </li> </ul> </li> <li>Confirmation: After a few moments, you should see the output <code>Mounted at /content/drive</code>.</li> <li>Verify in File Browser: Check the File Browser pane again (Folder icon). A new folder named <code>drive</code> should now be visible . If you expand it, you'll see <code>MyDrive</code>, which represents the root folder of your connected Google Drive.     </li> </ol> <p>Accessing Files in Code: Files within your mounted Google Drive are accessed via paths that always start with <code>/content/drive/MyDrive/</code>.</p> <ul> <li>If you have a file <code>my_course_data.csv</code> directly in your Google Drive's main (\"My Drive\") area, the path is:     <code>/content/drive/MyDrive/my_course_data.csv</code></li> <li>If you created a folder named <code>Data Course</code> in your Drive and put the file inside that folder, the path is:     <code>/content/drive/MyDrive/Data Course/my_course_data.csv</code></li> </ul> <pre><code># Example path within a 'Data Course' folder in Drive\ndrive_file_path = '/content/drive/MyDrive/Data Course/my_course_data.csv'\n\n# Example using Polars\nimport polars as pl\n\ntry:\n  df_drive = pl.read_csv(drive_file_path)\n  print(\"Successfully read from Google Drive:\")\n  print(df_drive.head(3))\nexcept Exception as e:\n  print(f\"Error reading file from Drive: {e}\")\n  print(f\"Does the file exist at '{drive_file_path}'?\")\n  print(\"Did you successfully mount your drive?\")\n</code></pre> <p>Key Benefit: Files accessed this way are durable. They reside in your Google Drive and will be available whenever you reconnect your Drive in future Colab sessions (you do need to re-run the <code>drive.mount()</code> cell in each new session).</p> <p>Recommendation: To stay organized, create a specific folder in your main Google Drive (e.g., <code>Data_Programming_Essentials</code>) and store all your course-related notebooks and datasets within it.</p>"},{"location":"getting-started/colab/data-files/#navigating-and-getting-file-paths","title":"Navigating and Getting File Paths","text":"<p>Whether using session storage or mounted Drive, the File Browser pane (Folder icon) lets you visually navigate folders. Typing long paths, especially Drive paths, is tedious and error-prone. Use this shortcut:</p> <ol> <li> <p>In the File Browser, navigate to the file you need .</p> <p></p> </li> <li> <p>Right-click on the filename (or click the three vertical dots <code>\u22ee</code> next to it) .</p> <p></p> </li> <li> <p>From the menu that appears, select \"Copy path\" .</p> <p></p> </li> <li> <p>Now, go to your code cell and paste (<code>Ctrl+V</code> or <code>Cmd+V</code>) the copied path directly into your code where the filename/path is needed (e.g., inside <code>pl.read_csv('PASTED_PATH_HERE')</code>).</p> </li> </ol> <p>This \"Copy path\" feature helps ensure you have the exact, correct path, preventing <code>FileNotFoundError</code> issues due to typos.</p>"},{"location":"getting-started/colab/essential-techniques/","title":"Essential Techniques","text":"<p>Beyond the basic interface and file handling, here are a few more tools and techniques crucial for your work in Colab.</p>"},{"location":"getting-started/colab/essential-techniques/#installing-python-packages","title":"Installing Python Packages","text":"<p>Python's power comes from its vast ecosystem of packages or libraries \u2013 collections of pre-written code that provide specific functionalities (like Polars for data manipulation, Altair for plotting, Scikit-learn for machine learning).</p> <ul> <li>Colab's Pre-installed Packages: Colab comes pre-installed with many common data science packages (like <code>numpy</code>, <code>pandas</code>, <code>scikit-learn</code>, <code>matplotlib</code>, etc.). Often, key libraries for this course like <code>polars</code> and <code>altair</code> might also be pre-installed or install quickly on first use.</li> <li> <p>Installing Additional Packages: If you need a package not included by default, or want to ensure you have a specific version, you can install it using <code>pip</code> (Python's package installer). Prefix the command with <code>!</code> in a code cell to run it as a shell command:</p> <p><pre><code># Example: Install the 'seaborn' plotting library\n!pip install seaborn\n\n# Example: Install multiple packages\n# !pip install package-one package-two\n\n# Tip: Add '--quiet' or '-q' to reduce installation output messages\n!pip install missingno --quiet\n</code></pre> * Temporary Installation: Remember, packages installed this way are temporary for the current Colab session. If you restart your runtime, you must re-run the <code>!pip install</code> cell(s) for any custom packages your notebook needs. Place these commands early in your notebook for easy access.</p> </li> </ul> <p>A Note on Package Versions, API Changes, and Deprecation</p> <p>Python libraries like Polars and Altair are under active development. This means they get updated frequently with new features, bug fixes, and performance improvements. However, these updates can sometimes change how the library works, which might affect code you find online or in older resources.</p> <ul> <li>Package Versions: Libraries typically use version numbers (e.g., <code>polars 1.2.3</code>). These often follow a pattern like <code>MAJOR.MINOR.PATCH</code>:<ul> <li><code>PATCH</code> changes usually indicate internal bug fixes.</li> <li><code>MINOR</code> changes typically add new features compatibly.</li> <li><code>MAJOR</code> changes often signal breaking changes.</li> <li>You can check the installed version of a library in your Colab environment:     <pre><code>import polars\nimport altair\n\nprint(f\"Polars version installed: {polars.__version__}\")\nprint(f\"Altair version installed: {altair.__version__}\")\n</code></pre></li> </ul> </li> <li>Breaking API Changes: Sometimes, library updates need to change the way existing functions or methods work (e.g., renaming a function, changing the required arguments, altering default behavior). These are \"breaking changes\" because code written for the older version might no longer run correctly with the newer version. Library authors try to minimize these, but they happen as libraries evolve and improve. An <code>AttributeError</code> (e.g., \"object has no attribute 'old_function_name'\") or <code>TypeError</code> (e.g., \"function got an unexpected keyword argument 'old_argument'\") can sometimes be a symptom of an API change.</li> <li>Deprecation Warnings: Libraries often provide advance notice before making a breaking change. You might see a <code>DeprecationWarning</code> or <code>FutureWarning</code> when you run code using an older feature. This warning usually doesn't stop your code from running right now, but it's telling you \"this way of doing things will be removed in a future version; please switch to the new recommended way.\" The warning message often suggests the alternative function or method to use.</li> </ul> <p>Why This Matters for You: You might find a code example online (from a blog, tutorial, Stack Overflow, or even an older book) that doesn't work immediately in your current Colab environment. This could be because the example was written for an older version of a library like Polars or Altair, and there has been a breaking change or deprecation since then.</p> <p>What to Do When Old Code Examples Break or Warn:</p> <ol> <li>Check Versions: Note the version of the library installed in your Colab environment (using <code>print(library.__version__)</code>). If the tutorial mentions a version, see if it's different.</li> <li>Read Errors Carefully: Look closely at <code>AttributeError</code> or <code>TypeError</code> messages \u2013 they might hint that a function or argument name has changed.</li> <li>Heed Warnings: Don't ignore <code>DeprecationWarning</code> or <code>FutureWarning</code>. Look up the function mentioned in the warning in the library's current documentation to find the recommended alternative.</li> <li>Consult Official Documentation: The current official documentation for the library (e.g., the Polars or Altair websites) is the most reliable source of truth for how the library works now. Search the documentation for the function or feature you're trying to use.</li> <li>Specify Versions (Use Cautiously): If you absolutely need to run an old code example as-is (perhaps just to understand it), you can try installing the specific older version the example might have used:     <pre><code># Caution: Installs an older version, potentially missing new features/fixes\n# !pip install polars==0.19.0\n# !pip install altair==4.2.0\n</code></pre> However, do this sparingly. The better long-term approach is usually to understand the change and update the code example to work with the current library versions, using the official documentation as your guide. Relying on old library versions means you miss out on important updates and bug fixes.</li> </ol> <p>Understanding that libraries evolve and knowing how to consult documentation are key skills for working with Python effectively!</p>"},{"location":"getting-started/colab/essential-techniques/#leveraging-built-in-gemini-ai","title":"Leveraging Built-in Gemini AI","text":"<p>As of mid-2025, Google Colab features integrated AI assistance, often leveraging models from the Gemini family. When used thoughtfully, this can significantly accelerate your learning and coding process.</p> <ul> <li>Accessing AI: Look for AI-related icons directly within the cell toolbar   (common icons include \u2728, the Gemini logo, or similar). You might also find options within the <code>Tools</code> menu or via the Command Palette (Ctrl+Shift+P). Note: The exact interface for AI features can evolve, so familiarize yourself with the current layout. </li> <li> <p>How AI Can Help You Learn:</p> <ul> <li>Code Generation: Ask for code snippets. Example prompt: \"Generate Python code using Polars to group the DataFrame 'df' by the 'category' column and calculate the average 'value'.\" Critically review generated code before running!</li> <li>Code Explanation: Select a code cell (yours or one you found) and ask the AI to clarify it. Example prompt: \"Explain what this block of Python code does step-by-step.\"</li> <li>Debugging Assistance: If you get an error, paste the error message and ask for help. Example prompt: \"Explain this Python error: NameError: name 'data' is not defined. How can I fix it?\"</li> <li>Adding Comments/Documentation: Ask the AI to document your code. Example prompt: \"Add comments to this Python function explaining each part.\"</li> <li>Learning from Examples: Paste code snippets from external sources (like documentation or tutorials) and ask the AI to explain them. This is a great way to deconstruct and learn new patterns. (See Section 7: Best Practices for a detailed workflow on this).</li> </ul> </li> </ul> <p>Important Considerations:</p> <ul> <li>AI is an Assistant, Not an Oracle: AI models can make mistakes (generate incorrect code, provide inaccurate explanations). Always think critically about the output. Does it make logical sense? Does the code work correctly?</li> <li>Prioritize Understanding: Use AI to aid your learning, not replace it. If AI generates code, strive to understand why it works that way. Ask follow-up questions.</li> <li>Be Mindful of Data: Check Google's terms regarding data privacy if you are working with sensitive information.</li> <li>Stay Updated: AI features change quickly. Refer to official Google Colab documentation for the latest capabilities and usage guidelines. </li> </ul>"},{"location":"getting-started/colab/essential-techniques/#basic-debugging","title":"Basic Debugging","text":"<p>Writing code involves finding and fixing errors \u2013 a process called debugging. Don't be intimidated by error messages; they are valuable clues!</p> <ul> <li>Errors are Normal: Expect to see errors, especially when learning. They don't mean you've failed; they mean the computer needs more precise instructions.</li> <li> <p>The Traceback: When Python code fails in Colab, it usually prints an error message and a Traceback. The traceback shows the sequence of function calls leading up to the error. It can look long, but focus on the key parts:</p> <ol> <li>Scroll to the Bottom: The most specific information about the error is almost always at the very end.</li> <li>Identify Error Type: Look for the line stating the error type (e.g., <code>SyntaxError</code>, <code>NameError</code>, <code>TypeError</code>, <code>FileNotFoundError</code>, <code>AttributeError</code>, <code>ValueError</code>, <code>IndexError</code>). This gives you the general category.</li> <li>Read the Message: The text immediately following the error type provides details about the specific problem. Read this carefully!</li> <li>Locate the Line: The traceback often points an arrow (<code>----&gt;</code>) to the exact line in your code cell where the error was detected.</li> <li> <p>Common Error Types and Meanings:</p> </li> <li> <p><code>SyntaxError</code>: A typo or grammatical mistake in your Python code itself (e.g., missing colon, parenthesis, incorrect indentation). Check the indicated line meticulously.</p> </li> <li><code>NameError</code>: You tried to use a variable or function name that hasn't been defined yet in the current execution order. Did you run the defining cell? Did you misspell the name? (See Section 3).</li> <li><code>TypeError</code>: You tried an operation on incompatible data types (e.g., adding text to a number).</li> <li><code>FileNotFoundError</code>: Python couldn't find the file at the path you specified. Check the path's spelling (case-sensitive!), ensure the file exists where you think it does, and make sure Drive is mounted if applicable. Use \"Copy path\"! (See Section 4).</li> <li><code>AttributeError</code>: You tried to use a method or attribute that doesn't exist for that specific object type (e.g., <code>my_dataframe.calculate_sumz()</code> when the method is actually <code>calculate_sums()</code>). Check for typos or consult the library's documentation.</li> <li><code>ValueError</code>: The function received an argument of the correct type but an inappropriate value.</li> <li><code>IndexError</code>: You tried to access an item in a sequence (like a list) using an index number that is out of bounds.</li> <li>Debugging Strategy: Read the error message -&gt; Locate the line -&gt; Think about what the message means in the context of your code -&gt; Formulate a hypothesis -&gt; Make a change -&gt; Re-run the cell. Repeat if necessary! Use the AI assistant if you get stuck understanding an error.</li> </ol> </li> </ul>"},{"location":"getting-started/colab/going-deeper/","title":"Going Deeper (Optional Exploration)","text":"<p>You've now covered all the essential Colab features needed to succeed in this course! This final section explores a few additional tools and concepts. These are optional \u2013 you don't strictly need them for your assignments \u2013 but learning them can make your workflow faster, more efficient, or help you understand the Colab environment more thoroughly.</p>"},{"location":"getting-started/colab/going-deeper/#useful-keyboard-shortcuts","title":"Useful Keyboard Shortcuts","text":"<p>While Colab's menus and toolbars are user-friendly, using keyboard shortcuts is often much faster once you learn a few key combinations. This can significantly speed up writing and manipulating your notebooks.</p> <p>Command Mode vs. Edit Mode: A key concept for shortcuts is understanding Colab's (and Jupyter's) modes:</p> <ul> <li>Edit Mode: You're actively typing inside a cell (usually indicated by a green border around the cell and a blinking cursor). Keystrokes insert text or code.</li> <li>Command Mode: You've selected a cell but aren't typing inside it (usually indicated by a blue border). Keystrokes act as commands on the cell itself (like deleting it, changing its type, etc.).<ul> <li>Press the &lt;prefix&gt; key combo, Ctrl+M or Esc,to switch from Edit Mode to Command Mode.</li> <li>Press <code>Enter</code> (or click inside a cell) to switch from Command Mode to Edit Mode.</li> </ul> </li> </ul> <p>Some Highly Useful Shortcuts:</p> <p>(Remember to be in the correct mode - usually Command Mode unless otherwise specified)</p> <ul> <li>Essential Execution:<ul> <li><code>Shift + Enter</code>: Run current cell, select cell below (works in both modes).</li> <li><code>Ctrl + Enter</code> (<code>Cmd + Enter</code> on Mac): Run selected cell(s), keep focus here (works in both modes).</li> <li><code>Alt + Enter</code> (<code>Option + Enter</code> on Mac): Run current cell, insert new code cell below (works in both modes).</li> </ul> </li> <li>Cell Manipulation (Command Mode):<ul> <li><code>A</code>: Insert new code cell Above current cell.</li> <li><code>B</code>: Insert new code cell Below current cell.</li> <li><code>D</code>, <code>D</code> (press <code>D</code> twice quickly): Delete selected cell(s) (use with care!).</li> <li><code>M</code>: Change cell type to Markdown (Text).</li> <li><code>Y</code>: Change cell type back to code (Y not intuitive, just is).</li> <li><code>Z</code>: Undo last cell operation (like delete).</li> <li><code>Shift + M</code>: Merge selected cells (select multiple with <code>Shift + Up/Down Arrow</code> first).</li> </ul> </li> <li>Editing (Edit Mode):<ul> <li><code>Ctrl + /</code> (<code>Cmd + /</code> on Mac): Comment or uncomment the selected line(s) of code with <code>#</code>.</li> </ul> </li> </ul> <p>Discover More: This is just a small sample! To see the full list of available shortcuts and even customize them, go to the main menu: <code>Tools</code> -&gt; <code>Keyboard shortcuts</code>. Learning just a few common ones can make a noticeable difference in your speed.</p>"},{"location":"getting-started/colab/going-deeper/#code-snippets","title":"Code Snippets","text":"<p>Colab includes a library of pre-written code snippets for common tasks, which can save you time and help you discover functionalities.</p> <ul> <li>How to Access: In the Left Sidebar, click the Code Snippets icon  (often looks like <code>&lt; &gt;</code>).</li> <li>Browse &amp; Insert: A pane will open with searchable snippets, often categorized (e.g., \"Visualization\", \"Importing data\") . Find a snippet you need (like \"Mount Google Drive\" or examples for plotting libraries)  and click the \"Insert\" button (or sometimes just click the snippet). The necessary code cell(s) will be added to your notebook.     </li> <li>Usefulness: Great for boilerplate code (like Drive mounting), exploring examples (especially for visualizations), or finding code for interacting with specific Google services.</li> </ul>"},{"location":"getting-started/colab/going-deeper/#the-command-palette","title":"The Command Palette","text":"<p>If you can't remember a keyboard shortcut or where a command is hidden in the menus, the Command Palette is a powerful search tool for actions.</p> <ul> <li>How to Access: Use the toolbar  or press <code>Ctrl + Shift + P</code> (or <code>Cmd + Shift + P</code> on Mac).</li> <li> <p>Search &amp; Execute: A search bar pops up. Start typing the name of the action you want  (e.g., \"markdown\", \"restart\", \"clear all\", \"table of contents\"). The list filters as you type. Select the desired command using arrow keys and <code>Enter</code>, or click it .</p> <p></p> </li> <li> <p>Benefit: Allows quick keyboard access to virtually any Colab command without needing to memorize everything.</p> </li> </ul>"},{"location":"getting-started/colab/going-deeper/#understanding-resource-limits","title":"Understanding Resource Limits","text":"<p>While Colab's free tier is generous, it's not infinite. Knowing the limitations helps prevent unexpected interruptions.</p> <ul> <li>Key Limits:<ul> <li>Session Timeouts: Your connection to the Colab backend (the Kernel) isn't permanent. It will disconnect after a certain period of inactivity (often 30-90 minutes) or after reaching a maximum total runtime (which can be around 12 hours, but varies).</li> <li>RAM (Memory): You get a significant amount of RAM (~12GB generally), but loading extremely large datasets or performing very complex calculations can exceed this, potentially crashing your session.</li> <li>Disk Space: The temporary session storage (where direct uploads go) is also limited.</li> <li>GPU/TPU: Access to specialized hardware (which we generally won't need for this course) is also restricted in time and availability.</li> </ul> </li> <li>Consequences of Limits/Timeouts: When your session ends (due to timeout or crash), the Kernel state is lost. This means:<ul> <li>All variables and loaded data in memory disappear.</li> <li>Any files uploaded to temporary session storage are deleted.</li> <li>Custom installed packages (<code>!pip install</code>) are removed.</li> <li>You will need to reconnect and re-run your initialization cells (imports, Drive mount, installs, data loading).</li> </ul> </li> <li>Working Within Limits:<ul> <li>Use Google Drive for persistent storage of notebooks and data (Section 4.3).</li> <li>Develop the habit of structuring your notebooks so initialization steps are easy to re-run.</li> <li>Use <code>Restart and run all</code> (Section 3.4) periodically to ensure your workflow is robust.</li> </ul> </li> <li>Paid Options: Google offers Colab Pro/Pro+ with extended runtimes and more resources, but these are not necessary for completing this course successfully.</li> </ul>"},{"location":"getting-started/colab/kernel-state/","title":"Kernel State","text":"<p>Understanding this section is crucial for working effectively with Colab (and any Jupyter-style notebook). It addresses the most common source of confusion for newcomers: the difference between the order of cells on your screen and the order in which the computer actually executes your commands.</p>"},{"location":"getting-started/colab/kernel-state/#execution-order","title":"Execution Order","text":"<p>It's Not Just Top-to-Bottom: Understanding Execution Flow</p> <p>Unlike a simple text document or a traditional computer script that typically runs line-by-line from start to finish, a Colab notebook is more interactive and stateful. This means the notebook's 'memory' or 'state' (like the values of variables you create) depends entirely on the sequence in which you have executed the cells, not just their top-to-bottom arrangement on the page.</p>"},{"location":"getting-started/colab/kernel-state/#the-common-pitfall","title":"The Common Pitfall","text":"<p>Imagine you write some code, run it, then scroll back up, change an earlier cell, and run only that changed cell. If you then continue running cells further down without re-running the intermediate steps, those later cells might use outdated information from before you made your change! This leads to results that don't seem to make sense based on the code you see.</p> <p>Let's See it in Action (Example):</p> <p>Consider these three simple code cells in your notebook:</p> <p>(Code Cell 1) <pre><code># Define an initial price for an item\nprice = 100\nprint(f\"Cell 1 Executed: Initial price is currently {price}\")\n</code></pre></p> <p>(Code Cell 2) <pre><code># Calculate a 10% tax based on the current price\ntax = price * 0.10\nprint(f\"Cell 2 Executed: Tax calculated is currently {tax}\")\n</code></pre></p> <p>(Code Cell 3) <pre><code># Calculate the final cost\nfinal_cost = price + tax\nprint(f\"Cell 3 Executed: Final cost is currently {final_cost}\")\n</code></pre></p> <p>Scenario 1: Running in Order (The Normal Flow)</p> <ol> <li>Run Cell 1 -&gt; Output includes <code>price is currently 100</code></li> <li>Run Cell 2 -&gt; Output includes <code>Tax calculated is currently 10.0</code></li> <li>Run Cell 3 -&gt; Output includes <code>Final cost is currently 110.0</code> This works exactly as you'd expect.</li> </ol> <p>Scenario 2: The Pitfall (Running Out of Order)</p> <ol> <li>Let's say you ran Cells 1, 2, and 3 just like above. The final cost is 110.0.</li> <li>Now, you decide the price should have been 200. You go back to Cell 1, change <code>price = 100</code> to <code>price = 200</code>, and run only Cell 1.     Output: <code>Cell 1 Executed: Initial price is currently 200</code></li> <li>You skip running Cell 2.</li> <li>You jump down and run Cell 3 again.<ul> <li>What you might expect: 220.0 (because price is now 200, so 200 + 20 tax).</li> <li>What you actually get: <code>Cell 3 Executed: Final cost is currently 210.0</code> (Huh?!)</li> </ul> </li> </ol> <p>Why the difference (210 vs 220)? When you re-ran Cell 3, the computer used the current value of <code>price</code> (which was updated to 200 when you re-ran Cell 1). However, it used the value of <code>tax</code> that was calculated the last time Cell 2 was run (which was 10.0, based on the old price of 100). You never re-ran Cell 2 to update the <code>tax</code> variable based on the new price! So Cell 3 calculated <code>200 + 10.0 = 210.0</code>.</p> <p>Key Takeaway: The notebook doesn't automatically recalculate everything below a change. You must explicitly re-run all dependent cells to update the notebook's state correctly.</p>"},{"location":"getting-started/colab/kernel-state/#the-kernel","title":"The Kernel","text":"<p>What actually keeps track of variables like <code>price</code> and <code>tax</code>? It's a background process called the Kernel.</p> <ul> <li>Think of the Kernel as the dedicated Python engine running behind your specific notebook session.</li> <li>When you run a code cell, the code is sent to the Kernel.</li> <li>The Kernel executes the code and, crucially, remembers the state \u2013 any variables created (<code>price</code>, <code>tax</code>), functions defined, libraries imported (<code>import polars as pl</code>), etc.</li> <li>This memory (state) persists throughout your session based on the sequence of cell executions.</li> </ul>"},{"location":"getting-started/colab/kernel-state/#hidden-state","title":"Hidden State","text":"<p>The Kernel's memory can sometimes lead to confusing situations, often called hidden state. This happens because the Kernel can remember things even if the cell that defined them is no longer visible in your notebook.</p> <p>Example:</p> <ol> <li>In Cell 1, type and run: <code>temporary_variable = \"Important Info\"</code></li> <li>In Cell 2, type and run: <code>print(temporary_variable)</code> -&gt; Output: <code>Important Info</code></li> <li>Now, delete Cell 1 completely from your notebook.</li> <li>Run Cell 2 again.</li> </ol> <p>Result: Surprisingly, Cell 2 might still work and print <code>Important Info</code>! The Kernel remembers the <code>temporary_variable</code> from when Cell 1 was executed, even though the defining cell is gone.</p> <p>This hidden state can make your notebook behave unexpectedly and makes it hard to guarantee that someone else running your notebook (or you, coming back later) will get the exact same results just by reading the visible cells.</p>"},{"location":"getting-started/colab/kernel-state/#managing-the-kernel","title":"Managing the Kernel","text":"<p>Thankfully, Colab provides essential tools to manage the Kernel and reset its state, located in the Runtime menu. Mastering these is key to avoiding frustration!</p> <p>Key Runtime Options:</p> <ul> <li><code>Restart runtime</code> (<code>Runtime</code> -&gt; <code>Restart runtime</code> or shortcut <code>Ctrl+M .</code>)<ul> <li>What it does: Stops the current Kernel and starts a completely fresh one.</li> <li>Effect: Clears all variables, function definitions, and imported libraries from the Kernel's memory. Locally uploaded files (session storage) might also be cleared. Your code cells remain, but the memory is wiped clean.</li> <li>When to use: This is your go-to fix when the notebook feels stuck, is behaving unpredictably, or you suspect hidden state issues. After restarting, you'll need to re-run necessary cells (like imports, data loading, variable definitions) from the top.</li> </ul> </li> <li><code>Restart and run all</code> (<code>Runtime</code> -&gt; <code>Restart and run all</code>)<ul> <li>What it does: First, it restarts the runtime (clearing the state, as above). Then, it automatically starts executing every single cell in your notebook sequentially, from the very first cell to the very last.</li> <li>Why it's crucial: This perfectly simulates running your notebook from a completely clean slate. It's the gold standard for checking reproducibility. Does your notebook run correctly from top to bottom without errors and produce the expected final results?</li> <li>Recommendation: Make it a habit to use <code>Restart and run all</code> before submitting any assignment or finalizing an analysis. It ensures your work is reliable and free from state-related bugs.</li> </ul> </li> <li><code>Interrupt execution</code> (<code>Runtime</code> -&gt; <code>Interrupt execution</code> or shortcut <code>Ctrl+M I</code>)<ul> <li>What it does: Attempts to stop the code currently being executed by the Kernel (e.g., if a cell is taking unexpectedly long or got stuck in a loop).</li> <li>When to use: When a cell seems frozen. It doesn't clear the memory, just stops the current task. You might still need to restart the runtime afterwards if things remain unstable.</li> </ul> </li> </ul> <p>Your Reproducibility Mantra:</p> <p>\"When things act weird, <code>Restart runtime</code>. To check my final work, <code>Restart and run all</code>.\"</p>"},{"location":"getting-started/python/","title":"Introduction to Python","text":"<ul> <li> Starter Colab Notebook</li> <li> Why Python?</li> <li> Modeling Information</li> <li> Modeling Structures</li> <li> Polars Teaser</li> <li> Automating Tasks</li> <li> Structuring Code</li> <li> Next Steps</li> </ul>"},{"location":"getting-started/python/code-organization/","title":"Part 6: Structuring Code - For Reusability and Clarity","text":"<p>As your analysis tasks become more complex, you'll want ways to organize your code to make it more readable, maintainable, and reusable. Copying and pasting blocks of code is generally a bad practice \u2013 if you find a mistake or need to make an update, you have to change it in multiple places! Python offers structures like functions and leverages the concepts of objects and classes to help manage complexity.</p>"},{"location":"getting-started/python/code-organization/#61-functions-creating-reusable-commands","title":"6.1 Functions: Creating Reusable Commands","text":"<p>Motivation: Imagine you need to perform the same specific calculation (like calculating a standardized score or cleaning text in a particular way) at several different points in your analysis. Instead of writing the same lines of code repeatedly, you can package that logic into a function.</p> <ul> <li>DRY (Don't Repeat Yourself): Functions allow you to define a block of code once and then call it whenever you need it, potentially with different inputs.</li> <li>Modularity &amp; Readability: Breaking a large analysis down into smaller, well-named functions makes the overall workflow much easier to understand, test, and debug. Each function handles one specific part of the job.</li> <li>Abstraction: When you call a function, you focus on what it does (e.g., <code>calculate_profit(revenue, cost)</code>) rather than getting bogged down in the specific lines of code how it does it.</li> </ul> <p>Defining a Function: You create your own functions using the <code>def</code> keyword:</p> <pre><code>def function_name(parameter1, parameter2, ...):\n    \"\"\"Optional: A docstring explaining what the function does.\"\"\" # Good practice!\n    # --- Indented function body ---\n    # Code to perform the function's task goes here.\n    # This code only runs when the function is CALLED.\n    # It can use the input parameters.\n    print(f\"Function received: {parameter1}, {parameter2}\")\n    calculation_result = parameter1 / parameter2 # Example action\n\n    # Use 'return' to send a value back out of the function\n    return calculation_result\n</code></pre> <ul> <li><code>def</code>: The keyword that starts a function definition.</li> <li><code>function_name</code>: Give your function a descriptive name (use <code>snake_case</code>).</li> <li><code>(parameters)</code>: Inside the parentheses are parameters \u2013 variables that act as placeholders for the inputs the function needs. A function can have zero, one, or many parameters.</li> <li><code>:</code>: A colon ends the function definition line.</li> <li>Indented Body: All the code belonging to the function must be consistently indented (usually 4 spaces).</li> <li><code>\"\"\"Docstring\"\"\"</code>: An optional (but highly recommended) string literal right after the <code>def</code> line explaining the function's purpose, parameters, and what it returns.</li> <li><code>return value</code>: The <code>return</code> statement exits the function and sends a specified <code>value</code> back to the part of the code that called it. If <code>return</code> is omitted, the function implicitly returns <code>None</code>.</li> </ul> <p>Calling a Function: To execute the function's code, you call it using its name followed by parentheses <code>()</code>, providing the required input values (arguments) inside the parentheses.</p> <pre><code># Call the function defined above, providing arguments 100 and 5\nreturned_value = function_name(100, 5) # 100 is passed to parameter1, 5 to parameter2\n\nprint(f\"The function returned: {returned_value}\")\n# Expected output based on example def:\n# Function received: 100, 5\n# The function returned: 20.0\n</code></pre>"},{"location":"getting-started/python/code-organization/#62-more-on-calling-functions-understanding-signatures-and-argument-passing","title":"6.2 More on Calling Functions: Understanding Signatures and Argument Passing","text":"<p>When you use functions from libraries (or even your own more complex functions), you'll find that there are specific rules about how you can pass arguments. The \"signature\" of a function defines the parameters it accepts and how arguments must be provided. Understanding these conventions is crucial for using libraries effectively.</p>"},{"location":"getting-started/python/code-organization/#function-signatures","title":"Function Signatures","text":"<p>A function's signature includes its name and the parameters it can accept. Library designers can enforce certain ways arguments are passed to ensure clarity and correctness.</p>"},{"location":"getting-started/python/code-organization/#positional-and-keyword-arguments","title":"Positional and Keyword Arguments","text":"<p>You've already seen positional arguments, where the order matters: <code>function_name(value1, value2)</code>. You can also often pass arguments using their parameter names; these are called keyword arguments:</p> <p><pre><code>def describe_pet(animal_type, pet_name):\n    print(f\"I have a {animal_type} named {pet_name}.\")\n\ndescribe_pet(\"hamster\", \"Harry\") # Positional arguments\ndescribe_pet(pet_name=\"Lucy\", animal_type=\"dog\") # Keyword arguments\ndescribe_pet(\"cat\", pet_name=\"Whiskers\") # Mix of positional and keyword\n</code></pre> Once you use a keyword argument, all subsequent arguments in that call must also be keyword arguments.</p>"},{"location":"getting-started/python/code-organization/#keyword-only-arguments","title":"Keyword-Only Arguments","text":"<p>Some functions or methods might be designed so that certain arguments must be passed as keyword arguments. This is often used to improve code readability when a function has many parameters, or to make it clear what a particular value represents.</p> <ul> <li>How it's defined (for your understanding):     A <code>*</code> in the function definition indicates that all parameters after it are keyword-only.     <pre><code>def create_user(username, *, preferred_language, send_newsletter):\n    print(f\"User: {username}, Language: {preferred_language}, Newsletter: {send_newsletter}\")\n</code></pre></li> <li>How to call it:     You must use keywords for <code>preferred_language</code> and <code>send_newsletter</code>.     <pre><code>create_user(\"john_doe\", preferred_language=\"English\", send_newsletter=True)\n# create_user(\"john_doe\", \"English\", True) # This would cause an error\n</code></pre>     Library providers might use this to ensure optional configuration parameters are explicitly named.</li> </ul>"},{"location":"getting-started/python/code-organization/#positional-only-arguments","title":"Positional-Only Arguments","text":"<p>Less commonly for users to define but possible to encounter, some arguments might be required to be positional only.</p> <ul> <li>How it's defined (for your understanding):     A <code>/</code> in the function definition indicates that all parameters before it are positional-only.     <pre><code>def item_price(item_id, quantity, / , currency=\"USD\"):\n    print(f\"ID: {item_id}, Qty: {quantity}, Currency: {currency}\")\n</code></pre></li> <li>How to call it: <code>item_id</code> and <code>quantity</code> must be positional.     <pre><code>item_price(101, 5)\nitem_price(102, 3, currency=\"EUR\")\n# item_price(item_id=101, quantity=5) # This would cause an error for item_id and quantity\n</code></pre>     Often, you'll see functions that have a few required positional arguments at the beginning, followed by optional keyword arguments.</li> </ul>"},{"location":"getting-started/python/code-organization/#arbitrary-positional-arguments-args","title":"Arbitrary Positional Arguments (<code>*args</code>)","text":"<p>Sometimes, you want a function to accept an arbitrary number of positional arguments without defining each one. This is where <code>*args</code> comes in. Inside the function, <code>args</code> becomes a tuple containing all the extra positional arguments.</p> <p><pre><code>def print_all_items(fixed_arg, *items):\n    print(f\"Fixed argument: {fixed_arg}\")\n    print(\"Additional items:\")\n    for item in items:\n        print(f\"- {item}\")\n\nprint_all_items(\"Shopping List\", \"milk\", \"eggs\", \"bread\")\n# Output:\n# Fixed argument: Shopping List\n# Additional items:\n# - milk\n# - eggs\n# - bread\n\nprint_all_items(\"Tasks\") # No additional items\n# Output:\n# Fixed argument: Tasks\n# Additional items:\n</code></pre> Many library functions use <code>*args</code> to allow you to pass multiple column names, values, or objects.</p>"},{"location":"getting-started/python/code-organization/#arbitrary-keyword-arguments-kwargs","title":"Arbitrary Keyword Arguments (<code>**kwargs</code>)","text":"<p>Similarly, <code>**kwargs</code> allows a function to accept any number of keyword arguments that are not explicitly defined in the function's signature. Inside the function, <code>kwargs</code> becomes a dictionary containing these additional keyword arguments.</p> <p><pre><code>def configure_system(main_setting, **options):\n    print(f\"Main setting: {main_setting}\")\n    print(\"Other options:\")\n    for key, value in options.items():\n        print(f\"- {key}: {value}\")\n\nconfigure_system(\"active\", debug_mode=True, log_level=\"info\", retries=3)\n# Output:\n# Main setting: active\n# Other options:\n# - debug_mode: True\n# - log_level: info\n# - retries: 3\n\nconfigure_system(\"inactive\")\n# Output:\n# Main setting: inactive\n# Other options:\n</code></pre> This is extremely common in data science libraries for passing various optional parameters, configurations, or styling attributes to functions and methods (e.g., <code>polars.read_csv(source, **read_options)</code>, <code>altair.Chart(data, **chart_properties)</code>).</p>"},{"location":"getting-started/python/code-organization/#the-importance-of-documentation","title":"The Importance of Documentation \ud83d\udcd6","text":"<p>Functions in libraries can combine all these types of parameters. For example: <code>def complex_function(pos1, pos2, /, pos_or_kw1, *, kw_only1, **kwargs):</code></p> <p>You are not expected to memorize the exact signature of every function or method you use. The most crucial skill is to consult the official documentation for the library function or method you are calling. The documentation will always specify:</p> <ul> <li>Required positional arguments.</li> <li>Optional arguments and their default values.</li> <li>Whether arguments are positional-only, keyword-only, or can be either.</li> <li>If <code>*args</code> or <code>**kwargs</code> are accepted and for what purpose.</li> </ul> <p>Example: Reusable Age Calculation Function Let's turn our earlier \"Age from Date of Birth\" logic into a reusable function. (Remember to <code>import datetime as dt</code> usually at the top of your notebook!)</p> <p><pre><code>import datetime as dt # Ensure datetime is imported\n\ndef calculate_age(date_of_birth):\n  \"\"\"Calculates the current age in years given a date_of_birth.\n\n  Args:\n    date_of_birth: A datetime.date object representing the date of birth.\n                   Can also be None, in which case None is returned.\n\n  Returns:\n    An integer representing the age, or None if date_of_birth was None.\n  \"\"\"\n  if date_of_birth is None:\n    return None # Handle missing input gracefully\n\n  if not isinstance(date_of_birth, dt.date):\n      print(\"Error: Input must be a datetime.date object.\")\n      return None # Handle incorrect input type\n\n  today = dt.date.today()\n  # Calculate basic year difference\n  age = today.year - date_of_birth.year\n\n  # Adjust if the birthday hasn't occurred yet this year\n  # Check if (current month, current day) tuple is earlier than (birth month, birth day) tuple\n  if (today.month, today.day) &lt; (date_of_birth.month, date_of_birth.day):\n    age -= 1 # Subtract 1 if birthday hasn't passed\n\n  return age\n\n# --- Now we can CALL the function easily and repeatedly ---\ndob_person1 = dt.date(1988, 10, 25)\ndob_person2 = dt.date(2002, 3, 15)\ndob_person3 = None\n\nage_person1 = calculate_age(dob_person1)\nage_person2 = calculate_age(dob_person2)\nage_person3 = calculate_age(dob_person3) # Handles None input\n\nprint(f\"Person 1 (Born {dob_person1}) Age: {age_person1}\")\nprint(f\"Person 2 (Born {dob_person2}) Age: {age_person2}\")\nprint(f\"Person 3 (Born {dob_person3}) Age: {age_person3}\")\n\n# Easily reuse for another date\nprint(f\"Someone born 1975-07-01 is Age: {calculate_age(dt.date(1975, 7, 1))}\")\n</code></pre> Functions are essential tools for writing cleaner, more organized, and more efficient Python code.</p>"},{"location":"getting-started/python/code-organization/#63-a-glimpse-into-objects-classes-understanding-our-libraries","title":"6.3 A Glimpse into Objects &amp; Classes (Understanding Our Libraries)","text":"<p>Motivation Revisited: Our <code>calculate_age</code> function works well, but notice that the function logically \"belongs\" with the date-of-birth data it operates on. This idea of bundling data together with the functions (actions) that are relevant to that data is the core concept behind Object-Oriented Programming (OOP).</p> <p>You've already been using objects extensively!</p> <ul> <li><code>\"some text\"</code> is an object of the <code>str</code> type (or class).</li> <li><code>[10, 20, 30]</code> is an object of the <code>list</code> type.</li> <li><code>dt.date(2025, 5, 2)</code> creates an object of the <code>date</code> type from the <code>datetime</code> module.</li> </ul>"},{"location":"getting-started/python/code-organization/#class-vs-object-blueprint-vs-instance","title":"Class vs. Object (Blueprint vs. Instance):","text":"<ul> <li>A Class is like a blueprint, template, or category definition. It specifies what kind of data (called attributes) objects of this class will hold, and what kind of actions (called methods) they can perform. Examples of classes we've seen: <code>str</code>, <code>int</code>, <code>list</code>, <code>dict</code>, <code>datetime.date</code>. When we use libraries like Polars, we'll encounter classes like <code>DataFrame</code> and <code>Series</code>.</li> <li>An Object is a specific instance created from a Class blueprint. Each object holds its own specific data values based on the blueprint. Examples: <code>my_name = \"Bob\"</code> is an object (instance) of the <code>str</code> class. <code>sales_figures = [100, 200]</code> is an object (instance) of the <code>list</code> class. <code>d1 = dt.date(2025, 5, 2)</code> is an object (instance) of the <code>date</code> class.</li> </ul>"},{"location":"getting-started/python/code-organization/#attributes-and-methods-an-objects-data-and-actions","title":"Attributes and Methods (An Object's Data and Actions):","text":"<p>Objects typically encapsulate both data and actions. You interact with these using dot notation (<code>.</code>):</p> <ol> <li> <p>Attributes: Data values stored inside the object. You access them using <code>object_name.attribute_name</code>.</p> <pre><code>d1 = dt.date(2025, 5, 2)\nprint(f\"The year attribute of d1 is: {d1.year}\")  # Accessing the 'year' attribute\nprint(f\"The month attribute of d1 is: {d1.month}\") # Accessing the 'month' attribute\n\n# When we use Polars later:\n# some_dataframe = pl.DataFrame(...)\n# print(some_dataframe.shape) # Accessing the 'shape' attribute (rows, cols)\n# print(some_dataframe.columns) # Accessing the 'columns' attribute (list of names)\n</code></pre> </li> <li> <p>Methods: Functions that are \"attached\" to the object and typically operate on the object's data or perform an action related to the object's purpose. You call methods using <code>object_name.method_name(arguments)</code> \u2013 notice the crucial parentheses <code>()</code> at the end (even if there are no arguments).     Methods follow the same calling conventions discussed for functions in section 6.1.1, including the use of positional, keyword, <code>*args</code>, and <code>**kwargs</code> arguments. Always check the method's documentation for specific calling requirements.</p> <pre><code>my_text = \"  needs cleaning  \"\ncleaned_text = my_text.strip() # Calling the strip() method ON the my_text string object\nprint(f\"Cleaned text: '{cleaned_text}'\")\n\nmy_numbers = [1, 2]\nmy_numbers.append(3) # Calling the append() method ON the my_numbers list object\nprint(f\"List after append: {my_numbers}\")\n\n# When we use Polars later:\n# filtered_data = some_dataframe.filter(pl.col('A') &gt; 10) # Calling the filter() method\n# sorted_data = some_dataframe.sort('column_name')       # Calling the sort() method\n</code></pre> </li> </ol>"},{"location":"getting-started/python/code-organization/#the-key-takeaway-for-this-course-practical-oop","title":"The Key Takeaway for This Course (Practical OOP):","text":"<p>You do not need to become an expert in designing or writing your own elaborate classes from scratch in this introductory course.</p> <p>However, it is absolutely essential that you:</p> <ol> <li>Understand the concept that objects bundle data (attributes) and actions (methods).</li> <li>Become comfortable with the dot notation syntax for accessing attributes (<code>object.attribute</code>) and calling methods (<code>object.method()</code>).</li> <li>Recognize the different ways arguments can be passed to functions and methods (positional, keyword, <code>*args</code>, <code>**kwargs</code>) and prioritize checking library documentation for their specific requirements.</li> </ol> <p>Why? Because the powerful data science libraries we will rely on \u2013 Polars, Altair, Scikit-learn \u2013 are fundamentally object-oriented. Your entire workflow will involve:</p> <ul> <li>Creating library objects (DataFrames, Series, Charts, Models, etc.).</li> <li>Inspecting their data using attributes (e.g., <code>my_df.columns</code>, <code>my_model.feature_names_in_</code>).</li> <li>Telling them what to do by calling their methods (e.g., <code>my_df.select(...)</code>, <code>my_chart.save(...)</code>, <code>my_model.fit(...)</code>, <code>my_model.predict(...)</code>), paying close attention to how arguments need to be supplied.</li> </ul> <p>Grasping this user-level interaction with objects and their methods/attributes, along with understanding function calling conventions, is the \"practical OOP\" you need to unlock the power of these essential data science tools.</p>"},{"location":"getting-started/python/debugging/","title":"Debugging","text":"<p>You've hit on a really important point! Using <code>print()</code>, <code>type()</code>, and <code>dir()</code> is indeed fundamental for basic Python troubleshooting, and students absolutely need to know how to use them for debugging.</p> <p>You are also right to question the best placement. Here\u2019s a breakdown of the pros and cons:</p> <ol> <li> <p>Placing in the Colab Documentation (e.g., Section 5.3 - Basic Debugging):</p> <ul> <li>Pros: Puts the debugging technique right where you discuss errors and tracebacks. Provides immediate, practical tools within the context of the IDE they are learning. Reinforces that debugging isn't just about reading errors but also about inspecting state.</li> <li>Cons: Assumes students have already learned these functions in Python. Might slightly blur the line between \"learning Colab\" and \"learning Python debugging\".</li> </ul> </li> <li> <p>Placing in the Introductory Python Lesson:</p> <ul> <li>Pros: Introduces these as core Python built-in functions early on. Logically fits with learning basic syntax, variables, and data types. Students understand what the functions are before needing them for debugging.</li> <li>Cons: Students might forget about their debugging utility by the time they encounter more complex errors later. The direct application to troubleshooting might be less immediate.</li> </ul> </li> </ol> <p>Recommendation (Best of Both Worlds):</p> <p>The most effective approach is likely to introduce these functions during your introductory Python lessons and then explicitly reference and reinforce their use for debugging within Section 5.3 of the Colab documentation.</p> <ul> <li>Python Intro Lesson: When teaching variables, data types, and basic operations, introduce:<ul> <li><code>print()</code>: As the primary way to see output and check values.</li> <li><code>type()</code>: To understand the kind of data a variable holds (essential in Python).</li> <li><code>dir()</code>: As a way to explore what attributes and methods an object has (can be introduced slightly later in the Python intro, perhaps when objects/methods are first discussed).</li> </ul> </li> <li> <p>Colab Docs - Section 5.3: After explaining how to read tracebacks, add a subsection like this:</p> <p>Using Basic Python Tools for Troubleshooting</p> <p>Beyond reading the error message, remember to use Python's built-in functions to inspect your code's state before the error occurs:</p> <ul> <li><code>print()</code>: This is your best friend for debugging! Insert <code>print()</code> statements just before the line that causes the error to check the values of variables involved. Are they what you expect?     <pre><code># Example: Check variables before a calculation\nprint(f\"Value of x before calculation: {x}\")\nprint(f\"Value of y before calculation: {y}\")\nresult = x / y # This line might cause a ZeroDivisionError if y is 0\n</code></pre></li> <li><code>type()</code>: If you get a <code>TypeError</code> or <code>AttributeError</code>, use <code>type()</code> to confirm the data type of your variable. Is it actually the type you think it is (e.g., a Polars DataFrame, a list, an integer)?     <pre><code># Example: Check the type if a method fails\nmy_variable = get_data_from_somewhere()\nprint(f\"The type of my_variable is: {type(my_variable)}\")\n# Now try to use a method specific to the expected type\n# my_variable.some_dataframe_method() # Might fail if it's not a DataFrame\n</code></pre></li> <li><code>dir()</code>: If you get an <code>AttributeError</code> (meaning you tried to use a method or attribute that doesn't exist), <code>dir(your_object)</code> can list all the valid attributes and methods for that object. This helps you spot typos in method names or discover the correct name.     <pre><code># Example: Explore available methods for a Polars DataFrame\n# import polars as pl\n# df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n# print(dir(df)) # Shows all methods/attributes, look for ones like 'filter', 'select' etc.\n</code></pre></li> </ul> </li> </ul> <p>This approach introduces the tools formally during the Python lessons and then jogs the students' memory about their practical application specifically for debugging within the Colab environment documentation. It makes Section 5.3 more actionable.</p>"},{"location":"getting-started/python/debugging/#using-these-functions-proactively-can-often-help-you-pinpoint-the-source-of-an-error-even-faster-than-just-reading-the-traceback","title":"Using these functions proactively can often help you pinpoint the source of an error even faster than just reading the traceback.","text":""},{"location":"getting-started/python/modeling-information/","title":"Modeling Information","text":"<p>Part 2: Modeling Information - The Basic Building Blocks</p> <p>To analyze data, we first need a way to represent that data in our code. We start with the simplest forms of information \u2013 basic facts and values \u2013 and learn how Python understands them.</p> <p>2.1 Literals &amp; Core Data Types: Raw Information</p> <p>The most basic way to represent information is by using literals \u2013 fixed values written directly into your code. Think of these as the raw facts you might start with:</p> <ul> <li><code>\"Midtown Office\"</code> (The name of a location - text)</li> <li><code>255</code> (The number of employees - a whole number)</li> <li><code>15750.25</code> (Monthly rent - a number with decimals)</li> <li><code>True</code> (Whether the lease is active - a yes/no status)</li> <li><code>None</code> (Perhaps the previous tenant is unknown - representing missing info)</li> </ul> <p>Each of these literals belongs to a fundamental data type. The data type tells Python what kind of information it is and what operations are allowed. Here are the core types we'll use constantly:</p> <ul> <li>Strings (<code>str</code>): Used for textual data. Enclose text in either single quotes (<code>'...'</code>) or double quotes (<code>\"...\"</code>).     <pre><code>print(\"This is a string.\")\nprint('So is this.')\nprint(\"Employee Name: Alice\")\n</code></pre></li> <li>Integers (<code>int</code>): Used for whole numbers (positive, negative, or zero).     <pre><code>print(42)\nprint(-100)\nprint(0)\n</code></pre></li> <li>Floats (<code>float</code>): Used for numbers that have a decimal component (floating-point numbers).     <pre><code>print(3.14159)\nprint(-0.5)\nprint(2.0) # Note: Even a .0 makes it a float, not an int\n</code></pre></li> <li>Booleans (<code>bool</code>): Represent logical <code>True</code> or <code>False</code> values. These are essential for making decisions and comparisons later (Automation!). Note the capitalization.     <pre><code>print(True)\nprint(False)\n</code></pre></li> <li>The <code>None</code> Type (<code>NoneType</code>): A special type representing the intentional absence of a value. It's often used as a placeholder or to indicate that data is missing or not applicable. There's only one <code>None</code> value.     <pre><code>print(None)\n</code></pre></li> </ul> <p>Checking the Type: Python lets you ask what type a specific value (or variable, as we'll see soon) is using the built-in <code>type()</code> function. This is useful for understanding your data.</p> <pre><code># Try running this cell in Colab!\nprint( type(\"Hello\") )\nprint( type(123) )\nprint( type(98.6) )\nprint( type(False) )\nprint( type(None) )\n</code></pre> <p>Expected Output: <pre><code>&lt;class 'str'&gt;\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n&lt;class 'bool'&gt;\n&lt;class 'NoneType'&gt;\n</code></pre> Knowing these basic types is the first step toward modeling information effectively in Python.</p> <p>2.2 Expressions &amp; Operators: Combining and Comparing Information</p> <p>Rarely do we work with just single literal values. We usually need to combine, compare, or perform calculations. We do this using expressions and operators.</p> <ul> <li>An expression is a piece of code that Python evaluates to produce a value (e.g., <code>5 + 3</code> is an expression that evaluates to <code>8</code>).</li> <li>Operators are the special symbols that perform the actions (like <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>).</li> </ul> <p>Common Operator Types:</p> <ul> <li>Arithmetic Operators: For math (primarily used with <code>int</code> and <code>float</code>):<ul> <li><code>+</code> Addition (<code>10 + 5</code> -&gt; <code>15</code>)</li> <li><code>-</code> Subtraction (<code>10 - 5</code> -&gt; <code>5</code>)</li> <li><code>*</code> Multiplication (<code>10 * 5</code> -&gt; <code>50</code>)</li> <li><code>/</code> True Division (<code>10 / 4</code> -&gt; <code>2.5</code>) - Result is always a float!</li> <li><code>//</code> Floor Division (<code>10 // 4</code> -&gt; <code>2</code>) - Discards the decimal/remainder.</li> <li><code>%</code> Modulo (<code>10 % 4</code> -&gt; <code>2</code>) - Gives the remainder.</li> <li><code>**</code> Exponentiation (<code>10 ** 2</code> -&gt; <code>100</code>) - 10 raised to the power of 2.</li> </ul> </li> <li>Comparison Operators: For comparing two values. These expressions always result in a Boolean (<code>True</code> or <code>False</code>):<ul> <li><code>==</code> Equal to (<code>5 == 5</code> -&gt; <code>True</code>)</li> <li><code>!=</code> Not equal to (<code>5 != 6</code> -&gt; <code>True</code>)</li> <li><code>&lt;</code> Less than (<code>5 &lt; 6</code> -&gt; <code>True</code>)</li> <li><code>&gt;</code> Greater than (<code>5 &gt; 6</code> -&gt; <code>False</code>)</li> <li><code>&lt;=</code> Less than or equal to (<code>5 &lt;= 5</code> -&gt; <code>True</code>)</li> <li><code>&gt;=</code> Greater than or equal to (<code>5 &gt;= 6</code> -&gt; <code>False</code>) (These also work for comparing strings alphabetically)</li> </ul> </li> <li>Logical Operators: For combining <code>True</code>/<code>False</code> values:<ul> <li><code>and</code>: <code>True and True</code> -&gt; <code>True</code> (Both sides must be True)</li> <li><code>or</code>: <code>True or False</code> -&gt; <code>True</code> (At least one side must be True)</li> <li><code>not</code>: <code>not True</code> -&gt; <code>False</code> (Reverses the boolean value)</li> </ul> </li> </ul> <p>Example Combining Operators:</p> <p><pre><code># Example scenario\nrevenue = 50000\ncost = 35000\ntarget_profit = 10000\nis_new_quarter = True\n\n# Calculate profit\nprofit = revenue - cost\n\n# Check conditions\nis_profitable = profit &gt; 0\nmeets_target = profit &gt;= target_profit\nshould_review = is_profitable and (not meets_target) and is_new_quarter # Example logic\n\nprint(f\"Profit: {profit}\")\nprint(f\"Is profitable? {is_profitable}\")\nprint(f\"Meets profit target? {meets_target}\")\nprint(f\"Needs quarterly review? {should_review}\")\nprint(f\"Type of 'meets_target': {type(meets_target)}\") # Output shows &lt;class 'bool'&gt;\n</code></pre> Expressions let you derive new information and check conditions based on your basic modeled data.</p> <p>2.3 Variables: Naming Information for Reuse</p> <p>Hardcoding literal values everywhere makes code hard to read and maintain. If a value needs to change (like a tax rate), you'd have to find and replace it everywhere! More importantly for our \"Modeling\" approach, we want to give meaningful names to the different pieces of our information model. We achieve this using variables.</p> <ul> <li>Assignment (<code>=</code>): You create a variable and assign it a value using the single equals sign (<code>=</code>).     <code>variable_name = value</code></li> <li>Analogy: Think of a variable as a label pointing to a value stored in the computer's memory. You use the label (the variable name) to refer to the value.</li> </ul> <p>Examples:</p> <pre><code># Modeling details for an employee\nemployee_id = \"EMP102\"         # Assigning a string\nemployee_name = \"Bob Johnson\"    # Assigning another string\nyears_with_company = 3          # Assigning an int\nhourly_rate = 22.50           # Assigning a float\nis_full_time = True           # Assigning a bool\n\n# Using the variables\nprint(employee_name)\nprint(hourly_rate)\n\n# Variables can be used in expressions\nweekly_pay_estimate = hourly_rate * 40\nprint(f\"Estimated weekly pay for {employee_name}: ${weekly_pay_estimate}\")\n\n# Variable values can be updated\nyears_with_company = years_with_company + 1 # Anniversary!\nprint(f\"{employee_name} now has {years_with_company} years with the company.\")\n</code></pre> <p>Variable Naming Conventions (Python Best Practice):</p> <ul> <li>Use <code>snake_case</code>: All lowercase letters, with words separated by underscores (e.g., <code>hourly_rate</code>, <code>customer_first_name</code>). This is the standard convention in the Python community.</li> <li>Be Descriptive: Choose names that clearly indicate the variable's purpose (<code>average_score</code> is better than <code>avg</code> or <code>s</code>).</li> <li>Rules: Names must start with a letter or underscore (<code>_</code>), followed by letters, numbers, or underscores. They cannot be Python keywords (like <code>if</code>, <code>for</code>, <code>True</code>, <code>None</code>, <code>def</code>, <code>class</code>). Names are case-sensitive (<code>age</code> is different from <code>Age</code>).</li> </ul> <p>Comments (<code>#</code>): Explaining Your Code As you assign variables and write logic, add comments using the hash symbol (<code>#</code>). Python ignores everything on a line after the <code>#</code>. Comments explain the why behind your code, making it understandable later.</p> <p><pre><code># Standard hourly rate for entry-level analysts\nbase_rate = 20.00\n\n# Bonus multiplier for employees with &gt; 2 years experience\nexperience_bonus_multiplier = 1.1\n\n# Calculate Bob's actual rate (assuming Bob has &gt; 2 years)\nactual_rate = base_rate * experience_bonus_multiplier # Apply experience bonus\n</code></pre> Use comments frequently!</p> <p>2.4 Working with Text (<code>str</code>): A Closer Look</p> <p>Text data (<code>str</code>) is fundamental in business (names, addresses, descriptions, logs, etc.) and often needs cleaning or formatting. Python's strings have powerful capabilities.</p> <ul> <li>Combining Strings (Concatenation): Use the <code>+</code> operator. Remember to add spaces if needed!     <pre><code>greeting = \"Hello\"\nname = \"Maria\"\nmessage = greeting + \", \" + name + \"!\"\nprint(message) # Output: Hello, Maria!\n</code></pre></li> <li>Common String Methods: Strings are objects with built-in functions called methods. You call them using <code>dot notation</code>: <code>your_string_variable.method_name()</code>. Here are some essentials for data cleaning:<ul> <li><code>.lower()</code>: Returns a new string with all characters converted to lowercase.</li> <li><code>.upper()</code>: Returns a new string with all characters converted to uppercase.</li> <li><code>.strip()</code>: Returns a new string with leading and trailing whitespace (spaces, tabs, newlines) removed. Invaluable!</li> <li><code>.replace(old, new)</code>: Returns a new string where all occurrences of the <code>old</code> substring are replaced with the <code>new</code> substring.</li> <li>(Note: Strings are immutable - these methods return new strings; they don't change the original one unless you reassign the variable.)</li> </ul> </li> </ul> <p>Example Usage:</p> <pre><code>raw_city_data = \"  New York City \\n\"\nprint(f\"Original: '{raw_city_data}'\")\n\n# Clean up for consistency\ncleaned_city = raw_city_data.strip().lower()\nprint(f\"Cleaned: '{cleaned_city}'\") # Output: Cleaned: 'new york city'\n\nstatus = \"Status: PENDING\"\nupdated_status = status.replace(\"PENDING\", \"COMPLETE\")\nprint(f\"Updated Status: '{updated_status}'\") # Output: Updated Status: 'Status: COMPLETE'\n</code></pre> <p>F-Strings (Formatted String Literals): The Best Way to Combine Strings and Variables We've used these in examples already. F-strings provide the clearest and most convenient way to create strings that include the values of variables.</p> <ul> <li>Start the string with the letter <code>f</code> immediately before the opening quote (<code>f\"...\"</code> or <code>f'...'</code>).</li> <li>Inside the string, place variable names or even simple expressions directly within curly braces <code>{}</code>.</li> </ul> <p><pre><code>product_id = \"XYZ789\"\nprice = 19.95\ndiscount_pct = 10 # As percentage points\n\n# Using f-string for clear output\noutput_message = f\"Product {product_id} costs ${price:.2f}. Current discount: {discount_pct}%\"\nprint(output_message)\n# Output: Product XYZ789 costs $19.95. Current discount: 10%\n\n# You can include calculations inside the braces\ndiscounted_price = price * (1 - discount_pct / 100)\nprint(f\"Discounted price for {product_id}: ${discounted_price:.2f}\")\n# Output: Discounted price for XYZ789: $17.95\n</code></pre> (Note: The <code>:.2f</code> inside the braces is special formatting to show the float with exactly two decimal places - useful for currency!)</p> <p>Get comfortable with basic string operations and f-strings; you'll use them constantly when preparing and presenting data.</p>"},{"location":"getting-started/python/modeling-structures/","title":"Modeling Structures","text":"<p>Part 3: Modeling Structures - Organizing Related Information</p> <p>We've seen how to represent basic pieces of information (<code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, <code>None</code>) and name them using variables. But data rarely exists in isolation. Information is usually related, comes in collections, or requires specialized tools (like handling dates). This section introduces Python's fundamental data structures for organizing related information and how we access external code tools using <code>import</code>.</p> <p>3.1 Importing Modules: Accessing More Tools</p> <p>Python itself provides the basic building blocks, but much of its power, especially for data analysis, comes from modules and packages (often called libraries). These are collections of pre-written code created by other developers that provide specialized functions and data types.</p> <p>Think of Python's built-in features as your standard toolbox. Modules are like specialized toolsets you can bring into your workshop when you need them \u2013 perhaps a set for advanced math, another for plotting, another for web access, or, importantly for us, tools for handling dates and times or performing large-scale data manipulation.</p> <p>To use the tools (functions, classes, etc.) from a module or library, you must first <code>import</code> it into your current Colab session.</p> <p>How to Import:</p> <ol> <li> <p><code>import library_name</code>: This brings the entire library into your session's memory. To use something from the library, you typically prefix it with the library's name followed by a dot (<code>.</code>).</p> <pre><code>import math # Import Python's built-in math module\n\n# Use the sqrt function *from* the math module\nnumber = 25\nsquare_root = math.sqrt(number)\nprint(f\"The square root of {number} is {square_root}\")\n</code></pre> </li> <li> <p><code>import library_name as alias</code>: This is extremely common, especially for libraries with long names or standard abbreviations. It imports the library but assigns it a shorter, convenient nickname (alias) that you use instead of the full name. This makes your code less verbose and often follows community conventions.</p> <pre><code># Import Python's built-in datetime module, using the standard alias 'dt'\nimport datetime as dt\n\n# Use the 'date' object type and the 'today()' method from the datetime module, via the alias\ntodays_date = dt.date.today()\nprint(f\"Today's date is: {todays_date}\")\n\n# Access attributes of the date object\nprint(f\"The current year is: {todays_date.year}\")\nprint(f\"The current month is: {todays_date.month}\")\nprint(f\"The current day is: {todays_date.day}\")\n</code></pre> </li> </ol> <p>Where Imports Go: By convention, all <code>import</code> statements are usually placed at the very top of a notebook or script. This clearly declares all the external tools your code relies on. We'll use <code>import library as alias</code> frequently for <code>polars</code>, <code>altair</code>, <code>sklearn</code>, and <code>datetime</code>.</p> <p>3.2 Lists (<code>list</code>): Ordered, Changeable Sequences</p> <p>Modeling Motivation: How do you represent a collection of related items where the order is important?</p> <ul> <li>A sequence of steps in a process.</li> <li>Monthly sales figures recorded in chronological order.</li> <li>A list of customer names, perhaps in the order they were acquired. Python's solution is the list.</li> </ul> <p>A list is an ordered sequence of items. The items can be of any data type (though often they're homogeneous, meaning all the same type), and the order in which you add them is preserved.</p> <p>Syntax: Create lists using square brackets <code>[]</code>, with items separated by commas.</p> <pre><code># A list of region names (strings)\nregions = [\"North\", \"South\", \"East\", \"West\"]\n\n# A list of quarterly sales figures (floats)\nquarterly_sales = [15000.50, 18200.00, 14100.75, 19500.20]\n\n# A list can technically mix types (but often less useful for data analysis)\nmixed_info = [\"Product A\", 100, 49.99, True]\n\n# An empty list, ready to be filled later\ntasks = []\n\nprint(regions)\nprint(quarterly_sales)\n</code></pre> <p>Accessing Items by Index: Because lists are ordered, you retrieve items using their numerical position, called an index. Crucially, Python indexing starts from 0!</p> <p><pre><code>first_region = regions[0]         # Index 0 is the FIRST item\nsales_q2 = quarterly_sales[1]     # Index 1 is the SECOND item\nlast_region = regions[-1]         # Index -1 is the LAST item\nsecond_last = regions[-2]     # Index -2 is the second-to-last\n\nprint(f\"First region: {first_region}\")\nprint(f\"Q2 Sales: {sales_q2}\")\nprint(f\"Last region: {last_region}\")\n</code></pre> (Trying to access an index outside the list's range will cause an <code>IndexError</code>.)</p> <p>Slicing (Getting Sub-Lists): Extract a portion using <code>[start:stop]</code>. The <code>start</code> index is included, but the <code>stop</code> index is excluded.</p> <pre><code># Get the 'South' and 'East' regions (index 1 up to, but not including, index 3)\nmiddle_regions = regions[1:3]\nprint(f\"Middle regions: {middle_regions}\") # Output: ['South', 'East']\n\n# Get from the beginning up to index 2 (exclusive)\nfirst_two = regions[:2]\nprint(f\"First two: {first_two}\") # Output: ['North', 'South']\n\n# Get from index 2 to the end\nlast_two = regions[2:]\nprint(f\"Last two: {last_two}\") # Output: ['East', 'West']\n</code></pre> <p>Mutability (Changeable): Lists are mutable, meaning you can change their contents after creation.</p> <ul> <li>Change an item: Use index assignment.     <pre><code>print(f\"Original Q1 Sales: {quarterly_sales[0]}\")\nquarterly_sales[0] = 15500.00 # Corrected Q1 sales\nprint(f\"Updated Q1 Sales: {quarterly_sales[0]}\")\n</code></pre></li> <li>Add an item to the end: Use the <code>.append()</code> method.     <pre><code>regions.append(\"Central\")\nprint(f\"Regions after adding Central: {regions}\")\n</code></pre></li> </ul> <p>Getting the Length: Use the built-in <code>len()</code> function.</p> <p><pre><code>num_regions = len(regions)\nprint(f\"Number of regions: {num_regions}\")\n</code></pre> Lists are fundamental for holding ordered collections of data that might need modification.</p> <p>3.3 Tuples (<code>tuple</code>): Ordered, Unchangeable Records</p> <p>Modeling Motivation: What if you want to group related pieces of information about a single thing, where the order matters, but the structure itself shouldn't change? * An (X, Y) coordinate point. * An RGB color value <code>(red, green, blue)</code>. * A basic record for a person: <code>(name, age, registration_date)</code>. Python provides tuples for this.</p> <p>A tuple is also an ordered sequence, much like a list.</p> <p>Syntax: Usually created using parentheses <code>()</code>, with items separated by commas. (Parentheses are often optional but recommended for clarity).</p> <pre><code># Tuple representing (x, y) coordinates\npoint = (150, 75)\n\n# Tuple representing a database record (ID, status, timestamp)\n# We need datetime for this example! Make sure 'import datetime as dt' ran earlier.\nimport datetime as dt # Place imports at the top usually, but here for example context\nrecord_status = (101, \"Processed\", dt.datetime.now())\n\n# A tuple requires a comma even for one item!\nsingle_value_tuple = (\"UniqueValue\",)\n\n# Empty tuple\nempty_tuple = ()\n\nprint(point)\nprint(record_status)\nprint(single_value_tuple)\n</code></pre> <p>Accessing Items by Index: Same zero-based indexing as lists.</p> <pre><code>x_coordinate = point[0]\nstatus = record_status[1]\nprint(f\"X Coordinate: {x_coordinate}\")\nprint(f\"Record Status: {status}\")\n</code></pre> <p>Immutability (Unchangeable): This is the defining characteristic and key difference from lists. Tuples are immutable. Once created, you cannot change, add, or remove elements.</p> <pre><code># These lines will cause an ERROR if you try to run them!\n# point[0] = 160  # Cannot change item assignment\n# record_status.append(True) # Tuples have no .append() method\n</code></pre> <p>Why Use Tuples?</p> <ul> <li>Data Integrity: Signals that this group of items represents a fixed record or structure that shouldn't be altered.</li> <li>Performance: Can be slightly more memory-efficient and sometimes faster to process than lists (though usually not a major factor for basic usage).</li> <li>Dictionary Keys: Tuples can be used as keys in dictionaries (see next section) because they are immutable; lists cannot.</li> </ul> <p>Common Use Case: List of Tuples A very frequent pattern for representing simple tables of data is a list of tuples, where each tuple represents one row or record.</p> <p><pre><code># List of tuples: (Product ID, Price, Quantity)\ninventory = [\n    (\"P1001\", 49.99, 50),\n    (\"P1002\", 19.50, 120),\n    (\"P1003\", 175.00, 15)\n]\n\n# Access the second product record (tuple)\nproduct2_record = inventory[1]\nprint(f\"Product 2 Record: {product2_record}\")\n\n# Access the price of the second product\nproduct2_price = inventory[1][1] # Index 1 for the tuple, Index 1 for the price within the tuple\nprint(f\"Product 2 Price: {product2_price}\")\n\n# Iterate through the inventory (we'll cover loops soon!)\n# for item in inventory:\n#   print(f\"Processing Product ID: {item[0]}\")\n</code></pre> Use lists when the order matters and the contents might change; use tuples when the order matters but the contents represent a fixed record.</p> <p>3.4 Dictionaries (<code>dict</code>): Labeled Information (Key-Value Pairs)</p> <p>Modeling Motivation: Accessing items by numerical position (index) like in lists and tuples isn't always intuitive. How would you model a configuration setting where you want to look up the 'username' or 'timeout_seconds'? Or represent a product where you want to directly access its 'price' or 'brand' using those names? Python's dictionaries are perfect for this.</p> <p>A dictionary stores a collection of key-value pairs. Each unique key is associated with a specific value. Think of it like a real-world dictionary where the word (key) points to its definition (value).</p> <p>Syntax: Dictionaries are created using curly braces <code>{}</code>, containing <code>key: value</code> pairs separated by commas. * Keys must be unique and immutable (strings and numbers are common keys; tuples can be keys, but lists cannot). * Values can be any data type.</p> <pre><code># Dictionary modeling configuration settings\nconfig = {\n    \"server_ip\": \"192.168.1.100\",\n    \"port\": 8080,\n    \"username\": \"admin\",\n    \"timeout_seconds\": 60,\n    \"feature_flags\": [\"feature_a\", \"feature_c\"] # Value can be a list!\n}\n\n# Dictionary modeling a student record\nstudent = {\n    \"student_id\": \"S5001\",\n    \"name\": \"Charlie Day\",\n    \"major\": \"Business Analytics\",\n    \"gpa\": 3.75,\n    \"is_active\": True\n}\n\n# Empty dictionary\nempty_dictionary = {}\n\nprint(config)\nprint(student)\n</code></pre> <p>Accessing Values by Key: You don't use numerical indices. Instead, you use the key inside square brackets <code>[]</code> to get its associated value.</p> <p><pre><code>server = config[\"server_ip\"]\nstudent_name = student[\"name\"]\n\nprint(f\"Connect to server: {server}\")\nprint(f\"Student Name: {student_name}\")\n</code></pre> (Attempting to access a key that doesn't exist will raise a <code>KeyError</code>.)</p> <p>Mutability (Changeable): Dictionaries are mutable. You can add new key-value pairs or change the value associated with an existing key after creation.</p> <pre><code># Add a new setting to the config\nconfig[\"log_level\"] = \"INFO\"\nprint(f\"Config after adding log_level: {config}\")\n\n# Update the student's GPA\nstudent[\"gpa\"] = 3.80\nprint(f\"Student record after GPA update: {student}\")\n</code></pre> <p>Key vs. Index: This is the fundamental difference in how you retrieve data: * Lists/Tuples: Ordered, access by numerical position (index) -&gt; <code>my_list[0]</code> * Dictionaries: Conceptually unordered (though order-preserving in modern Python), access by unique label (key) -&gt; <code>my_dict['label']</code></p> <p>Dictionaries are incredibly flexible for modeling objects or records where accessing information by a specific name or identifier is important.</p>"},{"location":"getting-started/python/next-steps/","title":"Next Steps","text":"<p>Part 7: Summary &amp; Next Steps</p> <p>7.1 Recap: Your Python Foundation</p> <p>Congratulations! You've navigated the essentials of Python programming, building a foundation specifically geared towards data analysis. We approached this by focusing on how Python helps us with two key tasks: Modeling Information and Automating Tasks.</p> <p>Let's quickly summarize what we've covered:</p> <ul> <li>Modeling Fundamentals: We started with Python's basic data types (<code>int</code>, <code>float</code>, <code>str</code>, <code>bool</code>, <code>None</code>) and learned how to represent raw information (literals) and give it meaningful names using variables (<code>=</code>). We covered expressions and operators for calculations and comparisons.</li> <li>Modeling Structures: We explored Python's core data structures for organizing related information:<ul> <li><code>list</code>: Ordered, changeable sequences (like columns or lists of items).</li> <li><code>tuple</code>: Ordered, unchangeable records (like rows or fixed groups).</li> <li><code>dict</code>: Unordered (conceptually), labeled key-value pairs (like records with named fields).</li> </ul> </li> <li>Automation Tools: We learned how to control the flow of our programs:<ul> <li><code>if</code>/<code>elif</code>/<code>else</code>: For making decisions based on conditions.</li> <li><code>for</code>/<code>while</code> loops: For repeating actions over sequences or based on conditions.</li> </ul> </li> <li>Structuring Code: We saw how functions (<code>def</code>, <code>return</code>) allow us to create reusable blocks of code, making our work modular and easier to manage. We also got a crucial introduction to the practical side of Object-Oriented Programming \u2013 understanding that objects (like strings, lists, dates, and the library objects we'll soon use) have data (attributes, accessed via <code>.</code>) and actions (methods, called via <code>.()</code>).</li> <li>Essential Practices: We touched upon vital skills like importing modules (<code>import</code>), writing comments (<code>#</code>), formatting strings (<code>f-strings</code>), basic string methods (<code>.strip()</code>, <code>.lower()</code>, etc.), and understanding the Colab environment's execution model.</li> </ul> <p>You now possess the fundamental vocabulary and grammatical rules of Python needed to start commanding the computer to perform data-related work.</p> <p>7.2 Looking Ahead: Applying Python to Data</p> <p>This introduction to Python is the essential groundwork, not just an abstract exercise. Every concept we've discussed \u2013 from variables and data types to loops, functions, and object interaction \u2013 directly enables the real-world data analysis we'll be doing next.</p> <p>In Module 2: Exploratory Data Analysis (EDA), we will immediately start applying these Python foundations as we learn to use powerful, industry-standard libraries:</p> <ul> <li>Polars: We'll see how Polars uses concepts similar to Python lists and dictionaries to create highly efficient Series (columns) and DataFrames (tables). You'll use your understanding of types, structures, and iteration as we learn to load massive datasets, clean messy real-world information, manipulate tables, group data, and calculate summary statistics \u2013 all powered by Python and Polars.</li> <li>Altair: We'll leverage Python lists, dictionaries, and the object-oriented pattern (<code>chart.method()</code>, <code>chart.attribute</code>) to define and create compelling, interactive data visualizations that help uncover insights and communicate findings effectively.</li> </ul> <p>The journey ahead involves using these foundational Python skills as leverage to operate much more sophisticated tools. Get ready to transition from learning the language basics to applying them to explore, understand, and communicate insights from data!</p>"},{"location":"getting-started/python/polars-teaser/","title":"Polars Teaser","text":"<p>Part 4: Teaser - Connecting to Data Analysis Tools</p> <p>4.1 The Bridge to Polars (and other Data Libraries)</p> <p>We've just spent time learning how to model information using Python's fundamental building blocks and data structures: numbers, strings, booleans, lists, tuples, and dictionaries. You might be thinking, \"Okay, that's interesting, but how does creating a list of names or a dictionary for one product help me analyze a dataset with thousands or millions of rows?\"</p> <p>That's a great question! The answer is that these fundamental Python concepts are the direct foundation upon which powerful data analysis libraries, like Polars (which we'll use extensively), are built and interacted with.</p> <p>From Python Lists/Tuples to Polars Series:</p> <p>Remember our Python lists holding sequences of items, like monthly sales or customer names?</p> <pre><code># A Python list of quarterly sales\nquarterly_sales = [15000.50, 18200.00, 14100.75, 19500.20]\n</code></pre> <p>In data analysis libraries like Polars (or its well-known predecessor, Pandas), a single column of data in a table is typically represented by a specialized object called a Series. Think of a Polars Series as a highly optimized, super-powered version of a Python list or tuple, specifically designed for:</p> <ul> <li>Holding data of a single type (like all numbers or all strings).</li> <li>Performing fast calculations across all items.</li> <li>Handling missing values efficiently.</li> <li>Aligning data based on labels (indices).</li> </ul> <p>Conceptual Link: Python List/Tuple \u2248 Polars Series (One Data Column)</p> <p>From Python Structures to Polars DataFrames:</p> <p>Now, recall how we structured data for multiple records or entities:</p> <ul> <li>List of Tuples: <code>[ (\"E101\", \"Alice\", \"Sales\"), (\"E102\", \"Bob\", \"Marketing\") ]</code></li> <li>List of Dictionaries: <code>[ {\"ID\": \"E101\", \"Name\": \"Alice\"}, {\"ID\": \"E102\", \"Name\": \"Bob\"} ]</code></li> <li>Dictionary of Lists: <code>{\"ID\": [\"E101\", \"E102\"], \"Name\": [\"Alice\", \"Bob\"]}</code></li> </ul> <p>These common Python patterns for organizing structured data directly mirror the concept of the central object in tabular data analysis: the DataFrame.</p> <p>A Polars DataFrame represents a table \u2013 think of a spreadsheet with rows and columns. Each column in the DataFrame is actually a Polars Series. The entire DataFrame allows you to efficiently store, manipulate, filter, aggregate, and analyze structured, two-dimensional data.</p> <p>Crucially, you can often create a Polars DataFrame directly from these Python structures (like a list of dictionaries or a dictionary of lists)!</p> <p>Conceptual Link: List of Dicts / Dict of Lists / List of Tuples \u2248 Polars DataFrame (Table)</p> <p>Why This Matters:</p> <p>Learning Python's core data types and structures isn't just an academic exercise. It's essential because:</p> <ol> <li>Input/Output: These structures are often the way you'll initially load data into DataFrames or get results out of them.</li> <li>Conceptual Foundation: The way you think about accessing elements (by index or key) and iterating over collections in Python provides the mental model for similar operations in Polars and other libraries (though the syntax will be different and more powerful).</li> <li>Custom Logic: While libraries provide optimized functions, you'll often use basic Python logic (loops, conditionals, functions) to perform custom transformations or analysis steps on your data within the larger framework.</li> </ol> <p>The Python fundamentals you're learning now are the necessary launching pad for effectively wielding the powerful data manipulation and analysis tools we'll explore next. You're building the foundation to work with data at scale.</p>"},{"location":"getting-started/python/task-automation/","title":"Task Automation","text":"<p>Part 5: Automating Tasks - Making Code Work for You</p> <p>We've explored how to use Python's types and data structures to model information. Now, let's shift focus to the second anchor concept: Automation. How do we instruct the computer to perform tasks repeatedly or make decisions based on the data we've modeled? This is where Python's control flow statements come in.</p> <p>5.1 Conditional Logic (<code>if</code>, <code>elif</code>, <code>else</code>)</p> <p>Automation Motivation: Often, the steps in an analysis or process depend on certain conditions. * If a customer spends over $100, apply free shipping. * If a survey response is \"Strongly Agree\", score it 5 points; if \"Agree\", score it 4 points. * If a data value is missing, handle it differently than if it's present. Conditional statements allow your program to make these kinds of decisions automatically based on whether certain conditions are <code>True</code> or <code>False</code>.</p> <p>Python uses the <code>if</code>, <code>elif</code> (short for \"else if\"), and <code>else</code> keywords for this.</p> <p>The Core Idea: These statements work by evaluating boolean expressions (expressions that result in <code>True</code> or <code>False</code>, often using the comparison and logical operators we saw in Section 2.2).</p> <ul> <li>The code block under <code>if</code> runs only if its condition is <code>True</code>.</li> <li>If the <code>if</code> condition is <code>False</code>, Python checks the condition of the first <code>elif</code> (if present). If that's <code>True</code>, its block runs.</li> <li>Python checks subsequent <code>elif</code> conditions only if all preceding <code>if</code>/<code>elif</code> conditions were <code>False</code>.</li> <li>The <code>else</code> block (if present) runs only if all the preceding <code>if</code> and <code>elif</code> conditions were <code>False</code>.</li> </ul> <p>Syntax (Indentation is Crucial!): Notice the colons (<code>:</code>) after each condition and the consistent indentation (typically 4 spaces) for the code blocks belonging to each statement. Indentation is how Python groups code blocks.</p> <pre><code>if some_boolean_condition:\n    # This block runs ONLY if some_boolean_condition is True\n    print(\"The if condition was met.\")\nelif another_boolean_condition:\n    # This block runs ONLY if the 'if' was False AND 'another_boolean_condition' is True\n    print(\"The elif condition was met.\")\nelse:\n    # This block runs ONLY if BOTH the 'if' AND 'elif' conditions were False\n    print(\"Neither the if nor the elif condition was met.\")\n</code></pre> <p>Examples:</p> <p><pre><code># Example 1: Simple If\nproject_status = \"Complete\"\nif project_status == \"Complete\":\n    print(\"Archiving project files...\")\n    # If status wasn't \"Complete\", this block is just skipped.\n\n# Example 2: If-Else\ninventory_count = 35\nif inventory_count &gt; 0:\n    print(f\"Stock available: {inventory_count} units.\")\nelse:\n    print(\"Item is out of stock.\")\n\n# Example 3: If-Elif-Else\nsatisfaction_score = 4 # Score from 1 to 5\n\nif satisfaction_score == 5:\n    feedback_category = \"Very Satisfied\"\nelif satisfaction_score == 4:\n    feedback_category = \"Satisfied\"\nelif satisfaction_score == 3:\n    feedback_category = \"Neutral\"\nelse: # Covers scores 1 and 2\n    feedback_category = \"Needs Attention\"\n\nprint(f\"Feedback Category: {feedback_category}\")\n</code></pre> Conditional logic allows you to create flexible programs that respond dynamically to different data inputs and situations, automating decision-making.</p> <p>5.2 Iteration: Repeating Actions</p> <p>Automation Motivation: Many data tasks involve doing the same thing to multiple pieces of data.</p> <ul> <li>Applying a calculation to every sales figure in a list.</li> <li>Checking the format of every email address in a customer database.</li> <li>Processing each row in a data table. Doing this manually is impractical. Iteration allows you to automate these repetitive processes using loops.</li> </ul> <p><code>for</code> Loops: Iterating Over Sequences (The Workhorse) The <code>for</code> loop is the most common and idiomatic way to iterate in Python, especially when working with data structures like lists, tuples, strings, or dictionaries. It lets you process each item in a collection one by one.</p> <p>Syntax:</p> <p><pre><code>for temporary_variable in sequence_or_iterable:\n    # This indented code block runs ONCE for EACH item\n    # in the 'sequence_or_iterable'.\n    # Inside the loop, 'temporary_variable' holds the current item being processed.\n    print(f\"Current item: {temporary_variable}\")\n\nprint(\"Loop finished.\")\n</code></pre> (Remember the colon <code>:</code> and the indentation!)</p> <p>Examples:</p> <pre><code># Iterating over a List\ncustomer_names = [\"Alice\", \"Bob\", \"Charlie\"]\nprint(\"--- Sending Welcome Emails ---\")\nfor name in customer_names:\n    print(f\"Sending email to: {name}\")\n\n# Iterating over a String\nticker = \"GOOGL\"\nprint(\"\\n--- Ticker Characters ---\")\nfor char in ticker:\n    print(char)\n\n# Using range() to loop a specific number of times\n# range(n) generates numbers from 0 up to (but not including) n\nprint(\"\\n--- Processing 3 Batches ---\")\nfor batch_number in range(3): # Generates 0, 1, 2\n    print(f\"Starting batch number {batch_number + 1}\")\n    # Code to process the batch would go here\n\n# range(start, stop) generates numbers from start up to (not including) stop\nprint(\"\\n--- Calculating Interest for Years ---\")\ninitial_balance = 1000\nrate = 0.05\nfor year in range(1, 4): # Generates 1, 2, 3\n    interest = initial_balance * rate\n    print(f\"Year {year} Interest: ${interest:.2f}\")\n    # In a real scenario, you'd update the balance too\n\n# Iterating over Dictionary Items\nproduct = {\"id\": \"XYZ-123\", \"name\": \"Mouse\", \"price\": 29.99}\n\nprint(\"\\n--- Product Keys ---\")\nfor key in product: # Looping directly over a dict iterates through its keys\n    print(key)\n\nprint(\"\\n--- Product Values ---\")\nfor value in product.values(): # Use .values() to get only values\n    print(value)\n\nprint(\"\\n--- Product Key-Value Pairs ---\")\nfor key, value in product.items(): # Use .items() to get (key, value) tuples\n    print(f\"{key}: {value}\")\n</code></pre> <p><code>while</code> Loops: Repeating While a Condition is True A <code>while</code> loop repeatedly executes its code block as long as a specified boolean condition remains <code>True</code>. They are less common than <code>for</code> loops for simply iterating over data collections but are useful when you don't know in advance how many times you need to loop.</p> <p>Syntax:</p> <pre><code>while some_boolean_condition:\n    # Code block runs repeatedly as long as the condition is True\n    print(\"Condition is still True, looping...\")\n    # --- VERY IMPORTANT ---\n    # Something inside this block MUST eventually make\n    # 'some_boolean_condition' False, otherwise you have an infinite loop!\n    # --- VERY IMPORTANT ---\n\nprint(\"Condition became False, while loop ended.\")\n</code></pre> <p>Example (Use with Care):</p> <p><pre><code># Example: Simulate processing items until a queue is empty (represented by a list)\nitems_to_process = [\"Task A\", \"Task B\", \"Task C\"]\nprocessed_count = 0\n\nwhile len(items_to_process) &gt; 0:\n    current_item = items_to_process.pop(0) # .pop(0) removes and returns the first item\n    print(f\"Processing: {current_item}\")\n    processed_count += 1 # Same as processed_count = processed_count + 1\n\nprint(f\"\\nFinished processing. Total items: {processed_count}\")\n</code></pre> Warning: Be cautious with <code>while</code> loops. If the condition never becomes <code>False</code>, the loop will run forever (an \"infinite loop\"), and you'll likely need to interrupt the execution (See Section 3.4). For iterating a known number of times or over existing sequences, <code>for</code> loops are generally preferred and safer.</p> <p>Loops are the engine of automation in programming, enabling you to handle large amounts of data and complex repetitive procedures with concise code.</p>"},{"location":"getting-started/python/why-python/","title":"Introduction to Python","text":"<p>Part 1: Why Learn Python for Data?</p> <p>1.1 The Goal: From Data to Decisions</p> <p>In today's business world, data isn't just numbers; it's the raw material for insight and strategy. From tracking customer behavior and market trends to optimizing operations and forecasting finances, organizations constantly generate and consume vast amounts of information. The critical challenge lies in transforming this raw data into clear insights that lead to smarter, more effective business decisions.</p> <p>While familiar tools like spreadsheets are invaluable, they often reach their limits when faced with modern data challenges:</p> <ul> <li>Scale: Analyzing datasets that grow beyond thousands into millions or even billions of records can become slow or impossible in traditional tools.</li> <li>Complexity: Performing sophisticated statistical analyses, applying machine learning models, or running complex simulations often requires more power and flexibility.</li> <li>Repetition: Manually repeating the same data cleaning, analysis, or reporting process every week or month is time-consuming and prone to errors.</li> <li>Integration: Combining and reconciling data from various sources (databases, APIs, different file types) can be a cumbersome manual task.</li> </ul> <p>This is where learning to program, specifically using Python, becomes a powerful asset. Programming allows you to instruct the computer to perform these tasks systematically. It equips you to:</p> <ul> <li>Handle large and complex datasets efficiently.</li> <li>Implement sophisticated analytical techniques reproducibly.</li> <li>Automate repetitive workflows, freeing up your time for higher-level thinking.</li> <li>Integrate diverse data sources seamlessly.</li> </ul> <p>Ultimately, the goal of learning data programming in this course isn't just about writing code \u2013 it's about significantly enhancing your analytical toolkit, enabling you to tackle more complex problems, derive deeper insights, and make more robust, data-informed decisions in your professional life.</p> <p>1.2 Our Approach: Modeling &amp; Automation</p> <p>Learning programming often feels like memorizing a dictionary of new words and grammar rules (syntax). While syntax is necessary, it's more helpful to understand why these rules exist and what they help us achieve. In this course, we'll frame our Python learning around two practical, core concepts directly relevant to data analysis:</p> <ol> <li>Modeling Information: How do we represent real-world things \u2013 customers, products, transactions, survey responses \u2013 in a way a computer can work with? We'll learn how Python's fundamental components (like data types and structures) act as building blocks to create flexible and accurate models of the information we care about.</li> <li>Automating Tasks: How do we get the computer to do the heavy lifting? Whether it's processing thousands of data entries, applying the same calculation repeatedly, or making decisions based on specific criteria, we need to automate these processes. We'll learn how Python's control structures (like loops and conditionals) allow us to define these automated workflows precisely.</li> </ol> <p>Thinking in terms of Modeling and Automation provides purpose to the Python features we learn. Variables aren't just names; they label parts of our model. Lists aren't just lists; they structure collections within our model. Loops aren't just syntax; they are engines for automation. This practical mindset will be our guide as we explore Python's capabilities.</p> <p>1.3 Python: Our Tool</p> <p>Why Python? Among the many programming languages available, Python has become a dominant force in the world of data science, analytics, and machine learning for several key reasons:</p> <ul> <li>Vast Data Science Ecosystem: Python has an unparalleled collection of specialized libraries (pre-built code packages) tailored for data tasks. We'll use powerful libraries like Polars (for high-speed data manipulation), Altair (for creating insightful visualizations), and Scikit-learn (the standard for machine learning). Learning Python unlocks access to these state-of-the-art tools.</li> <li>Readability and Ease of Learning: Python's syntax is often described as clean and readable, sometimes resembling plain English. This generally makes it easier for beginners to pick up compared to other languages, allowing you to focus more on concepts and less on complex syntax.</li> <li>Large &amp; Active Community: Its popularity means there's a huge global community of Python users. This translates into abundant learning resources (tutorials, forums, books), readily available help, and a high demand for Python skills in industry.</li> </ul> <p>Learning Python provides a versatile, powerful, and highly relevant foundation for anyone working with data today.</p> <p>1.4 Colab: Our Workshop</p> <p>As introduced in the Getting Started with Google Colab guide, we will be using Google Colab as our primary environment for writing and running Python code in this course.</p> <p>Remember, Colab:</p> <ul> <li>Runs entirely in your web browser.</li> <li>Requires no complex setup or installation on your part.</li> <li>Provides a Python environment with many necessary libraries ready to go.</li> <li>Integrates seamlessly with your Google Drive for saving notebooks and accessing data.</li> </ul> <p>If you need a quick refresher on the Colab interface (creating notebooks, cells, running code, etc.), please revisit the Colab interface lesson. </p> <p>Now, let's begin building our understanding of Python, starting with the fundamental ways we can model basic pieces of information.</p>"},{"location":"modeling/","title":"Introduction to Modeling","text":"<ul> <li> <p> Introduction to Modeling Concepts and Workflow</p> <p> Introduction to Modeling</p> </li> <li> <p> Robust Modeling Workflow</p> <p> Introduction to Production-grade Workflow </p> </li> <li> <p> Modeling Without a Target</p> <p> Introduction to Unsupervised Learning</p> </li> </ul>"},{"location":"modeling/modeling-concepts-workflow/","title":"Introduction to Modeling with scikit-learn","text":"<ul> <li> Starter Colab Notebook</li> <li> Introduction to Modeling </li> <li> Your First Model </li> <li> Workflow </li> <li> Workflow in Action </li> <li> Estimator API </li> <li> Guide to <code>scikit-learn</code> Datasets </li> <li> Next Steps </li> </ul>"},{"location":"modeling/modeling-concepts-workflow/first-model/","title":"Our First Modeling Algorithm","text":"<p>Given the diverse range of business problems, it follows that there is a diverse range of modeling algorithms available. A crucial step in any data science project is selecting the appropriate algorithm for the task at hand. A model designed for real-time fraud detection has fundamentally different operational requirements than one built for forecasting annual sales.</p> <p>Think of machine learning algorithms as a collection of specialized tools in a data scientist's toolkit. While many tools can perform similar functions, some are better suited for specific jobs based on their power, precision, and ease of use. The choice of model often involves navigating a trade-off between several key factors.</p>"},{"location":"modeling/modeling-concepts-workflow/first-model/#key-factors-in-model-selection","title":"Key Factors in Model Selection","text":"<p>While dozens of algorithms exist, the selection process generally involves evaluating them against three primary criteria:</p> <ol> <li> <p>Performance: This refers to the predictive accuracy of the model. For a given problem, which model yields the most accurate and reliable predictions? This is often the primary consideration, but it's rarely the only one.</p> </li> <li> <p>Interpretability (or Explainability): This is the extent to which the model's inner workings and decision-making process can be understood by humans. In many business contexts, especially regulated industries, a \"black box\" model that provides a prediction without a clear explanation is unacceptable. High interpretability is crucial for stakeholder buy-in, regulatory compliance, and model debugging.</p> </li> <li> <p>Computational Cost: This factor considers the resources required to train and deploy a model. This includes processing time, memory usage, and the associated financial cost. A highly complex model might be slightly more accurate but prohibitively expensive or too slow for a real-time application.</p> </li> </ol>"},{"location":"modeling/modeling-concepts-workflow/first-model/#our-first-algorithm-the-decision-tree","title":"Our First Algorithm: The Decision Tree","text":"<p>For our initial lessons, we will use an algorithm known as a Decision Tree. We are selecting this algorithm as our starting point for one primary reason: its exceptional interpretability.</p> <p>A Decision Tree makes predictions by learning a hierarchical set of \"if/then\" rules, which can be visualized as a flowchart. This transparent structure allows us to see exactly how the model arrives at a conclusion, making it an excellent tool for learning the core concepts of modeling without the complexity of a \"black box\" algorithm.</p> <p>The goal of this lesson is not to master dozens of algorithms, but to master the end-to-end modeling workflow. By learning this process with one transparent model, you will build a framework that can be applied to any algorithm you encounter in the future. We will now proceed to that workflow.</p>"},{"location":"modeling/modeling-concepts-workflow/first-model/#recap-learning-activity","title":"Recap Learning Activity","text":"\ud83d\udca1 Activity: Feature Analysis for Model Simplification <p>Objective:</p> <p>To analyze a series of visualizations of the Iris dataset and, based on this analysis, make a data-driven recommendation on how to simplify a machine learning model for classifying iris species. This activity emphasizes the role of data visualization as an integral part of the modeling workflow for feature selection and building model intuition.</p> <p>Introduction:</p> <p>In machine learning, our goal is not always to build the most complex model possible. Often, a simpler, more efficient, and more interpretable model is preferable, provided it meets our performance requirements. One of the most effective ways to simplify a model is to reduce the number of input features, using only those with the most predictive power.</p> <p>Before we even train a model, we can use data visualization to gain insights into our features and make informed hypotheses about their usefulness. In this activity, you will act as a data scientist preparing to build a classification model for the Iris dataset. Your task is to analyze the provided charts to determine which features are most effective at discriminating between the three species (<code>setosa</code>, <code>versicolor</code>, and <code>virginica</code>) and which might be less useful or even redundant.</p> <p>Part 1: Chart-by-Chart Analysis</p> <p>Please examine each of the following visualizations and answer the guiding questions.</p> <p>**Chart 1: Univariate Density Plots **</p> <p></p> <p>This set of charts shows the distribution of values for each of the four features independently.</p> <ul> <li>Guiding Questions:<ul> <li>Look at the <code>petalLength</code> and <code>petalWidth</code> density plots. Do these distributions appear to be unimodal (one peak) or multimodal (multiple peaks)? What might the presence of multiple peaks suggest about the underlying data?</li> <li>Now consider the <code>sepalLength</code> and <code>sepalWidth</code> plots. How do their shapes differ from the petal measurements? Do they appear to offer a clear separation of groups based on their distributions alone?</li> </ul> </li> </ul> <p>Chart 2: Pairwise Scatter Plots (``)</p> <p></p> <p>This \"scatter plot matrix\" shows the relationship between every pair of features, with points colored by species.</p> <ul> <li>Guiding Questions:<ul> <li>Which single feature (looking at either an x-axis or y-axis) appears to provide the best separation for the <code>setosa</code> species (blue dots) from the other two species?</li> <li>Examine the plot of <code>petalLength</code> vs. <code>petalWidth</code>. How separable do the three species look in this two-dimensional view?</li> <li>Now look at <code>sepalLength</code> vs. <code>sepalWidth</code>. How well can you distinguish between the <code>versicolor</code> (orange) and <code>virginica</code> (pink) species in this view?</li> </ul> </li> </ul> <p>Chart 3: Scatter Plot with Histograms (``)</p> <p></p> <p>This chart provides a focused view of <code>petalLength</code> vs. <code>petalWidth</code> and includes histograms on the margins showing the distribution of each feature, binned and colored by species.</p> <ul> <li>Guiding Questions:<ul> <li>The histogram on the right shows the distribution of <code>petalWidth</code>. Based on this histogram, could you define a simple rule (e.g., \"if <code>petalWidth</code> is less than X...\") to perfectly identify the <code>setosa</code> species?</li> <li>How much overlap is there between <code>versicolor</code> and <code>virginica</code> along the <code>petalWidth</code> and <code>petalLength</code> dimensions?</li> </ul> </li> </ul> <p>Chart 4: Parallel Coordinates Plot (``)</p> <p></p> <p>This plot displays each individual flower as a line, connecting its measurements across all four features. Each line is colored by its species.</p> <ul> <li>Guiding Questions:<ul> <li>Trace the lines for the <code>setosa</code> species (blue) with your eyes. Do they follow a distinct pattern compared to the other two species? Which two features show the most dramatic separation for the <code>setosa</code> group?</li> <li>Now compare the <code>versicolor</code> (orange) and <code>virginica</code> (pink) lines. Where do their paths most overlap, and where do they show the most separation? Does this confirm what you observed in the other charts?</li> </ul> </li> </ul> <p>Part 2: Synthesis and Recommendation</p> <p>Based on your comprehensive analysis of all four charts, please formulate a recommendation for a modeling team.</p> <ul> <li>Final Task:<ol> <li>If you were asked to build a classification model with only two features, which two would you choose and why? Justify your answer by referencing specific insights from at least two of the provided charts.</li> <li>Which single feature appears to be the least useful for discriminating between all three species? Explain your reasoning.</li> <li>Bonus Question: How could a model trained on only your two chosen features potentially be simpler, faster, and more cost-effective in a real-world application (e.g., a mobile app that identifies flowers from measurements) compared to a model that uses all four features?</li> </ol> </li> </ul>"},{"location":"modeling/modeling-concepts-workflow/modeling-intro/","title":"Introduction to Modeling","text":""},{"location":"modeling/modeling-concepts-workflow/modeling-intro/#applications-of-predictive-modeling","title":"Applications of Predictive Modeling","text":"<p>In previous lessons, we established how to perform exploratory data analysis (EDA) to uncover patterns and trends within datasets. The logical progression from this analysis is to operationalize these insights. This is achieved through the development of predictive models.</p> <p>A model is a computational system that learns patterns and relationships directly from historical data. Instead of being explicitly programmed with a set of rules, a model infers its own logic, enabling automated and scalable decision-making. This section outlines the primary business applications of this technology.</p>"},{"location":"modeling/modeling-concepts-workflow/modeling-intro/#forecasting-and-prediction","title":"Forecasting and Prediction","text":"<p>Forecasting is a primary application of machine learning, enabling organizations to make data-driven projections about future events. These applications can be categorized by their operational cadence.</p> <ul> <li> <p>Batch Prediction for Complex Decisions: These models are executed periodically to inform strategic planning and resource allocation. They are not typically time-sensitive.</p> <ul> <li>Business Case: Forecasting quarterly revenue based on sales pipelines, marketing spend, and macroeconomic indicators.</li> <li>Business Case: Performing an annual analysis to predict which employees have a high probability of voluntary attrition, allowing HR to implement targeted retention strategies.</li> </ul> </li> <li> <p>Real-Time Prediction for Quick Reaction: These models are integrated into live systems to power immediate, automated decisions where low latency is critical.</p> <ul> <li>Business Case: A financial services platform processing a credit card transaction and instantly scoring it for fraud risk.</li> <li>Business Case: An e-commerce site dynamically selecting the optimal promotion to display to a user as a webpage renders.</li> </ul> </li> </ul>"},{"location":"modeling/modeling-concepts-workflow/modeling-intro/#recommendation-and-personalization","title":"Recommendation and Personalization","text":"<p>Recommendation engines are models designed to predict user preference and personalize customer experiences at scale. By anticipating customer needs, these systems are critical for driving engagement and sales in digital environments.</p> <ul> <li>Business Case: An online retailer presenting a \"Frequently Bought Together\" section, powered by a model that identifies product associations from transaction histories.</li> <li>Business Case: A streaming service (e.g., Netflix) curating a personalized library for each user by predicting which content they are most likely to watch based on their viewing history and similarity to other users.</li> </ul> <p>In these systems, models analyze past user behavior (features) to predict affinity for specific items (target), directly impacting key metrics like customer lifetime value and conversion rates.</p>"},{"location":"modeling/modeling-concepts-workflow/modeling-intro/#anomaly-and-outlier-detection","title":"Anomaly and Outlier Detection","text":"<p>Anomaly detection models are designed to identify rare events or observations that deviate significantly from established patterns. Their function is to flag critical exceptions that require investigation.</p> <ul> <li>Business Case: In manufacturing, a model monitors sensor data from production-line machinery. By identifying subtle deviations from normal operating parameters, it can trigger a proactive maintenance alert, preventing costly equipment failure.</li> <li>Business Case: For cybersecurity, models analyze network traffic and user access logs in real-time. An account exhibiting anomalous behavior, such as accessing unusual files at an odd hour, can be automatically flagged as a potential security threat.</li> </ul> <p>Each of these applications\u2014forecasting, personalization, and anomaly detection\u2014is powered by a model trained on historical data. The effectiveness of these applications is entirely dependent on the quality and performance of the underlying model. To build high-quality models consistently, a structured and repeatable methodology is required.</p> <p>This methodology is the modeling workflow, which is the focus of this lesson.</p>"},{"location":"modeling/modeling-concepts-workflow/next-steps/","title":"Next Steps and Things to Try","text":"<p>Mastery of a new skill comes from practice and continued exploration. This final section provides a set of recommended exercises to solidify your understanding of the modeling workflow and introduces related topics for further independent study.</p>"},{"location":"modeling/modeling-concepts-workflow/next-steps/#recommended-practice-exercises","title":"Recommended Practice Exercises","text":"<p>The following exercises are designed to build a more intuitive understanding of the concepts covered in this lesson. We recommend performing them in your lab notebook.</p> <ol> <li> <p>Analyze Hyperparameter Impact:</p> <ul> <li>The <code>max_depth</code> hyperparameter directly controls the complexity of a Decision Tree. In our lab, we used a value of <code>3</code>.</li> <li>Task: Re-run the workflow multiple times, using different values for <code>max_depth</code> (e.g., <code>1</code>, <code>2</code>, <code>5</code>, <code>10</code>, and <code>None</code>). For each run, record both the accuracy on the training set and the accuracy on the test set.</li> <li>Analysis: Observe the relationship between model complexity and performance. At what depth does the model begin to exhibit signs of overfitting (i.e., high training accuracy but declining or stagnant test accuracy)?</li> </ul> </li> <li> <p>Investigate Feature Importance:</p> <ul> <li>Our model used all four available features. However, not all features contribute equally to predictive power.</li> <li>Task: Train two new, separate models: one using only the <code>petal width (cm)</code> and <code>petal length (cm)</code> features, and another using only the <code>sepal width (cm)</code> and <code>sepal length (cm)</code> features.</li> <li>Analysis: Quantify the performance difference between these two models. Which set of features appears to be more informative for this classification problem?</li> </ul> </li> <li> <p>Evaluate the Impact of the Test/Train Split:</p> <ul> <li>The <code>test_size</code> parameter in the <code>train_test_split</code> function determines the allocation of data between training and testing.</li> <li>Task: Modify the <code>test_size</code> to <code>0.5</code> (50% for testing) and then to <code>0.1</code> (10% for testing). Re-run the workflow for each and observe the resulting test accuracy.</li> <li>Analysis: Consider the trade-offs involved. A larger test set provides a more robust evaluation but leaves less data for model training. How might this choice impact your confidence in the model?</li> </ul> </li> <li> <p>Apply the Workflow to a New Dataset:</p> <ul> <li>The most effective way to confirm your understanding of the process is to apply it to a new problem.</li> <li>Task: <code>scikit-learn</code> includes a dataset on the chemical analysis of wines from three different cultivars (<code>load_wine</code>). Apply the complete end-to-end workflow to this dataset: load the data, prepare <code>X</code> and <code>y</code>, split, train a <code>DecisionTreeClassifier</code>, and evaluate its accuracy.</li> </ul> </li> </ol>"},{"location":"modeling/modeling-concepts-workflow/next-steps/#topics-for-further-exploration","title":"Topics for Further Exploration","text":"<p>For those who wish to look ahead, the following topics are logical extensions of this lesson. Understanding these concepts is fundamental to professional data science practice.</p> <ol> <li> <p>The Confusion Matrix:</p> <ul> <li>Accuracy provides a single, high-level score. A Confusion Matrix offers a more detailed breakdown of a classification model's performance, showing the number of true positives, true negatives, false positives, and false negatives. This is essential when the business costs of different types of errors are unequal.</li> </ul> Confusion Matrix Normalized Confusion Matrix </li> <li> <p>Precision, Recall, and F1-Score:</p> <ul> <li>Derived from the Confusion Matrix, these metrics provide more nuanced insights than accuracy alone. Precision measures the accuracy of positive predictions, while Recall measures the model's ability to identify all actual positive instances. They are critical for problems like disease detection or fraud analytics.</li> </ul> </li> <li> <p>Alternative Classification Algorithms:</p> <ul> <li>The Decision Tree is just one approach. An alternative and highly intuitive method is K-Nearest Neighbors (KNN), which classifies a data point based on the majority class of its closest neighbors. Exploring other algorithms reinforces the concept that different problems may benefit from different tools.</li> </ul> </li> <li> <p>Cross-Validation:</p> <ul> <li>A single train-test split provides one estimate of model performance, which can be subject to luck based on how the data was partitioned. Cross-Validation is a more robust evaluation technique where the splitting and training process is repeated multiple times to provide a more stable and reliable estimate of the model's performance on unseen data. This will be a core topic in our next lesson.</li> </ul> </li> </ol>"},{"location":"modeling/modeling-concepts-workflow/sklearn-api/","title":"Estimator API","text":""},{"location":"modeling/modeling-concepts-workflow/sklearn-api/#the-scikit-learn-estimator-api-a-consistent-framework-for-modeling","title":"The <code>scikit-learn</code> Estimator API: A Consistent Framework for Modeling","text":"<p>One of the primary reasons for <code>scikit-learn</code>'s dominance in the machine learning landscape is not just the breadth of its algorithms, but the thoughtful and consistent design of its Application Programming Interface (API). Understanding this API is key to moving from executing a single workflow to efficiently experimenting with a wide array of modeling techniques.</p>"},{"location":"modeling/modeling-concepts-workflow/sklearn-api/#the-power-of-consistency","title":"The Power of Consistency","text":"<p>In business, standardized processes like GAAP in accounting or ISO standards in manufacturing are critical because they create a common language, ensuring reliability and interoperability. The <code>scikit-learn</code> API provides an analogous benefit for machine learning.</p> <p>Instead of learning unique syntax for every algorithm, you learn a single, unified pattern. Once you understand the workflow for a <code>DecisionTreeClassifier</code>, you inherently understand the workflow for hundreds of other models, from <code>LogisticRegression</code> to <code>GradientBoostingClassifier</code>. This consistency provides a significant strategic advantage:</p> <ul> <li>It reduces cognitive load: You can focus on the business problem and the modeling strategy rather than memorizing new commands.</li> <li>It accelerates experimentation: Swapping one model for another is often a one-line code change, making it easy to benchmark multiple approaches.</li> <li>It enables robust automation: The uniform structure allows for the seamless combination of different processing and modeling steps into powerful, automated pipelines.</li> </ul>"},{"location":"modeling/modeling-concepts-workflow/sklearn-api/#the-core-api-the-three-key-methods","title":"The Core API: The Three Key Methods","text":"<p>The <code>scikit-learn</code> API is built around three primary methods, or \"verbs,\" that correspond to distinct modeling tasks. Every object you encounter will implement one or more of these methods.</p> <ul> <li> <p>The Learner: <code>.fit(X, y)</code>     This is the universal method for training any model. It takes the training data (<code>X_train</code>) and corresponding labels (<code>y_train</code>) as input. The <code>.fit()</code> method executes the learning algorithm, and the resulting learned parameters are stored as attributes within the estimator object itself.</p> </li> <li> <p>The Predictor: <code>.predict(X)</code>     This method is used by supervised models (classifiers and regressors) after they have been fitted. It takes new data (e.g., <code>X_test</code>) for which the target is unknown and returns the model's predictions.</p> </li> <li> <p>The Transformer: <code>.transform(X)</code>     This method is used by preprocessing estimators (e.g., <code>StandardScaler</code> for feature scaling or <code>PCA</code> for dimensionality reduction). It takes a dataset as input and returns a transformed version of that data. A common and powerful shortcut is <code>.fit_transform(X)</code>, which learns the transformation parameters from the data and applies the transformation in a single step. Caution: This shortcut should almost exclusively be used on training data to avoid data leakage.</p> </li> </ul>"},{"location":"modeling/modeling-concepts-workflow/sklearn-api/#building-blocks-and-orchestrators-simple-vs-meta-estimators","title":"Building Blocks and Orchestrators: Simple vs. Meta-Estimators","text":"<p>The objects in <code>scikit-learn</code> can be thought of in two categories, allowing them to be composed into powerful workflows.</p> <ul> <li> <p>Simple Estimators: These are the fundamental building blocks. Each one performs a single, well-defined task. Examples include <code>DecisionTreeClassifier</code> (a classifier), <code>LinearRegression</code> (a regressor), and <code>StandardScaler</code> (a transformer).</p> </li> <li> <p>Meta-Estimators: These are \"orchestrator\" or \"manager\" objects that take other estimators as inputs. They are used to automate and control the modeling workflow. The two most important meta-estimators are:</p> <ul> <li><code>Pipeline</code>: This object chains together a sequence of transformers and a final estimator. For example, you can create a single <code>Pipeline</code> object that first scales your data and then trains a classifier. This is the industry-standard tool for ensuring that data preprocessing steps are applied correctly and for preventing data leakage.</li> <li><code>GridSearchCV</code>: This object automates the process of hyperparameter tuning. It takes an estimator (which can be a single model or an entire <code>Pipeline</code>) and a grid of hyperparameters to test, then systematically finds the best combination using cross-validation.</li> </ul> </li> </ul>"},{"location":"modeling/modeling-concepts-workflow/sklearn-api/#practical-considerations-instantiation-and-state","title":"Practical Considerations: Instantiation and State","text":"<p>Understanding two final concepts is crucial for using the API effectively and avoiding common errors.</p> <ul> <li> <p>Instantiation: As we saw in the lab, all estimators are configured before use by passing hyperparameters during instantiation (e.g., <code>model = DecisionTreeClassifier(max_depth=3)</code>). This is a universal pattern across the entire library.</p> </li> <li> <p>The \"Fitted\" State: A <code>scikit-learn</code> estimator is a \"stateful\" object. Calling the <code>.fit()</code> method modifies the object in place, changing its state from \"unfitted\" to \"fitted.\"</p> <ul> <li>The Underscore Convention: You can identify a fitted estimator by the presence of attributes that end with an underscore (<code>_</code>). For example, after fitting a <code>DecisionTreeClassifier</code>, you can access <code>model.feature_importances_</code>. These attributes only exist after <code>.fit()</code> has been called.</li> <li>Common Pitfall: Data Leakage. A frequent mistake is to fit a transformer (e.g., <code>StandardScaler</code>) on the entire dataset before the train-test split. This \"leaks\" information from the test set into the training process, leading to overly optimistic performance estimates. The correct approach is to fit the scaler only on the training data and then use it to transform both the training and test sets, a process that a <code>Pipeline</code> handles automatically.</li> <li>The Golden Rule: The <code>.fit()</code> and <code>.fit_transform()</code> methods should only ever see training data. The only method that should be applied to the test data is <code>.transform()</code> (for preprocessors) or <code>.predict()</code> (for models).</li> </ul> </li> </ul>"},{"location":"modeling/modeling-concepts-workflow/sklearn-datasets/","title":"A Student's Guide to Datasets in Scikit-learn:","text":"<p>A common hurdle for students starting in machine learning is the belief that progress requires access to large, \"real-world\" datasets. This can be paralyzing. However, the goal of learning is to master concepts and workflows, and for this, a different kind of dataset is often superior. <code>scikit-learn</code> provides a comprehensive suite of datasets perfect for building foundational skills without the overhead of data acquisition and cleaning.</p> <p>Think of these datasets not as a substitute for real-world data, but as a specialized learning environment\u2014like a laboratory or a flight simulator. They allow you to isolate variables, test algorithms, and understand core principles in a controlled setting.</p>"},{"location":"modeling/modeling-concepts-workflow/sklearn-datasets/#types-of-datasets-and-their-purpose","title":"Types of Datasets and Their Purpose","text":"<p><code>scikit-learn</code> offers three main categories of datasets, each serving a distinct educational purpose.</p> <p>1. Toy Datasets (<code>load_*</code>)</p> <p>These are small, clean, and pre-packaged with the library. They require no downloading and are ready for immediate use. Their simplicity is their greatest strength; they allow you to focus entirely on the model's behavior without getting bogged down in data preparation.</p> <p>2. Real-World Datasets (<code>fetch_*</code>)</p> <p>These are larger datasets that have been used in academic and benchmark studies. They are downloaded and cached by <code>scikit-learn</code> the first time you use them. They are more complex than toy datasets and better represent the scale and noise of real-world problems, making them an excellent \"next step\" after mastering the basics.</p> <p>3. Generated Datasets (<code>make_*</code>)</p> <p>These are synthetic datasets created with precise mathematical properties. Their value is in allowing you to control for specific variables like the number of features, the number of classes, the amount of noise, and the underlying structure of the data.</p> <p>Why Do We Need Generated Datasets?</p> <p>You may wonder why we would ever use \"fake\" data. Generated datasets are like a wind tunnel for testing an airplane. They allow you to test your algorithms under perfectly known conditions.</p> <ul> <li>Want to see how a clustering algorithm behaves when the clusters are perfectly spherical versus elongated? Use <code>make_blobs</code>.</li> <li>Need to test if a linear model can solve a non-linear problem? Use <code>make_moons</code> or <code>make_circles</code> and watch it fail, then see a non-linear model succeed.</li> </ul> <p>This ability to control the data's structure is invaluable for building a deep, intuitive understanding of how different algorithms work and where their limitations lie.</p>"},{"location":"modeling/modeling-concepts-workflow/sklearn-datasets/#recommendations-for-practice","title":"Recommendations for Practice","text":"<p>Here are specific datasets you can use to practice different machine learning tasks, starting today.</p>"},{"location":"modeling/modeling-concepts-workflow/sklearn-datasets/#for-classification-tasks","title":"For Classification Tasks","text":"<p>Your goal is to predict a discrete category (e.g., type of flower, presence of a disease).</p> Task Type Recommended Dataset Algorithm to Try Getting Started <code>load_iris()</code>: The \"hello world\" of classification. Predict one of three iris species from four measurements. It's clean and simple. <code>DecisionTreeClassifier</code>, <code>KNeighborsClassifier</code> Binary Classification <code>load_breast_cancer()</code>: Predict whether a tumor is malignant or benign. A classic binary problem. <code>LogisticRegression</code>, <code>SVC</code> (Support Vector Classifier) Visualizing Boundaries <code>make_moons(noise=0.1)</code>: Two interleaving half-circles. Excellent for seeing how non-linear models like <code>SVC</code> or <code>DecisionTreeClassifier</code> create complex decision boundaries. <code>SVC(kernel='rbf')</code>, <code>DecisionTreeClassifier</code> Image Classification <code>load_digits()</code>: A dataset of 8x8 pixel images of handwritten digits (0-9). A step up in feature complexity. <code>RandomForestClassifier</code>, <code>SVC</code>"},{"location":"modeling/modeling-concepts-workflow/sklearn-datasets/#for-regression-tasks","title":"For Regression Tasks","text":"<p>Your goal is to predict a continuous numerical value (e.g., price, measurement).</p> Task Type Recommended Dataset Algorithm to Try Getting Started <code>load_diabetes()</code>: Predict disease progression one year after baseline from ten physiological measurements. Small and well-behaved. <code>LinearRegression</code>, <code>Ridge</code>, <code>Lasso</code> More Complex Regression <code>fetch_california_housing()</code>: A larger dataset where the goal is to predict the median house value in a California district. More features and more samples. <code>RandomForestRegressor</code>, <code>GradientBoostingRegressor</code> Understanding Non-Linearity <code>make_regression(n_features=1, noise=20)</code> paired with some non-linear transformation of <code>y</code>. Compare <code>LinearRegression</code> to <code>DecisionTreeRegressor</code>"},{"location":"modeling/modeling-concepts-workflow/sklearn-datasets/#for-clustering-tasks","title":"For Clustering Tasks","text":"<p>Your goal is to find natural groupings in data without having any pre-defined labels.</p> Task Type Recommended Dataset Algorithm to Try Getting Started <code>make_blobs(n_samples=200, centers=4)</code>: The ideal dataset for understanding clustering. Generates perfectly round, well-separated \"blobs\" of data. <code>KMeans</code>, <code>AgglomerativeClustering</code> Non-Spherical Clusters <code>make_moons(noise=0.05)</code>: Use the same dataset as for classification, but this time without the labels. See how <code>KMeans</code> struggles and how an algorithm like <code>DBSCAN</code> excels. <code>KMeans</code>, <code>DBSCAN</code> High-Dimensional Data Use the features (<code>X</code>) from <code>load_digits()</code>: Can a clustering algorithm group the handwritten digits without seeing the labels? <code>KMeans</code>, <code>PCA</code> + <code>KMeans</code> <p>By leveraging these powerful and accessible resources within <code>scikit-learn</code>, you can immediately begin to build, test, and understand machine learning models without the \"need for real-world large datasets\" holding you back.</p>"},{"location":"modeling/modeling-concepts-workflow/workflow-in-action/","title":"Workflow in Action","text":""},{"location":"modeling/modeling-concepts-workflow/workflow-in-action/#workflow-in-action","title":"Workflow in Action","text":"<p>In this we will translate the six-step theoretical workflow into a practical, end-to-end coding exercise. You will train, predict with, and evaluate your first machine learning model using a Iris dataset, a classic toy dataset tht's very useful for learning various topics in machine learning.</p> <p>Objective: Apply the complete modeling workflow to the Iris dataset using <code>scikit-learn</code> and develop familiarity with <code>scikit-learn</code>'s estimator API.</p>"},{"location":"modeling/modeling-concepts-workflow/workflow-in-action/#setup-importing-libraries","title":"Setup: Importing Libraries","text":"<p>First, we import the necessary libraries. We need <code>Polars</code> for data handling and various modules from <code>scikit-learn</code> for loading the dataset, splitting the data, instantiating the model, and evaluating its performance.</p> <pre><code>import polars as pl\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# For the bonus visualization\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n</code></pre>"},{"location":"modeling/modeling-concepts-workflow/workflow-in-action/#step-1-load-prepare-data","title":"Step 1: Load &amp; Prepare Data","text":"<p>We will use the classic Iris dataset, which is conveniently included with <code>scikit-learn</code>. Our first action is to load the data and convert it into a Polars DataFrame, which allows for easier inspection and manipulation. We then separate our features (<code>X</code>) from our target (<code>y</code>).</p> <pre><code># Load the dataset from scikit-learn\niris_data = load_iris()\n\n# Create a Polars DataFrame\n# The data is in '.data' and column names in '.feature_names'\ndf = pl.DataFrame(\n    data=iris_data.data,\n    schema=iris_data.feature_names\n).with_columns(\n    # Add the target variable, which is in '.target'\n    pl.Series(name=\"target\", values=iris_data.target)\n)\n\n# Define our features (X) and target (y)\nX = df.select(pl.exclude(\"target\"))\ny = df.select(\"target\")\n\n# Display the first 5 rows of the feature DataFrame\nprint(\"Features (X):\")\nprint(X.head())\n\n# Display the first 5 rows of the target DataFrame\nprint(\"\\nTarget (y):\")\nprint(y.head())\n</code></pre>"},{"location":"modeling/modeling-concepts-workflow/workflow-in-action/#step-2-split-the-data","title":"Step 2: Split the Data","text":"<p>Next, we partition the data into training and testing sets. This is a critical step to ensure an unbiased evaluation of our model. We will allocate 30% of the data for the test set and set a <code>random_state</code> to ensure that our split is reproducible every time the code is run.</p> <p><pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Verify the dimensions of the resulting datasets\nprint(f\"Shape of X_train: {X_train.shape}\")\nprint(f\"Shape of X_test: {X_test.shape}\")\n</code></pre> You should see that <code>X_train</code> has 105 rows and <code>X_test</code> has 45 rows, corresponding to a 70/30 split of the original 150 rows.</p>"},{"location":"modeling/modeling-concepts-workflow/workflow-in-action/#step-3-instantiate-the-model","title":"Step 3: Instantiate the Model","text":"<p>We will now instantiate our <code>DecisionTreeClassifier</code>. To prevent the model from becoming overly complex, we will set the <code>max_depth</code> hyperparameter to <code>3</code>. We also pass the <code>random_state</code> for reproducibility of the model's training process.</p> <pre><code>model = DecisionTreeClassifier(max_depth=3, random_state=42)\nprint(model)\n</code></pre>"},{"location":"modeling/modeling-concepts-workflow/workflow-in-action/#step-4-train-the-model","title":"Step 4: Train the Model","text":"<p>With our model instance created and our training data ready, we can execute the training step. We use the <code>.fit()</code> method on our training data (<code>X_train</code> and <code>y_train</code>).</p> <p><pre><code>model.fit(X_train, y_train)\n</code></pre> After this cell runs, the <code>model</code> object is no longer an empty shell; it now contains the learned rules (parameters) from the Iris training data.</p>"},{"location":"modeling/modeling-concepts-workflow/workflow-in-action/#step-5-generate-predictions","title":"Step 5: Generate Predictions","text":"<p>Our model is now trained. We can use the <code>.predict()</code> method to generate predictions on the unseen data we held back in our test set (<code>X_test</code>).</p> <pre><code># Generate predictions for the test set\npredictions = model.predict(X_test)\n\n# Display the first 10 predictions\nprint(predictions[:10])\n</code></pre>"},{"location":"modeling/modeling-concepts-workflow/workflow-in-action/#step-6-evaluate-model-performance","title":"Step 6: Evaluate Model Performance","text":"<p>This is the final step where we quantify the model's performance. We compare the <code>predictions</code> our model made against the actual ground truth labels in <code>y_test</code> using the <code>accuracy_score</code> metric.</p> <p><pre><code># Calculate the accuracy of the model\naccuracy = accuracy_score(y_test, predictions)\n\nprint(f\"Model Accuracy on Test Set: {accuracy:.4f}\")\n</code></pre> The accuracy score represents the proportion of correct classifications. An accuracy of 1.0 would mean a perfect score on the test set.</p>"},{"location":"modeling/modeling-concepts-workflow/workflow-in-action/#visualizing-the-models-logic","title":"Visualizing the Model's Logic","text":"<p>A key advantage of the Decision Tree is its interpretability. We can visualize the rules the model learned during the <code>.fit()</code> step to understand exactly how it is making decisions.</p> <p><pre><code># Set the figure size for better readability\nplt.figure(figsize=(20, 10))\n\n# Create the plot\nplot_tree(\n    model,\n    feature_names=X.columns,\n    class_names=iris_data.target_names,\n    filled=True,\n    rounded=True\n)\n\n# Display the plot\nplt.show()\n</code></pre> This visualization shows the flowchart that the model uses. Starting from the top (root) node, you can trace a path down the tree based on the feature values of a given flower to see how it arrives at a final classification (leaf node).</p> Looking under the hood of a decision tree model <p>We can also visually explore to understand how different features contribute to how model performs and identify opportunities to simplify the model in the next iteration.</p> <pre><code>iris = load_iris()\n\n# Parameters\nn_classes = 3\nplot_colors = \"ryb\"\nplot_step = 0.02\n\n\nfor pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):\n    # We only take the two corresponding features\n    X = iris.data[:, pair]\n    y = iris.target\n    class_names = iris.target_names\n\n    # Train\n    clf = tree.DecisionTreeClassifier(max_depth=3, random_state=42).fit(X, y)\n\n    # Plot the decision boundary\n    ax = plt.subplot(2, 3, pairidx + 1)\n    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n    DecisionBoundaryDisplay.from_estimator(\n        clf,\n        X,\n        cmap=plt.cm.RdYlBu,\n        response_method=\"predict\",\n        ax=ax,\n        xlabel=iris.feature_names[pair[0]],\n        ylabel=iris.feature_names[pair[1]],\n    )\n\n    # Plot the training points\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.asarray(y == i).nonzero()\n        plt.scatter(\n            X[idx, 0],\n            X[idx, 1],\n            c=color,\n            label=iris.target_names[i],\n            edgecolor=\"black\",\n            s=15,\n        )\n\nplt.suptitle(\"Decision surface of decision trees trained on pairs of features\")\nplt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n_ = plt.axis(\"tight\")\n</code></pre> Some features are more equal than the others!"},{"location":"modeling/modeling-concepts-workflow/workflow/","title":"The Modeling Workflow with scikit-learn","text":"<p>A standardized workflow is essential for building reliable and reproducible machine learning models. Adhering to a defined process minimizes errors, prevents common pitfalls like data leakage, and ensures that the final model's performance is evaluated objectively.</p> <p>This section outlines the standard modeling workflow and maps each conceptual step to its implementation using the <code>scikit-learn</code> library, the industry standard for general-purpose machine learning in Python.</p>"},{"location":"modeling/modeling-concepts-workflow/workflow/#step-1-define-objective-and-prepare-data","title":"Step 1: Define Objective and Prepare Data","text":"<ul> <li>Concept: Before any code is written, the business objective must be clearly defined. This objective is then translated into a machine learning problem by identifying the target to be predicted and the features that will be used to predict it.<ul> <li>The Target (commonly denoted as <code>y</code>) is the outcome variable we want to predict (e.g., customer churn, sales revenue).</li> <li>The Features (denoted as <code>X</code>) are the independent variables or predictors used to inform the prediction (e.g., customer tenure, marketing spend).</li> </ul> </li> <li>API Implementation: This step is primarily handled using a data manipulation library like Polars. You will use Polars to clean your data, select the relevant columns for your features and target, and handle any missing values. The result is two distinct data objects, <code>X</code> and <code>y</code>, that will be passed to <code>scikit-learn</code>.</li> </ul>"},{"location":"modeling/modeling-concepts-workflow/workflow/#step-2-split-data-for-unbiased-evaluation","title":"Step 2: Split Data for Unbiased Evaluation","text":"<ul> <li>Concept: A fundamental principle of modeling is to evaluate a model on data it has never seen before. This simulates how the model would perform in a real-world production environment. To achieve this, we partition our dataset into two separate sets:<ul> <li>Training Set: The subset of data the model will learn from.</li> <li>Testing Set: The subset of data held back to provide an unbiased evaluation of the final model's performance. This prevents \"information leakage\" and ensures the model is not simply memorizing the training data.</li> </ul> </li> <li>API Implementation: <code>scikit-learn</code> provides a straightforward utility function for this purpose: <code>train_test_split</code> from the <code>sklearn.model_selection</code> module. It takes <code>X</code> and <code>y</code> as inputs and returns four new datasets: <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code>. The <code>test_size</code> parameter controls the proportion of the split, and <code>random_state</code> is set to ensure the split is reproducible.</li> </ul>"},{"location":"modeling/modeling-concepts-workflow/workflow/#step-3-instantiate-the-model","title":"Step 3: Instantiate the Model","text":"<ul> <li>Concept: This step involves selecting the algorithm from the \"toolbox\" and creating an instance of it. This is where we configure the model's hyperparameters\u2014the settings that are specified before the training process begins. For a Decision Tree, a key hyperparameter is <code>max_depth</code>, which controls the complexity of the final model.</li> <li>API Implementation: A model is instantiated in Python by creating an object of its class. For example: <code>model = DecisionTreeClassifier(max_depth=3, random_state=42)</code>. The chosen hyperparameters are passed as arguments during this instantiation.</li> </ul>"},{"location":"modeling/modeling-concepts-workflow/workflow/#step-4-train-the-model-fitting","title":"Step 4: Train the Model (Fitting)","text":"<ul> <li>Concept: Training is the core learning phase. During this step, the algorithm systematically processes the training data (<code>X_train</code> and <code>y_train</code>) to learn the optimal internal parameters that map the features to the target. For a Decision Tree, this involves determining the most effective series of if/then rules.</li> <li>API Implementation: <code>scikit-learn</code> has a remarkably consistent API. The training process for virtually every algorithm is executed using the <code>.fit()</code> method. The call is <code>model.fit(X_train, y_train)</code>. After this step is complete, the <code>model</code> object now contains the learned parameters.</li> </ul>"},{"location":"modeling/modeling-concepts-workflow/workflow/#step-5-generate-predictions","title":"Step 5: Generate Predictions","text":"<ul> <li>Concept: Once the model has been trained, it can be used to make predictions on new, unseen data. The first application of this is to generate predictions for our held-back testing set (<code>X_test</code>) so we can evaluate its performance.</li> <li>API Implementation: Predictions are generated using the <code>.predict()</code> method: <code>predictions = model.predict(X_test)</code>. The output is an array of predicted values, one for each observation in <code>X_test</code>.</li> </ul>"},{"location":"modeling/modeling-concepts-workflow/workflow/#step-6-evaluate-model-performance","title":"Step 6: Evaluate Model Performance","text":"<ul> <li>Concept: This final step quantifies the model's quality and business value. By comparing the model's <code>predictions</code> to the actual values (<code>y_test</code>), we can calculate performance metrics. This evaluation reveals how well the model is likely to perform in production and exposes potential issues like overfitting\u2014a condition where the model performs well on training data but poorly on new data because it has memorized noise rather than learning the true underlying signal.</li> <li>API Implementation: The <code>sklearn.metrics</code> module contains a wide range of evaluation functions. For a classification task, a common starting point is <code>accuracy_score</code>. It is called as <code>accuracy_score(y_test, predictions)</code> and returns the proportion of correct predictions.</li> </ul>"},{"location":"modeling/modeling-concepts-workflow/workflow/#the-iterative-nature-of-the-workflow","title":"The Iterative Nature of the Workflow","text":"<p>It is critical to understand that the six steps outlined above represent a single cycle within a larger, iterative and exploratory process. In a business context, a model is rarely built once and then forgotten. The workflow is continuously repeated to refine performance, adapt to new information, and meet evolving business demands.</p> <p>The initial model developed serves as a baseline. From there, data science teams iterate through the workflow to improve upon this baseline. This cyclical process is driven by several factors:</p> <ul> <li> <p>Model &amp; Hyperparameter Experimentation: The first choice of algorithm and its settings is just a starting point. Subsequent iterations will involve testing different algorithms and tuning their hyperparameters to improve predictive performance.</p> </li> <li> <p>Evolving Data Landscape: The data available to an organization is not static. New data sources may become available, or the statistical properties of the input data can change over time (a concept known as \"data drift\"), requiring the model to be retrained or redesigned.</p> </li> <li> <p>Shifting Business Requirements: The initial problem definition or performance expectation may change. Stakeholders might request a model that prioritizes a different metric (e.g., shifting focus from overall accuracy to minimizing a specific type of error), or the required performance threshold for deployment might be raised.</p> </li> <li> <p>Changing Infrastructure: The availability of new compute infrastructure or more efficient software libraries can enable the use of more complex and powerful models that were previously not feasible.</p> </li> </ul> <p>Therefore, view the modeling workflow not as a linear path to a final answer, but as a systematic framework for continuous improvement and adaptation in a dynamic environment. The skills learned here allow you to execute each cycle with rigor and efficiency.</p>"},{"location":"modeling/modeling-without-target/","title":"Modeling without a Target","text":"<ul> <li> Starter Colab Notebook</li> <li> Modeling without a Target </li> <li> Clustering </li> <li> Dimensionality Reduction </li> <li> Feature Engineering </li> <li> Clustering + Dimensionality Reduction </li> <li> Fine-tuning Feature Engineering </li> <li> Next Steps </li> </ul>"},{"location":"modeling/modeling-without-target/clustering/","title":"An Introduction to Clustering","text":"<p>Clustering is a fundamental unsupervised learning technique used to discover natural groupings within data. The objective is to partition data points into distinct subgroups, or clusters, such that points within the same cluster are highly similar, while points in different clusters are dissimilar.</p> <p>A classic business application is market analysis. Analysts traditionally group stocks using pre-defined categories like GICS sectors (\"Technology,\" \"Healthcare,\" \"Financials\"). However, these labels may not accurately reflect how stocks actually behave based on their price movements.</p> <p>A more sophisticated approach, demonstrated in a well-known <code>scikit-learn</code> example, is to use a clustering algorithm like Affinity Propagation to analyze the correlation of stock price fluctuations. This method learns the structure directly from the data, revealing data-driven clusters of companies whose stocks tend to move together. These discovered groupings can be more meaningful for portfolio diversification and risk management than traditional industry classifications.</p>"},{"location":"modeling/modeling-without-target/clustering/#a-case-study-dataset-handwritten-digits","title":"A Case Study Dataset: Handwritten Digits","text":"<p>Throughout this lesson, we will use the UCI ML handwritten digits dataset as a running example. This dataset, available in <code>scikit-learn</code> via <code>load_digits()</code>, contains thousands of 8x8 pixel images of handwritten digits (0-9). Each image is represented as a row of 64 features (one for each pixel's intensity).</p> <p>This dataset is an excellent learning tool because it's complex enough to be interesting but small enough for rapid experimentation. Crucially, while we will treat it as unlabeled data for our clustering tasks, it comes with ground truth labels (we know which image corresponds to which digit). This allows us to objectively evaluate how well our unsupervised algorithms perform at rediscovering the inherent structure of the data.</p>"},{"location":"modeling/modeling-without-target/clustering/#k-means-clustering-a-foundational-algorithm","title":"K-Means Clustering: A Foundational Algorithm","text":"<p>One of the most popular and intuitive clustering algorithms is K-Means. Its goal is to partition the data into a pre-specified number of clusters ('K') by finding cluster centers, or centroids, that minimize the distance from each data point to its assigned centroid.</p>"},{"location":"modeling/modeling-without-target/clustering/#key-hyperparameters","title":"Key Hyperparameters","text":"<p>When instantiating a <code>KMeans</code> model, you will encounter several key hyperparameters:</p> <ul> <li><code>n_clusters</code>: This is the most important hyperparameter. It defines the number of clusters the algorithm must find. The choice of <code>n_clusters</code> is a critical part of the modeling process.</li> <li><code>init</code>: This parameter specifies the method for initializing the centroids. The default, <code>'k-means++'</code>, is a \"smart\" initialization method that places initial centroids far apart, leading to more reliable and consistent results than the <code>'random'</code> option.</li> <li><code>n_init</code>: Because the algorithm's outcome can depend on the starting position of the centroids, <code>n_init</code> controls how many times the algorithm will be run with different random initializations. The default value of <code>10</code> is standard practice, and <code>scikit-learn</code> will automatically return the best result.</li> </ul>"},{"location":"modeling/modeling-without-target/clustering/#assumptions-and-limitations","title":"Assumptions and Limitations","text":"<p>K-Means is powerful but relies on several key assumptions:</p> <ul> <li>Pre-specification of 'k': You must decide the number of clusters beforehand.</li> <li>Isotropic Distribution: K-Means works best when clusters are isotropic (spherical) and have similar variance. It assumes all directions are equally important.</li> <li>Cluster Size: The algorithm implicitly assumes that clusters are of a similar size and density. It can struggle with identifying elongated clusters or groups of varying sizes.</li> </ul>"},{"location":"modeling/modeling-without-target/clustering/#evaluating-clustering-performance","title":"Evaluating Clustering Performance","text":"<p>Since we have no \"ground truth\" target, we cannot use metrics like accuracy. Instead, we use metrics that evaluate the quality of the clusters themselves. These fall into two categories.</p>"},{"location":"modeling/modeling-without-target/clustering/#metrics-requiring-ground-truth-labels","title":"Metrics Requiring Ground Truth Labels","text":"<p>When true labels are available (as in our <code>digits</code> dataset), we can use them to score our model's performance.</p> <ul> <li>Homogeneity: Measures if each cluster contains only members of a single class.</li> <li>Completeness: Measures if all members of a given class are assigned to the same cluster.</li> <li>V-Measure: The harmonic mean of homogeneity and completeness.</li> <li>Adjusted Rand Index (ARI) &amp; Adjusted Mutual Information (AMI): These measure the similarity between the true and predicted labels, correcting for chance. Scores close to 1.0 are good.</li> </ul>"},{"location":"modeling/modeling-without-target/clustering/#metrics-not-requiring-ground-truth-labels","title":"Metrics Not Requiring Ground Truth Labels","text":"<p>In most real-world scenarios, you won't have true labels. The Silhouette Coefficient is a powerful metric for these situations.</p> <ul> <li>Silhouette Coefficient: This metric measures how similar a data point is to its own cluster compared to how similar it is to other, nearby clusters. The score ranges from -1 to 1, where higher values indicate that the object is well-matched to its own cluster and poorly matched to neighboring clusters. It can be used to evaluate the quality of the clustering itself and to help determine the optimal number of clusters.</li> </ul>"},{"location":"modeling/modeling-without-target/clustering/#understanding-the-digits-dataset","title":"Understanding the Digits Dataset","text":"<p>Before we model, it's crucial to understand our data. The <code>load_digits</code> dataset contains images of handwritten digits (0 through 9).</p> <ul> <li> <p>What is a row? Each row in the <code>data</code> array represents a single, complete image of a handwritten digit.</p> </li> <li> <p>What is a column? Each image is a tiny 8x8 grid of pixels. To turn this grid into a data row, it's \"unrolled\" into a line of 64 pixel values. So, each of the 64 columns represents the brightness of a single pixel in the 8x8 image.</p> </li> <li> <p>What does clustering achieve here? Our goal is to see if a machine learning model, without any prior knowledge of the actual digit labels, can group the images based on their visual similarity. Essentially, we're asking the model to discover the concept of \"a zero,\" \"a one,\" and so on, just by looking at the pixel patterns.</p> </li> </ul> <p>The first part of the code loads this data and confirms its structure: there are 10 unique digits, 1797 sample images, and 64 features (pixels) for each image. The <code>plt.matshow</code> command then gives us a visual confirmation by reshaping one of the rows back into its original 8x8 image format.</p> <pre><code>## 1. Load and Inspect the Data\ndata, labels = load_digits(return_X_y=True)\n(n_samples, n_features), n_digits = data.shape, np.unique(labels).size\nprint(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")\n\n## 2. Visualize a Single Sample\ndigits = load_digits()\nplt.matshow(digits.images[0], cmap=\"gray\")\nplt.show()\n</code></pre>"},{"location":"modeling/modeling-without-target/clustering/#fitting-and-evaluating-the-clustering-model","title":"Fitting and Evaluating the Clustering Model","text":"<p>The next block of code executes our workflow: building a pipeline, fitting the model, and evaluating the results.</p>"},{"location":"modeling/modeling-without-target/clustering/#fitting-the-model","title":"Fitting the Model","text":"<p>We use <code>make_pipeline</code> to chain our steps. This ensures a robust and reproducible process.</p> <ol> <li><code>StandardScaler()</code>: This is a preprocessing step. It standardizes the pixel values in each feature (column) to have a mean of 0 and a standard deviation of 1. This helps the K-Means algorithm treat all pixels equally, regardless of their raw brightness values.</li> <li><code>KMeans(...)</code>: This is our clustering algorithm. We configure it with key hyperparameters:<ul> <li><code>init=\"k-means++\"</code>: Uses a smart method to initialize the cluster centers, leading to more reliable results.</li> <li><code>n_clusters=n_digits</code>: We tell the model to find 10 clusters, since we know there are 10 unique digits.</li> <li><code>n_init=4</code>: The algorithm will run 4 times with different random starting points and will automatically keep the best result.</li> <li><code>random_state=42</code>: This ensures our results are reproducible.</li> </ul> </li> </ol> <p>The <code>.fit(data)</code> command executes the entire pipeline on our image data.</p> <pre><code>## 3. Fit the Model\nestimator = make_pipeline(\n    StandardScaler(),\n    KMeans(\n        init=\"k-means++\",\n        n_clusters=n_digits,\n        n_init=4,\n        random_state=42),\n).fit(data)\n</code></pre>"},{"location":"modeling/modeling-without-target/clustering/#interpreting-the-evaluation-metrics","title":"Interpreting the Evaluation Metrics","text":"<p>Because we have the true labels for this dataset, we can use a rich set of metrics to evaluate how well our unsupervised algorithm rediscovered the underlying digit categories.</p> <ul> <li>Homogeneity: Measures if each cluster contains only images of a single digit. A score of 1.0 would mean every cluster is perfectly \"pure.\"</li> <li>Completeness: Measures if all images of a given digit are assigned to the same cluster. A score of 1.0 means no digit's images are split across multiple clusters.</li> <li>V-Measure: A balanced average of Homogeneity and Completeness.</li> <li>Adjusted Rand Index (ARI) and Adjusted Mutual Info (AMI): These measure the similarity between the true labels and the cluster assignments, with scores close to 1.0 indicating a strong agreement. They are \"adjusted\" to ensure that random assignments get a score near 0.</li> <li>Silhouette Coefficient: This is the only metric here that does not use the true labels. It measures how dense and well-separated the clusters are based only on the data's geometry. A score close to 1 indicates that the clusters are well-defined.</li> </ul> <p>The final block of code calculates these scores and presents them in a clean table, giving us a comprehensive view of our model's performance.</p> <pre><code>## 4. Score the Model\nmetrics={\n        \"Homogeneity\": homogeneity_score(labels, estimator[-1].labels_),\n        \"Completeness\": completeness_score(labels, estimator[-1].labels_),\n        \"V Measure\": v_measure_score(labels, estimator[-1].labels_),\n        \"Adj. Rand\": adjusted_rand_score(labels, estimator[-1].labels_),\n        \"Adj. Mutual Info\": adjusted_mutual_info_score(labels, estimator[-1].labels_),\n        \"Silhouette\": silhouette_score(\n            data,\n            estimator[-1].labels_,\n            metric=\"euclidean\",\n            sample_size=300,\n        )\n    }\n\npd.DataFrame(metrics.values(), columns=[\"Score\"], index=metrics.keys())\n</code></pre>"},{"location":"modeling/modeling-without-target/clustering/#exploring-other-clustering-algorithms","title":"Exploring Other Clustering Algorithms","text":"<p>KMeans is a powerful and popular algorithm, but it's just one of many tools available for clustering. As you continue your learning, we encourage you to explore other algorithms, each with different strengths and assumptions.</p> <p>Some notable algorithms available in <code>scikit-learn</code> include:</p> <ul> <li>Agglomerative Clustering: A \"bottom-up\" hierarchical approach that starts with each data point as its own cluster and progressively merges the closest pairs of clusters.</li> <li>DBSCAN: A density-based algorithm that is excellent at finding arbitrarily shaped clusters and identifying outliers. Unlike KMeans, it doesn't require you to pre-specify the number of clusters.</li> <li>BIRCH: An efficient algorithm designed specifically for very large datasets.</li> </ul>"},{"location":"modeling/modeling-without-target/clustering/#a-checklist-for-trying-new-algorithms","title":"A Checklist for Trying New Algorithms","text":"<p>When you experiment with a new algorithm, always follow a systematic process:</p> <ol> <li>Understand the Hyperparameters: Read the <code>scikit-learn</code> documentation to understand the algorithm's key hyperparameters. For <code>DBSCAN</code>, this would be <code>eps</code> and <code>min_samples</code>, not <code>n_clusters</code>.</li> <li>Apply Appropriate Preprocessing: Does the new algorithm have specific data requirements? For example, distance-based algorithms like DBSCAN are sensitive to feature scales, so using <code>StandardScaler</code> is crucial.</li> <li>Use Relevant Evaluation Metrics: Apply a range of metrics to understand performance. Use the Silhouette Coefficient to assess the quality of the clusters and, if you have ground truth labels, use metrics like the Adjusted Rand Index (ARI) to measure accuracy.</li> </ol>"},{"location":"modeling/modeling-without-target/dimensionality-reduction/","title":"Dimensionality Reduction","text":"<p>The second major type of unsupervised learning we will cover is dimensionality reduction. As its name suggests, this is the process of reducing the number of features (or dimensions) in a dataset while trying to preserve as much of the important information as possible.</p>"},{"location":"modeling/modeling-without-target/dimensionality-reduction/#motivation-why-reduce-dimensions","title":"Motivation: Why Reduce Dimensions?","text":"<p>You might wonder why we would intentionally discard features. There are several key motivations:</p> <ul> <li>The \"Curse of Dimensionality\": As the number of features grows, the amount of data required to support a robust model grows exponentially. Too many features can make models perform worse on unseen data, a phenomenon known as the curse of dimensionality.</li> <li>Visualization: Humans can't visualize data beyond three dimensions. To plot and visually explore a dataset with many features, we must first reduce it to two or three dimensions.</li> <li>Efficiency: Fewer features mean that models require less memory and can be trained much faster, which is a critical consideration in production environments.</li> </ul>"},{"location":"modeling/modeling-without-target/dimensionality-reduction/#principal-component-analysis-pca-the-intuition","title":"Principal Component Analysis (PCA): The Intuition","text":"<p>One of the most popular techniques for dimensionality reduction is Principal Component Analysis (PCA). In essence, PCA works by finding new, artificial axes for the data. Instead of using the original features (like <code>petalLength</code> or <code>sepalWidth</code>), it creates a set of new, combined features called principal components.</p> <p>These new components are designed to capture the maximum possible variance (or spread) in the data. The first principal component (PC1) is the single axis that captures the most variance. The second principal component (PC2) is the axis that captures the most remaining variance, and so on.</p> <p>By using only the first few principal components (e.g., PC1 and PC2), we can often represent a significant portion of the original dataset's information in a much lower-dimensional space, making it easier to visualize and model.</p>"},{"location":"modeling/modeling-without-target/dimensionality-reduction/#a-concrete-example-for-pca-and-latent-structure","title":"A Concrete Example for PCA and Latent Structure","text":"<p>Imagine a company that wants to understand customer satisfaction and conducts a detailed survey with 20 questions.</p>"},{"location":"modeling/modeling-without-target/dimensionality-reduction/#the-observed-features","title":"The Observed Features","text":"<p>The raw data consists of customer ratings (1-5) for questions like:</p> <ul> <li>Q1: How would you rate our product's price?</li> <li>Q2: Do you feel our product offers good value?</li> <li>Q3: Was our customer support agent helpful?</li> <li>Q4: Was your support issue resolved quickly?</li> <li>Q5: Do you trust our brand?</li> <li>Q6: Would you recommend us to a friend?</li> <li>...and 14 other questions.</li> </ul> <p>Building a model on all 20 features can be complex and inefficient, as many questions are related.</p>"},{"location":"modeling/modeling-without-target/dimensionality-reduction/#discovering-the-latent-structure-with-pca","title":"Discovering the Latent Structure with PCA","text":"<p>Instead of treating all 20 questions as independent, PCA can analyze the correlation patterns in the responses and discover the underlying, or latent, drivers of satisfaction.</p> <p>It might find that the responses are primarily driven by three \"meta-features\":</p> <ol> <li>Component 1 (Value Perception): A new, single feature that captures the combined essence of the questions about price and value (Q1, Q2, ...).</li> <li>Component 2 (Service Quality): A second feature that summarizes the questions about the support experience (Q3, Q4, ...).</li> <li>Component 3 (Brand Loyalty): A third feature that represents the themes of trust and recommendation (Q5, Q6, ...).</li> </ol>"},{"location":"modeling/modeling-without-target/dimensionality-reduction/#the-business-value","title":"The Business Value","text":"<p>Instead of working with 20 noisy and correlated features, the company can now use just these 3 powerful principal components. This allows them to build simpler, faster models and gain a clearer understanding of what truly drives customer satisfaction.</p>"},{"location":"modeling/modeling-without-target/dimensionality-reduction/#code-walkthrough-visualizing-high-dimensional-data-with-pca","title":"Code Walkthrough: Visualizing High-Dimensional Data with PCA","text":"<p>The Iris dataset has four features, making it impossible to visualize in its entirety on a single 2D plot. This code example demonstrates how to use Principal Component Analysis (PCA) to reduce the data from four dimensions to two, allowing us to create a meaningful visualization that still captures the essence of the original data.</p>"},{"location":"modeling/modeling-without-target/dimensionality-reduction/#1-setup-and-data-loading","title":"1. Setup and Data Loading","text":"<p>First, we import the necessary libraries. We need <code>matplotlib</code> for plotting, <code>PCA</code> from <code>sklearn.decomposition</code>, and the <code>load_iris</code> dataset itself.</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names\n</code></pre> <ul> <li><code>X</code> now holds our data with 4 features (the four measurements).</li> <li><code>y</code> holds the corresponding species label (0, 1, or 2).</li> </ul>"},{"location":"modeling/modeling-without-target/dimensionality-reduction/#2-applying-pca","title":"2. Applying PCA","text":"<p>Next, we instantiate and fit the <code>PCA</code> model. The most important hyperparameter here is <code>n_components=2</code>, where we explicitly tell PCA that we want to reduce the data to just two principal components. The <code>.fit(X)</code> command learns the optimal 2D projection from our 4D data, and <code>.transform(X)</code> then applies this projection to create our new, low-dimensional dataset <code>X_r</code>.</p> <pre><code># Instantiate and fit the PCA model\npca = PCA(n_components=2)\nX_r = pca.fit_transform(X)\n</code></pre> <p>After this step, <code>X_r</code> is a new array with the same number of rows as the original data, but now with only two columns representing PC1 and PC2.</p>"},{"location":"modeling/modeling-without-target/dimensionality-reduction/#3-analyzing-the-explained-variance","title":"3. Analyzing the Explained Variance","text":"<p>This is a crucial step to understand how much information our new components have retained. The <code>explained_variance_ratio_</code> attribute tells us the percentage of the original data's variance that is captured by each principal component.</p> <pre><code># Print the explained variance ratio\nprint(\n    \"explained variance ratio (first two components): %s\"\n    % str(pca.explained_variance_ratio_)\n)\n</code></pre> <p>The output will show that the first principal component (PC1) alone captures the vast majority of the variance (typically &gt;90%), and together, the first two components capture a very high percentage of the original information. This gives us confidence that our 2D plot will be a meaningful representation of the data.</p>"},{"location":"modeling/modeling-without-target/dimensionality-reduction/#4-creating-the-visualization","title":"4. Creating the Visualization","text":"<p>Finally, we create a scatter plot of our new 2D data (<code>X_r</code>). We loop through each class (<code>setosa</code>, <code>versicolor</code>, <code>virginica</code>), plotting the points for each with a different color and label.</p> <pre><code># Set up the plot\nplt.figure()\ncolors = [\"navy\", \"turquoise\", \"darkorange\"]\nlw = 2\n\n# Plot each class separately\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    plt.scatter(\n        X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=0.8, lw=lw, label=target_name\n    )\n\n# Add title and legend\nplt.legend(loc=\"best\", shadow=False, scatterpoints=1)\nplt.title(\"PCA of IRIS dataset\")\nplt.show()\n</code></pre> Applying Principle Components Analysis to Iris Dataset"},{"location":"modeling/modeling-without-target/dimensionality-reduction/#5-interpreting-the-result","title":"5. Interpreting the Result","text":"<p>The final plot shows a 2D \"shadow\" of the original 4D data. You can clearly see that the three species form distinct clusters. The <code>setosa</code> class is perfectly separated, while the <code>versicolor</code> and <code>virginica</code> classes are mostly separated, with some minor overlap. This visualization confirms that even after reducing the dimensionality by half, the fundamental structure required to distinguish between the species is preserved. This is the power of PCA in action.</p>"},{"location":"modeling/modeling-without-target/feature-engineering/","title":"Feature Engineering: Creating Better Inputs for Better Models","text":""},{"location":"modeling/modeling-without-target/feature-engineering/#introduction-to-feature-engineering","title":"Introduction to Feature Engineering","text":"<p>So far, we have largely used the features provided in our datasets as-is. However, one of the most impactful activities in the entire machine learning workflow is feature engineering\u2014the process of using domain knowledge and statistical techniques to create new features from existing ones. The quality of your features directly determines the maximum performance your model can achieve. Better features lead to better models.</p> <p>There are three primary categories of feature engineering:</p> <ol> <li> <p>Feature Transformation: This involves modifying existing features to make them more suitable for a model. This includes scaling numerical features or encoding categorical ones. You have already been doing this with tools like <code>StandardScaler</code> and <code>OneHotEncoder</code> inside your <code>Pipeline</code>. Other examples include creating polynomial features or applying mathematical transformations like logarithms.</p> </li> <li> <p>Feature Selection: This is the process of automatically selecting a subset of the most relevant features from your original dataset and discarding the rest. The goal is to reduce noise, improve model efficiency, and often, enhance predictive performance by focusing only on what's important.</p> </li> <li> <p>Feature Extraction: This involves creating entirely new features by combining or transforming existing ones. The goal is to capture the most important information in a more compact and powerful representation. You have already seen this with PCA, where we extracted new \"principal component\" features from the original pixel data.</p> </li> </ol> <p>A key takeaway for any data scientist is that supervised and unsupervised techniques are often used together to achieve analytical goals. We use unsupervised methods like PCA for feature extraction and supervised methods like feature selection to prepare the best possible inputs for our final predictive models.</p>"},{"location":"modeling/modeling-without-target/feature-engineering/#feature-selection-in-practice","title":"Feature Selection in Practice","text":"<p>Let's demonstrate how feature selection works. We will take the clean Iris dataset, which has 4 highly informative features, and intentionally add 20 \"noisy,\" irrelevant features. We will then use a feature selection technique to see if it can correctly identify and select the original 4 features, and we'll measure whether this selection improves our model's performance.</p>"},{"location":"modeling/modeling-without-target/feature-engineering/#code-walkthrough-selecting-the-best-features","title":"Code Walkthrough: Selecting the Best Features","text":"<p>1. Setup: Create a Noisy Dataset First, we load the Iris data and then create 20 columns of random noise. We use <code>np.hstack</code> to stack our 4 original features and the 20 new noisy features side-by-side, creating a dataset with 24 features in total.</p> <pre><code># The iris dataset\nX, y = load_iris(return_X_y=True)\nprint(f\"X shape before introducing noisy features: {X.shape}\")\n\n# Some noisy data not correlated\nE = np.random.RandomState(42).uniform(0, 0.1, size=(X.shape[0], 20))\n\n# Add the noisy data to the informative features\nX = np.hstack((X, E))\nprint(f\"X shape after introducing noisy features: {X.shape}\")\n\n# Split dataset to select features and evaluate the classifier\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=0\n)\n</code></pre> <p>2. Scoring Features with <code>SelectKBest</code> We will use <code>SelectKBest</code>, a <code>scikit-learn</code> utility that scores features using a statistical test (in this case, <code>f_classif</code>, which is an ANOVA F-test) and selects the 'k' features with the highest scores. We'll set <code>k=4</code>, hypothesizing that it can find our four original features.</p> <pre><code>selector = SelectKBest(f_classif, k=4)\nselector.fit(X_train, y_train)\nscores = -np.log10(selector.pvalues_)\nscores /= scores.max()\n\n# Plot the scores\nX_indices = np.arange(X.shape[-1])\nplt.figure(1)\nplt.clf()\nplt.bar(X_indices - 0.05, scores, width=0.2)\nplt.title(\"Feature univariate score\")\nplt.xlabel(\"Feature number\")\nplt.ylabel(r\"Univariate score ($-Log(p_{value})$)\")\nplt.show()\n</code></pre> <p>The bar chart clearly shows that the first four features have significantly higher scores than the 20 noisy features we added, confirming that our selection method is working as expected.</p> <p>3. Comparing Model Performance Now for the critical test: does removing the noisy features actually improve our model's performance? We will train two models: one on the full 24-feature dataset and another on a pipeline that includes our <code>SelectKBest</code> step.</p> <pre><code># Model 1: Using all 24 features\nclf = make_pipeline(MinMaxScaler(), LinearSVC())\nclf.fit(X_train, y_train)\nprint(\n    \"Classification accuracy without selecting features: {:.3f}\".format(\n        clf.score(X_test, y_test)\n    )\n)\n\n# Model 2: Pipeline with feature selection\nclf_selected = make_pipeline(SelectKBest(f_classif, k=4), MinMaxScaler(), LinearSVC())\nclf_selected.fit(X_train, y_train)\nprint(\n    \"Classification accuracy after univariate feature selection: {:.3f}\".format(\n        clf_selected.score(X_test, y_test)\n    )\n)\n</code></pre> <p>The output shows a clear improvement in classification accuracy after performing feature selection. By removing the irrelevant noise, we enabled the <code>LinearSVC</code> model to learn more effectively from the signals that truly matter.</p> <p>The final block of code visualizes a comparison of the feature scores, confirming that the selection process correctly identified the most important features for the model.</p> <pre><code># ... (visualization code from the prompt) ...\nplt.title(\"Comparing feature selection\")\nplt.xlabel(\"Feature number\")\nplt.yticks(())\nplt.axis(\"tight\")\nplt.legend(loc=\"upper right\")\nplt.show()\n</code></pre> <p>This example demonstrates that adding more features is not always better. Intelligent feature selection is a crucial step for building efficient and high-performing models.</p>"},{"location":"modeling/modeling-without-target/feature-engineering/#feature-extraction-with-pca","title":"Feature Extraction with PCA","text":"<p>We've seen how feature selection chooses the best existing features. Feature extraction, by contrast, creates entirely new features from the old ones. The goal is to capture the most important information from the original feature set in a smaller, more powerful set of new features.</p> <p>Principal Component Analysis (PCA) is a primary tool for this. By using the principal components as our new features, we can often reduce the complexity of our model, reduce noise, and even improve predictive performance.</p>"},{"location":"modeling/modeling-without-target/feature-engineering/#code-walkthrough-finding-the-optimal-number-of-components","title":"Code Walkthrough: Finding the Optimal Number of Components","text":"<p>This example demonstrates a complete workflow where we use <code>GridSearchCV</code> to find the optimal number of principal components to keep for a classification task on the handwritten digits dataset.</p> <p>1. Setting Up the Pipeline First, we define a three-step <code>Pipeline</code>. This will be the blueprint for our experiment.</p> <pre><code># Define a pipeline to search for the best combination of PCA truncation\n# and classifier regularization.\npca = PCA()\n# Define a Standard Scaler to normalize inputs\nscaler = StandardScaler()\n# set the tolerance to a large value to make the example faster\nlogistic = LogisticRegression(max_iter=10000, tol=0.1)\n\npipe = Pipeline(steps=[(\"scaler\", scaler), (\"pca\", pca), (\"logistic\", logistic)])\nX_digits, y_digits = datasets.load_digits(return_X_y=True)\n</code></pre> <ul> <li><code>scaler</code>: Our standard preprocessing step to scale the pixel data.</li> <li><code>pca</code>: The feature extraction step. We create a <code>PCA</code> object without specifying the number of components yet; this will be tuned by our grid search.</li> <li><code>logistic</code>: Our final supervised model, a <code>LogisticRegression</code> classifier.</li> </ul> <p>2. Defining the Search Space Next, we create a <code>param_grid</code> to tell <code>GridSearchCV</code> what to tune. This is where the synergy between the components happens.</p> <pre><code># Parameters of pipelines can be set using '__' separated parameter names:\nparam_grid = {\n    \"pca__n_components\": [5, 15, 30, 45, 60],\n    \"logistic__C\": np.logspace(-4, 4, 4),\n}\n</code></pre> <ul> <li><code>\"pca__n_components\"</code>: This is the key. We are telling the grid search to treat the number of principal components as a hyperparameter. It will build models using the top 5, 15, 30, 45, and 60 components.</li> <li><code>\"logistic__C\"</code>: We are also simultaneously tuning the regularization strength <code>C</code> of our final classifier.</li> </ul> <p>3. Running the Search and Analyzing Results Finally, we create a <code>GridSearchCV</code> object with our pipeline and parameter grid and fit it to the data.</p> <pre><code>search = GridSearchCV(pipe, param_grid, n_jobs=2)\nsearch.fit(X_digits, y_digits)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)\n</code></pre> <p>The grid search will now automatically test every combination. For example, it will train a model using 5 principal components and a <code>C</code> value of 0.0001, then a model with 5 components and a <code>C</code> of 0.01, and so on, for all possible pairs.</p> <p>The final output from <code>search.best_params_</code> will tell us the winning combination\u2014the optimal number of extracted features and the best classifier setting for this specific problem, demonstrating a powerful and automated approach to feature engineering.</p>"},{"location":"modeling/modeling-without-target/feature-engineering/#exploring-other-feature-extraction-methods","title":"Exploring Other Feature Extraction Methods","text":"<p>While PCA is a powerful and common technique, <code>scikit-learn</code> offers other methods for feature extraction that may be better suited for different types of problems.</p> <p>As you continue to learn, I encourage you to explore <code>FeatureAgglomeration</code>. This technique works by applying hierarchical clustering to the features themselves, progressively grouping together features that are most similar.</p> <ul> <li>Typical Use Case: <code>FeatureAgglomeration</code> is particularly useful when you have many features that are redundant or measure similar underlying concepts. By grouping them, you can create a simplified set of \"meta-features\" that are easier for a model to interpret, which can be especially effective in domains like bioinformatics or text analysis where you might have thousands of highly correlated features.</li> </ul>"},{"location":"modeling/modeling-without-target/informed-initialization/","title":"Clustering + Dimensionality Reduction","text":"<p>This section demonstrates a useful technique that combines dimensionality reduction (PCA) and clustering (KMeans) to solve a common problem in clustering: finding good starting points.</p> <pre><code>from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nkmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=4, random_state=0)\n\nkmeans = KMeans(init=\"random\", n_clusters=n_digits, n_init=4, random_state=0)\n\npca = PCA(n_components=n_digits).fit(data)\nkmeans = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)\n</code></pre>"},{"location":"modeling/modeling-without-target/informed-initialization/#the-problem-where-to-start","title":"The Problem: Where to Start?","text":"<p>The KMeans algorithm works by iteratively refining the position of its cluster centers (centroids). However, its final performance can be very sensitive to where those centroids are placed initially.</p> <ul> <li>Random Initialization (<code>init=\"random\"</code>): This is the simplest approach. It places centroids randomly. If it gets an unlucky start (e.g., placing two centroids very close together in the middle of a single cluster), the final result can be poor. That's why it needs to be run multiple times (<code>n_init=4</code>) to hope for a good outcome.</li> <li>K-Means++ Initialization (<code>init=\"k-means++\"</code>): This is a \"smarter\" random start. It tries to place the initial centroids far away from each other, which usually leads to much better and more consistent results than the purely random method. It is the default and recommended starting point.</li> </ul>"},{"location":"modeling/modeling-without-target/informed-initialization/#the-solution-using-pca-as-an-intelligent-guide","title":"The Solution: Using PCA as an Intelligent Guide","text":"<p>The third method, PCA-based initialization, offers an even more informed starting strategy. Here\u2019s the thinking behind it:</p> <ol> <li> <p>What does PCA do? As we've learned, PCA finds the directions of maximum variance in the data. These \"principal components\" are the axes along which the data is most spread out. These directions represent the fundamental structure of the dataset.</p> </li> <li> <p>How does this help KMeans? The directions where the data is most spread out are also the directions where clusters are most likely to be separated. Instead of starting with random points, we can give KMeans a powerful hint.</p> </li> <li> <p>The Code in Action:</p> <ul> <li><code>pca = PCA(n_components=n_digits).fit(data)</code>: First, we run PCA to find the 10 most important \"directions\" in our 64-dimensional data.</li> <li><code>kmeans = KMeans(init=pca.components_, ...)</code>: This is the key step. We are telling KMeans: \"Don't start with random points. Instead, use the 10 principal components you found as the initial locations for the 10 cluster centroids.\"</li> </ul> </li> </ol> <p>By initializing the centroids along the primary axes of variation, we are seeding the algorithm with strong prior information about the data's structure. This gives it a significant head start, often leading to a stable and high-quality result without needing multiple random restarts (which is why <code>n_init</code> is set to <code>1</code>).</p> <p>This example is a perfect demonstration of how unsupervised techniques can be used in conjunction to create a more robust and intelligent modeling process.</p> <p>While feature engineering typically focuses on transforming the input data (<code>X</code>), a more advanced strategy involves using data insights to transform the model itself.</p> <p>The \"PCA-based\" KMeans initialization described here is a perfect example of this. Here we didn't change the data the KMeans algorithm processed; instead, we used a feature extraction method (PCA) to find the data's underlying structure and passed that information to the model's <code>init</code> hyperparameter.</p> <p>Think of this as \"feature engineering for the model's starting state.\" It's a powerful demonstration of how insights from one algorithm can be used to make another algorithm more effective, showcasing the deep synergy that is possible in a machine learning workflow.</p>"},{"location":"modeling/modeling-without-target/informed-initialization/#learning-activity-evaluating-initialization-strategies","title":"Learning Activity: Evaluating Initialization Strategies","text":"<p>See these concepts in action. Run the complete code block in your accompanying Jupyter notebook. Once the code finishes, you will see an output table comparing the three different initialization strategies.</p>"},{"location":"modeling/modeling-without-target/informed-initialization/#analyze-the-results","title":"Analyze the Results","text":"<p>Examine the output table closely and answer the following questions:</p> <ol> <li> <p>Performance Metrics: Compare the scores for <code>k-means++</code> and <code>PCA-based</code> initialization across the different metrics (e.g., ARI, AMI, Silhouette). Which method achieved a better clustering performance on this dataset?</p> </li> <li> <p>Execution Time: Look at the <code>time</code> column. How does the execution time of the <code>PCA-based</code> method compare to the others?</p> </li> <li> <p>Synthesis: Based on your observations, what is the trade-off between using the <code>k-means++</code> and the <code>PCA-based</code> initialization methods for this particular problem? When might you choose one over the other?</p> </li> </ol>"},{"location":"modeling/modeling-without-target/next-steps/","title":"Next Steps and Active Learning","text":""},{"location":"modeling/modeling-without-target/next-steps/#50-next-steps-and-module-summary","title":"5.0 Next Steps and Module Summary","text":"<p>This section provides recommended exercises to practice the concepts of unsupervised learning and concludes with a summary of the entire modeling module.</p>"},{"location":"modeling/modeling-without-target/next-steps/#homework-and-active-learning","title":"Homework and Active Learning","text":"<ol> <li> <p>Experiment with a Different Clustering Algorithm: <code>KMeans</code> is excellent for spherical clusters, but other algorithms have different strengths.</p> <ul> <li>Task: Use the <code>make_moons</code> dataset from <code>sklearn.datasets</code>. Apply both <code>KMeans</code> and another algorithm called <code>DBSCAN</code> to it. Visualize the results from both.</li> <li>Analysis: How does <code>DBSCAN</code>'s performance on this non-spherical data compare to <code>KMeans</code>?</li> </ul> </li> <li> <p>Explore an Alternative for Visualization: While PCA is great, t-SNE is another powerful technique specifically designed for visualizing high-dimensional data.</p> <ul> <li>Task: Import <code>TSNE</code> from <code>sklearn.manifold</code>. Apply it to the <code>digits</code> dataset (reducing it to 2 components) and create a scatter plot, just as you did with PCA.</li> <li>Analysis: Compare the t-SNE visualization to the PCA visualization. Does t-SNE create more distinct visual clusters for the different digits?</li> </ul> </li> <li> <p>Feature Engineering with PCA: In our lab, we used clustering to create a feature. Now, apply PCA for the same purpose.</p> <ul> <li>Task: Take all the numerical <code>sqft_</code> features from the King County housing dataset (<code>sqft_living</code>, <code>sqft_lot</code>, <code>sqft_above</code>, etc.). Use a <code>Pipeline</code> to scale them and then apply <code>PCA</code> to reduce them to just two \"size components.\" Add these two new features to your dataset and train a regression model.</li> <li>Analysis: Does your model's performance improve compared to using the original <code>sqft_</code> features?</li> </ul> </li> <li> <p>Fine-Tune Your Clustering Model:</p> <ul> <li>Task: Apply the Elbow Method to the Iris dataset's features to programmatically determine the optimal number of clusters.</li> <li>Analysis: Does the result from the Elbow Method correctly suggest <code>k=3</code>, which we know to be the true number of species?</li> </ul> </li> </ol>"},{"location":"modeling/modeling-without-target/next-steps/#lesson-and-module-summary","title":"Lesson and Module Summary","text":"<p>In this lesson, you explored unsupervised learning, discovering how to find inherent structure in data without a target variable. You learned to use K-Means for clustering to identify meaningful segments and PCA for dimensionality reduction to simplify and visualize complex data. Most importantly, you saw how these powerful techniques can be used to engineer better features for your supervised models.</p> <p>This concludes our module on modeling. You have progressed from understanding the basic supervised learning workflow in Lesson 1 to building robust, automated pipelines with hyperparameter tuning in Lesson 2. Now, in Lesson 3, you've added unsupervised learning to your toolkit. You now possess a comprehensive framework for building, evaluating, and improving machine learning models to solve a wide array of data science problems.</p>"},{"location":"modeling/modeling-without-target/tuning-feature-engineering/","title":"Fine-tuning Feature Engineering","text":""},{"location":"modeling/modeling-without-target/tuning-feature-engineering/#the-goal-finding-the-best-feature-engineering-strategy-automatically","title":"The Goal: Finding the Best Feature Engineering Strategy Automatically","text":"<p>So far, we have explored feature selection and feature extraction (PCA) as separate techniques. But in a real-world project, a critical question arises: \"For my specific problem, which feature engineering strategy is best?\"</p> <p>Is it better to select the best original features, or is it better to extract entirely new features? And what is the optimal number of features to use?</p> <p>The code below answers this question systematically. It builds a single, powerful <code>GridSearchCV</code> experiment that creates a \"competition\" between three different feature engineering methods (<code>PCA</code>, <code>NMF</code>, and <code>SelectKBest</code>) to find the single best combination of data reduction technique and model hyperparameters. This complexity is the key to automating what would otherwise be a very tedious, manual process of building and comparing separate models.</p> Non-Negative Matrix Factorization (NMF): An Alternative to PCA <p>While PCA is a powerful and general-purpose tool for dimensionality reduction, it is not the only technique available. In our upcoming example, you will see another method called Non-Negative Matrix Factorization (NMF).</p> <p>NMF is a dimensionality reduction algorithm that, like PCA, aims to find a new, more compact representation of the data. However, it operates under a significant constraint: it does not allow for any negative values in the resulting components. This makes it particularly useful for datasets where the features can only be added together meaningfully, such as pixel intensities in an image or word counts in a text document. Think of NMF as trying to explain the data as a sum of its parts.</p>"},{"location":"modeling/modeling-without-target/tuning-feature-engineering/#pca-vs-nmf-a-brief-comparison","title":"PCA vs. NMF: A Brief Comparison","text":"Feature Principal Component Analysis (PCA) Non-Negative Matrix Factorization (NMF) Core Goal Finds directions of maximum variance in the data. Decomposes the data into a sum of non-negative parts. Component Values Components can have both positive and negative values, which can sometimes be hard to interpret. Components are strictly non-negative (zero or positive). Best For General-purpose dimensionality reduction, especially for data centered around a mean. Datasets where features are counts or represent parts of a whole (e.g., text analysis, image processing, audio signal processing). Interpretation The first component is the most important, followed by the second, and so on. All components can be equally important, representing different additive parts of the original data."},{"location":"modeling/modeling-without-target/tuning-feature-engineering/#code-walkthrough-a-competition-pipeline","title":"Code Walkthrough: A Competition Pipeline","text":"<p>Let's dissect the code piece by piece.</p>"},{"location":"modeling/modeling-without-target/tuning-feature-engineering/#1-the-pipeline-blueprint","title":"1. The Pipeline Blueprint","text":"<p>First, we define a <code>Pipeline</code> that acts as a template for our experiment. Notice the crucial second step.</p> <pre><code>pipe = Pipeline(\n    [\n        (\"scaling\", MinMaxScaler()),\n        # the reduce_dim stage is populated by the param_grid\n        (\"reduce_dim\", \"passthrough\"),\n        (\"classify\", LinearSVC(dual=False, max_iter=10000)),\n    ]\n)\n</code></pre> <ul> <li><code>(\"scaling\", MinMaxScaler())</code>: Our standard first step to scale the data.</li> <li><code>(\"reduce_dim\", \"passthrough\")</code>: This is the key. We are creating a placeholder step named <code>reduce_dim</code>. The <code>\"passthrough\"</code> value tells the pipeline to do nothing at this stage by default. This placeholder will be dynamically replaced by our feature engineering objects (<code>PCA</code>, <code>NMF</code>, <code>SelectKBest</code>) during the grid search.</li> <li><code>(\"classify\", LinearSVC(...))</code>: Our final classification model.</li> </ul>"},{"location":"modeling/modeling-without-target/tuning-feature-engineering/#2-the-parameter-grid-defining-the-competition","title":"2. The Parameter Grid: Defining the Competition","text":"<p>This is the most complex part. Instead of a single dictionary, <code>param_grid</code> is a list of dictionaries. <code>GridSearchCV</code> will run a completely separate search for each dictionary in the list.</p> <p>Competition Bracket 1: Feature Extraction (<code>PCA</code> vs. <code>NMF</code>) The first dictionary defines the competition between our two feature extraction methods.</p> <pre><code>param_grid = [\n    {\n        \"reduce_dim\": [PCA(iterated_power=7), NMF(max_iter=1_000)],\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n# ...\n]\n</code></pre> <ul> <li><code>\"reduce_dim\": [PCA(...), NMF(...)]</code>: This tells <code>GridSearchCV</code>: \"For the <code>reduce_dim</code> step in the pipeline, first try using <code>PCA</code>, and then try using <code>NMF</code>.\"</li> <li><code>\"reduce_dim__n_components\": [2, 4, 8]</code>: This uses the double-underscore (<code>__</code>) syntax to set a hyperparameter on the object inside the <code>reduce_dim</code> step. It says: \"For whichever method you are trying (<code>PCA</code> or <code>NMF</code>), test it with 2, 4, and 8 components.\"</li> <li><code>\"classify__C\": [1, 10, 100, 1000]</code>: This sets the <code>C</code> hyperparameter for our final <code>LinearSVC</code> classifier.</li> </ul> <p>Competition Bracket 2: Feature Selection (<code>SelectKBest</code>) The second dictionary defines the experiment for our feature selection method.</p> <pre><code># ...\n    {\n        \"reduce_dim\": [SelectKBest(mutual_info_classif)],\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n]\n</code></pre> <ul> <li><code>\"reduce_dim\": [SelectKBest(...)]</code>: This tells <code>GridSearchCV</code>: \"Now, for the <code>reduce_dim</code> step, try using <code>SelectKBest</code>.\"</li> <li><code>\"reduce_dim__k\": [2, 4, 8]</code>: This targets the hyperparameter for <code>SelectKBest</code>, telling it to try selecting the top 2, 4, and 8 features.</li> <li><code>\"classify__C\": [1, 10, 100, 1000]</code>: Again, this tunes the final classifier.</li> </ul>"},{"location":"modeling/modeling-without-target/tuning-feature-engineering/#3-running-the-search","title":"3. Running the Search","text":"<p>Finally, we put it all together. <code>GridSearchCV</code> will now exhaustively test every possible combination defined in the <code>param_grid</code>. It will try PCA with 2, 4, and 8 components; NMF with 2, 4, and 8 components; and SelectKBest with 2, 4, and 8 features, each paired with all four <code>C</code> values for the classifier.</p> <pre><code>grid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid)\ngrid.fit(X, y)\n</code></pre> <p>After the search is complete, you can inspect <code>grid.best_estimator_</code> and <code>grid.best_params_</code> to see which combination\u2014which feature engineering strategy, which number of features, and which classifier setting\u2014emerged as the ultimate winner of this automated competition. This is the power and purpose of this complex-seeming setup: it provides a robust, automated framework for making optimal feature engineering decisions.</p>"},{"location":"modeling/modeling-without-target/unsupervised-learning/","title":"Introduction to Unsupervised Learning","text":""},{"location":"modeling/modeling-without-target/unsupervised-learning/#uncovering-structure-with-unsupervised-learning","title":"Uncovering Structure with Unsupervised Learning","text":"<p>In our previous lessons, we focused on supervised learning, where our primary goal was to predict a well-defined target variable (<code>y</code>). We had an \"answer key\" in our historical data, which allowed us to train and evaluate our models' ability to predict outcomes like house prices or iris species.</p> <p>However, many business problems do not come with a clear-cut target variable. We often have large amounts of data and a general goal to \"find something interesting\" or \"understand our customers better.\" This is where unsupervised learning comes in.</p> <p>Unsupervised learning is a class of machine learning techniques used to find patterns, structures, and relationships in data that has not been labeled with a target outcome. Instead of predicting a known answer, the goal is to discover the inherent structure within the data itself.</p> <p>In this lesson, we will explore the two most common types of unsupervised learning:</p> <ol> <li>Clustering: The task of automatically grouping similar data points together. This is widely used for applications like customer segmentation, where the goal is to discover distinct groups of customers based on their behavior or demographics.</li> <li>Dimensionality Reduction: The process of reducing the number of features in a dataset while retaining as much of the important information as possible. This is useful for simplifying models, improving performance, and enabling visualization of high-dimensional data.</li> </ol> <p>Most importantly, we will see how these techniques are not just for descriptive analysis; they are powerful tools that can be used to enhance modeling workflows by creating better, more informative features. </p>"},{"location":"modeling/robust-modeling-workflow/","title":"Production-grade Workflow","text":"<ul> <li> Starter Colab Notebook</li> <li> Production-grade Workflow </li> <li> Preprocessing Pipelines </li> <li> Cross Validation </li> <li> Our First Pipeline </li> <li> Automated Hyperparameter Tuning </li> <li> Tuning Ensemble Models </li> <li> Next Steps </li> </ul>"},{"location":"modeling/robust-modeling-workflow/cross-validation/","title":"What is Cross-Validation? \ud83e\udd14","text":"<p>In machine learning, you build a model to make predictions on new, unseen data. But how do you know how well your model will perform before you deploy it? You need to test it.</p> <p>The simplest way is to split your data into two sets: one for training the model and one for testing it. This is a good start, but what if you were just unlucky with your split? Maybe the test set accidentally contained all the \"easy\" examples, making your model look better than it is. Or maybe it contained all the \"hard\" ones, making it look worse.</p> <p>Cross-validation (CV) solves this problem. It's a technique where you systematically split the data into multiple training and testing sets to get a more reliable estimate of your model's performance. Instead of just one test score, you get several, which you can then average to get a more accurate and stable measure of how your model will generalize to new data.</p> <sup>Image Credit: https://scikit-learn.org/ </sup> Cross validation workflow."},{"location":"modeling/robust-modeling-workflow/cross-validation/#the-basic-split-train_test_split","title":"The Basic Split: <code>train_test_split</code>","text":"<p>This is the technique we previously introduced in our six-step workflow. This is just a starting point. While not technically a cross-validation method, it's the fundamental building block.</p> <ul> <li>How it works: It shuffles your dataset and splits it into two parts: a training set and a testing set. You decide the proportions, like 80% for training and 20% for testing.</li> <li>When to use it: Use this for a quick and simple model evaluation, especially with large datasets where the computational cost of running full cross-validation might be too high. It's a fast and easy way to get a first impression of your model.</li> </ul> <p>Limitation: It only gives you one performance score. That score can be misleading if the random split isn't representative of the overall dataset.</p> <pre><code>from sklearn.model_selection import train_test_split\n\n# X is your features, y is your target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/cross-validation/#cross-validation-techniques-in-scikit-learn","title":"Cross-Validation Techniques in Scikit-Learn","text":"<p>Now let's get into the actual cross-validation methods. These are more robust.</p>"},{"location":"modeling/robust-modeling-workflow/cross-validation/#k-fold","title":"K-Fold","text":"<p>This is the most common and straightforward cross-validation technique.</p> <ul> <li> <p>How it works:</p> <ol> <li>It splits the dataset into 'k' equal-sized parts, or \"folds\" (e.g., k=5 or k=10).</li> <li>It uses one fold as the test set and the remaining k-1 folds as the training set.</li> <li>It repeats this process 'k' times, with each fold getting a turn to be the test set.</li> <li>You end up with 'k' different performance scores, which you can average to get a final, more reliable score.</li> </ol> <p> <sup>Image Credit: https://scikit-learn.org/ </sup> Cross validation using k-folds of training data.  </p> </li> <li> <p>When to use it: K-Fold is a great default choice for cross-validation when you're working with a regression problem (predicting a number) or a balanced classification problem (where each class has a similar number of samples).</p> </li> </ul>"},{"location":"modeling/robust-modeling-workflow/cross-validation/#stratified-k-fold","title":"Stratified K-Fold","text":"<p>This is an improved version of K-Fold specifically for classification problems where the classes are imbalanced.</p> <ul> <li>How it works: It does the same thing as K-Fold but with one crucial difference: when making the folds, it ensures that each fold has roughly the same percentage of samples from each class as the original dataset. For example, if your dataset is 80% Class A and 20% Class B, Stratified K-Fold makes sure every fold has that same 80/20 split.</li> <li> <p>Why that matters: This prevents a situation where, by pure chance, one of your folds ends up with very few or zero samples from a minority class, which would make the model's evaluation for that fold meaningless.</p> </li> <li> <p>When to use it: Always use Stratified K-Fold for classification problems, especially when your classes are imbalanced. It's generally a safer and more reliable choice than standard K-Fold for classification.</p> </li> </ul>"},{"location":"modeling/robust-modeling-workflow/cross-validation/#group-k-fold","title":"Group K-Fold","text":"<p>This is a specialized version for when your data has \"groups\" or \"clusters.\"</p> <ul> <li>How it works: In some datasets, data points are not independent. For example, you might have multiple medical readings from the same patient, or multiple product reviews from the same user. If you use regular K-Fold, you might end up with data from the same patient in both the training and testing sets. This is bad because the model might learn to recognize the specific patient rather than the general medical condition, a form of \"data leakage.\"     Group K-Fold ensures that all the data from a single group (e.g., one patient) will be in either the training set or the test set, but never both.</li> <li>When to use it: Use Group K-Fold when you have groups of related or non-independent data points. Common examples include repeated measurements on the same subject, medical data from multiple patients, or reviews from the same users.</li> </ul>"},{"location":"modeling/robust-modeling-workflow/cross-validation/#stratified-group-k-fold","title":"Stratified Group K-Fold","text":"<p>As the name suggests, this method combines the features of Stratified K-Fold and Group K-Fold.</p> <ul> <li>How it works: It keeps groups intact (like Group K-Fold) while also preserving the class balance within each fold as best as possible (like Stratified K-Fold).</li> <li>When to use it: Use Stratified Group K-Fold when you have both grouped data and an imbalanced class distribution. This is common in medical diagnoses (multiple patients with an imbalanced disease rate) or user-based classification problems where the outcome is imbalanced.</li> </ul>"},{"location":"modeling/robust-modeling-workflow/cross-validation/#putting-it-into-practice","title":"Putting It Into Practice \u26a1","text":"<p>While it's important to understand how <code>KFold</code> and <code>StratifiedKFold</code> work under the hood, you will rarely instantiate them manually in your day-to-day work.</p> <p>Scikit-learn has convenient high-level functions that perform the entire cross-validation loop for you. The best part? They automatically choose the right kind of cross-validation for you!</p> <ul> <li>If you give them a classifier, they will use <code>StratifiedKFold</code> by default.</li> <li>If you give them a regressor, they will use <code>KFold</code> by default.</li> </ul> <p>This helps prevent common mistakes and makes your code much cleaner.</p>"},{"location":"modeling/robust-modeling-workflow/cross-validation/#the-easy-way-cross_val_score","title":"The Easy Way: <code>cross_val_score</code>","text":"<p>Let's look at the most common way to run a cross-validation using <code>cross_val_score()</code>. This function fits and evaluates a model on all the folds and returns the list of scores.</p> <p>Notice how much simpler the code is. By just setting <code>cv=5</code>, we are telling scikit-learn to perform a 5-fold cross-validation. Because we are using a classifier (<code>LogisticRegression</code>), it automatically uses stratification.</p> <pre><code>import numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\n\n# --- 1. Setup: Create a sample imbalanced dataset and a model ---\n# X = features, y = target labels\nX, y = make_classification(n_samples=100, n_features=20, \n                           n_informative=2, n_redundant=10, \n                           weights=[0.9, 0.1], # Imbalanced classes (90% vs 10%)\n                           flip_y=0, random_state=42)\n\n# Create a logistic regression model\nmodel = LogisticRegression()\n\n\n# --- 2. The Easy Way: Let scikit-learn do the work ---\n# By passing cv=5 to a classifier, scikit-learn automatically uses StratifiedKFold\n# to ensure class balance is preserved in each fold.\nscores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n\n\n# --- 3. Review the results ---\nprint(f\"Scores for each of the 5 folds: {np.round(scores, 2)}\")\nprint(f\"Average CV Accuracy: {scores.mean():.2f} (+/- {scores.std():.2f})\")\n\n# Expected Output:\n# Scores for each of the 5 folds: [0.9  0.9  0.9  0.95 0.85]\n# Average CV Accuracy: 0.90 (+/- 0.03)\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/cross-validation/#when-would-you-manually-create-a-kfold-object","title":"When Would You Manually Create a <code>KFold</code> Object?","text":"<p>You only need to manually create a cross-validation object when you need specific control that the default behavior doesn't offer. For example:</p> <ol> <li>Forcing a shuffle: To shuffle the data before splitting, you need to create an object. This is good practice.</li> <li>Reproducibility: To get the same folds every time you run your code, you must set a <code>random_state</code>.</li> <li>Using Groups: When you need to use <code>GroupKFold</code> or <code>StratifiedGroupKFold</code>, you must instantiate them and pass the object to the <code>cv</code> parameter.</li> </ol> <p>Here is how you would do that:</p> <pre><code>from sklearn.model_selection import StratifiedKFold, cross_val_score\n\n# I want to control the shuffling and ensure my results are reproducible\n# so I create a StratifiedKFold object.\ncustom_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Now, I pass this object to the cv parameter\nscores = cross_val_score(model, X, y, cv=custom_cv)\n\nprint(f\"Scores from custom CV object: {np.round(scores, 2)}\")\nprint(f\"Average CV Accuracy: {scores.mean():.2f}\")\n</code></pre> <p>In summary: Start by simply using an integer for the <code>cv</code> parameter (e.g., <code>cv=5</code>). Only create a full <code>KFold</code> or <code>StratifiedKFold</code> object when you have a specific reason to, like needing to set a <code>random_state</code> or use groups.</p>"},{"location":"modeling/robust-modeling-workflow/cross-validation/#summary-which-one-should-i-use","title":"Summary: Which One Should I Use?","text":"Method When to Use It Key Idea <code>train_test_split</code> For a quick, preliminary check, especially with very large data. A single, simple split. <code>KFold</code> The go-to for regression problems. Also fine for balanced classification. Splits data into 'k' folds, rotating which fold is the test set. <code>StratifiedKFold</code> The default choice for classification problems, especially with imbalanced classes. \ud83c\udfc6 Same as K-Fold, but keeps class proportions the same in each fold. <code>GroupKFold</code> When your data has groups (e.g., multiple samples from the same patient or user). Ensures that all data from a group is in either training or testing, not both. <code>StratifiedGroupKFold</code> When you have both grouped data and imbalanced classes. A combination of stratified and group logic."},{"location":"modeling/robust-modeling-workflow/first-pipeline/","title":"Building a Regression Pipeline","text":"<p>In this section, we'll apply the concepts of preprocessing and pipelines to a regression problem. Our goal is to build a basic model that predicts house prices from the King County dataset. This exercise will serve as the foundation for the more advanced tuning and evaluation techniques we'll cover next.</p> <p>First, ensure you have the <code>kc_house_data.csv</code> file available in your environment.</p>"},{"location":"modeling/robust-modeling-workflow/first-pipeline/#setup-importing-libraries-and-loading-data","title":"Setup: Importing Libraries and Loading Data","text":"<p>We begin by importing the necessary libraries and loading our dataset into a Polars DataFrame.</p> Truncated code listing <p>The code shown in this page are to help your understanding of your concepts  and workflows being discussed in this specific lesson. Please see the  code demo notebook file for a fully functional code.</p> <pre><code>import polars as pl\nfrom sklearn.model_selection import train_test_split\n# pipeline meta estimator\nfrom sklearn.pipeline import (\n    Pipeline,\n    make_pipeline,\n)\n# transformers\nfrom sklearn.preprocessing import (\n    StandardScaler, \n    OneHotEncoder,\n    FunctionTransformer,\n)\n# models\nfrom sklearn.tree import DecisionTreeRegressor\n#metrics\nfrom sklearn.metrics import mean_absolute_error\n\n# Load the dataset\ndf = pl.read_csv(\"kc_house_data.csv\")\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/first-pipeline/#step-1-feature-selection-and-data-preparation","title":"Step 1: Feature Selection and Data Preparation","text":"<p>For this initial model, we will select a small subset of numerical features that we hypothesize will be predictive of the house price. We will define our feature matrix <code>X</code> and our target vector <code>y</code>.</p> <pre><code># Select a subset of features for our initial model\nnumeric_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n                    'floors', 'waterfront', 'view', 'condition', 'grade',\n                    'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', \n                    'lat', 'long', 'sqft_living15', 'sqft_lot15', ]\n\ncategorical_features = [\n    # 'zipcode', \n    'school_district',\n    ]\ntarget = \"price\"\n\nX = df\ny = df.select(target)\n\n# Display the shape of our feature matrix\nprint(f\"Shape of X: {X.shape}\")\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/first-pipeline/#step-2-create-the-preprocessing-pipeline","title":"Step 2: Create the Preprocessing Pipeline","text":"<p>Different feature subsets often require unique preprocessing. We can approach this systematically with the following steps:</p> <ul> <li>Identify Subsets &amp; Steps: First, identify the different feature types (e.g., numerical, categorical) and the specific preprocessing each requires. Note that these steps are often order-sensitive; for example, missing value imputation should typically occur before scaling or encoding.</li> <li>Build Feature Pipelines: For each feature subset, create a dedicated pipeline that performs its required sequence of transformations.</li> <li>Combine into a Preprocessor: Combine the individual feature pipelines into a single, comprehensive preprocessor. This composite transformer will apply the correct steps to the correct columns of your dataset.</li> <li>Create a Final Model Pipeline: Finally, create the full pipeline by adding your chosen machine learning model (the estimator) as the last step. This final object encapsulates the entire workflow, from data preprocessing to model training.</li> </ul> <pre><code># function that perform data cleaning step\ndef tweak_housing(df):\n    pass\n\ntweak_transformer = FunctionTransformer(tweak_housing)\n\n# Create the transformer pipeline for a subset of features\nnumeric_transformer = make_pipeline(\n    SimpleImputer(strategy='median'),\n    StandardScaler(),\n)\n\n# Note the alternative syntax for creating a pipeline\ncategorical_transformer = Pipeline(\n    steps=[\n         ('onehot', OneHotEncoder(handle_unknown='ignore',\n                              sparse_output=False)),\n    # ('target', TargetEncoder()),\n    # ('std', StandardScaler()),\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat',categorical_transformer, categorical_features),\n        ],\n        # remainder='passthrough',\n    )\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/first-pipeline/#step-3-split-train-and-predict","title":"Step 3: Split, Train, and Predict","text":"<p>We will now execute the familiar workflow: split the data into training and testing sets, then fit our entire pipeline on the training data. The <code>Pipeline</code> object handles passing the data through each step correctly. Finally, we'll make predictions on the test set.</p> <pre><code># Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\npipe = Pipeline(steps=[\n    ('tweak', tweak_transformer),\n    ('preprocessor', preprocessor),\n    ('dt', DecisionTreeRegressor()),\n    ])\n# Fit the entire pipeline on the training data\npipe.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = pipe.predict(X_test)\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/first-pipeline/#step-4-evaluate-the-model","title":"Step 4: Evaluate the Model","text":"<p>Since this is a regression task, we can't use \"accuracy.\" We will use the Mean Absolute Error (MAE), which measures the average absolute difference between our model's predictions and the actual house prices. This gives us an error value in the same unit as our target (in this case, US dollars).</p> <pre><code># Calculate Mean Absolute Error\nmae = mean_absolute_error(y_test, predictions)\n\nprint(f\"Mean Absolute Error of the model: ${mae:,.2f}\")\n</code></pre> <p>This MAE value serves as our baseline. In the following sections, we will learn techniques like cross-validation and hyperparameter tuning to try and improve upon this initial result.</p>"},{"location":"modeling/robust-modeling-workflow/first-pipeline/#improving-performance-hyperparameter-tuning","title":"Improving Performance: Hyperparameter Tuning","text":"<p>Our initial pipeline provides a performance baseline, but it's unlikely to be optimal. The <code>DecisionTreeRegressor</code> we used was created with its default settings. To improve our model, we need to find the best configuration for our specific problem. This is accomplished through hyperparameter tuning.</p> <p>It's crucial to distinguish between parameters and hyperparameters:</p> <ul> <li>Parameters are values the model learns from the data during the <code>.fit()</code> process. For a Decision Tree, these are the questions it learns to ask at each split (e.g., \"is <code>sqft_living</code> &gt; 2000?\"). You don't set these yourself.</li> <li>Hyperparameters are settings you, the data scientist, choose before training the model. They are passed as arguments when you instantiate the model. For our <code>DecisionTreeRegressor</code>, examples include <code>max_depth</code> (the deepest the tree can go) or <code>min_samples_leaf</code> (the minimum number of data points required to be at a leaf node).</li> </ul> <p>The process of finding the optimal combination of these settings is called tuning. Manually testing different combinations is tedious and inefficient. In the next section, we'll introduce a more robust and automated approach to both evaluate and tune our models.</p>"},{"location":"modeling/robust-modeling-workflow/grid-search/","title":"Automated Hyperparameter Tuning","text":"<p>This lab demonstrates the complete, robust workflow. We will use <code>GridSearchCV</code> to automatically search for the best hyperparameters for our pipeline, using cross-validation on our training data. We will then perform a final evaluation on the held-out test set.</p>"},{"location":"modeling/robust-modeling-workflow/grid-search/#setup-imports-and-data-splits","title":"Setup: Imports and Data Splits","text":"<p>First, we'll import <code>GridSearchCV</code> and reuse the <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code> sets from our previous lab.</p> <pre><code>from sklearn.model_selection import GridSearchCV\n\n# We assume X_train, X_test, y_train, y_test are already created\n# from the previous lab's train_test_split.\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/grid-search/#step-1-define-the-parameter-grid","title":"Step 1: Define the Parameter Grid","text":"<p>We need to tell <code>GridSearchCV</code> which hyperparameters to test. We define this in a dictionary where the keys are the names of the parameters and the values are lists of settings to try.</p> <p>To specify a hyperparameter for a step in a pipeline, we use the step's name (in lowercase), followed by two underscores (<code>__</code>), and then the hyperparameter name. The default name for the <code>DecisionTreeRegressor</code> step in our <code>make_pipeline</code> object is <code>decisiontreeregressor</code>.</p> <pre><code># Define the grid of hyperparameters to search\nparam_grid = {\n    'decisiontreeregressor__max_depth': [3, 5, 7, 10, None],\n    'decisiontreeregressor__min_samples_leaf': [1, 2, 4, 6],\n    'decisiontreeregressor__max_features': [None, 'sqrt', 'log2']\n}\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/grid-search/#step-2-set-up-and-run-gridsearchcv","title":"Step 2: Set Up and Run GridSearchCV","text":"<p>Now, we instantiate <code>GridSearchCV</code>. We provide our pipeline (<code>pipe</code>), the <code>param_grid</code>, the number of cross-validation folds (<code>cv=5</code>), and the scoring metric. Since <code>GridSearchCV</code> tries to maximize a score, and we want to minimize error, we use <code>'neg_mean_absolute_error'</code>.</p> <p>Note: This step can take a few moments to run, as it is training <code>60</code> models (<code>4 * 3 * 5</code>) five times each (<code>300</code> total fits).</p> <pre><code># Set up GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=pipe,\n    param_grid=param_grid,\n    cv=5,\n    scoring='neg_mean_absolute_error',\n    verbose=1 # This will print progress updates\n)\n\n# Fit the grid search on the training data\ngrid_search.fit(X_train, y_train)\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/grid-search/#step-3-inspect-the-results","title":"Step 3: Inspect the Results","text":"<p><code>GridSearchCV</code> stores the best combination of parameters it found in the <code>best_params_</code> attribute. The <code>best_estimator_</code> attribute holds the pipeline that was refit on the entire training set using these optimal parameters.</p> <pre><code># Print the best hyperparameters found\nprint(\"Best Hyperparameters:\")\nprint(grid_search.best_params_)\n\n# The best score is negative, so we multiply by -1 to get the MAE\nbest_mae = -grid_search.best_score_\nprint(f\"\\nBest Cross-Validated MAE: ${best_mae:,.2f}\")\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/grid-search/#step-4-final-evaluation-on-the-test-set","title":"Step 4: Final Evaluation on the Test Set","text":"<p>Finally, we use the <code>best_estimator_</code> found by <code>GridSearchCV</code> to make predictions on our held-out test set. This provides our final, unbiased assessment of the tuned model's performance.</p> <pre><code># Use the best estimator to make predictions on the test set\nfinal_predictions = grid_search.predict(X_test)\n\n# Calculate the final MAE on the test set\nfinal_mae = mean_absolute_error(y_test, final_predictions)\n\nprint(f\"\\nFinal Model MAE on Held-Out Test Set: ${final_mae:,.2f}\")\n</code></pre> <p>You can now compare this <code>final_mae</code> to the baseline MAE from our first lab to quantify the improvement gained from hyperparameter tuning.</p>"},{"location":"modeling/robust-modeling-workflow/next-steps/","title":"Next Steps and Active Learning","text":""},{"location":"modeling/robust-modeling-workflow/next-steps/#active-learning-exercise","title":"Active Learning Exercise","text":"<p>To solidify your understanding of building and tuning robust modeling pipelines, we recommend you try the following tasks. These exercises are designed to give you hands-on practice with the iterative and exploratory nature of data science.</p> <ol> <li> <p>Incorporate Categorical Features: Our model only used numerical features.</p> <ul> <li>Task: Add the <code>zipcode</code> feature to your feature set <code>X</code>. Since <code>zipcode</code> is a categorical variable, you will need to add a preprocessing step for it in your pipeline. Use the <code>OneHotEncoder</code> or <code>TargetEncoder</code> from <code>scikit-learn</code> to handle this. You will need to use <code>ColumnTransformer</code> to apply different preprocessing steps (scaling for numerical, one-hot encoding for categorical) to different columns.</li> <li>Analysis: Does adding this location data improve your model's final performance?</li> </ul> </li> <li> <p>Experiment with a Different Model: We demonstrated how to swap a <code>DecisionTreeRegressor</code> for an <code>XGBRegressor</code>.</p> <ul> <li>Task: Build and tune a new pipeline using a <code>KNeighborsRegressor</code>. Research its key hyperparameters (like <code>n_neighbors</code>) and create a new parameter grid for <code>GridSearchCV</code>.</li> <li>Analysis: How does the performance and training time of the K-Nearest Neighbors model compare to the Decision Tree and XGBoost models?</li> </ul> </li> <li> <p>Optimize for a Different Metric: We used Mean Absolute Error as our scoring metric.</p> <ul> <li>Task: Re-run one of your <code>GridSearchCV</code> experiments, but this time set <code>scoring='neg_root_mean_squared_error'</code>.</li> <li>Analysis: Does optimizing for RMSE result in a different set of best hyperparameters? Why might a business choose to optimize for RMSE over MAE? (Hint: How does RMSE penalize large errors?)</li> </ul> </li> </ol>"},{"location":"modeling/robust-modeling-workflow/next-steps/#lesson-summary","title":"Lesson Summary","text":"<p>In this lesson, you have significantly upgraded your modeling capabilities, moving from a manual, analytical workflow to a robust, automated process ready for real-world application.</p> <p>You learned that a <code>Pipeline</code> is the professional standard for encapsulating preprocessing and modeling steps, ensuring consistency and preventing critical errors like data leakage. We replaced the unreliable, single train-test split with k-fold cross-validation to build confidence in our model's performance. You saw how to systematically optimize hyperparameters using <code>GridSearchCV</code>, and witnessed the power of ensemble models like XGBoost to improve predictive accuracy.</p> <p>Most importantly, you now have a reusable framework for building, evaluating, and tuning models in a systematic and reproducible way\u2014a core competency for any data science practitioner.</p>"},{"location":"modeling/robust-modeling-workflow/preprocessing-pipelines/","title":"Preprocessing Pipelines","text":"<p>Preprocessing Pipelines are an essential element of the robust workflow. In this section we will discuss key motivations behind the use of pipelines. </p> <p>When working with real-world data, it's rare that the data is perfectly formatted for a machine learning algorithm. Different features often have vastly different scales, which can cause problems for many models.</p> <p>For example, consider the King County housing dataset we analyzed for the data storytelling lesson. We have features like <code>bedrooms</code> (with values typically from 1 to 5) and <code>sqft_living</code> (with values from 500 to over 10,000). Many algorithms, including linear models and distance-based algorithms, are sensitive to these scale differences. A feature with a large scale can have an outsized influence on the model's outcome, not because it's more important, but simply because its numerical values are larger.</p> <p>To address this, we apply preprocessing, such as feature scaling, to standardize the range of our features. While we could perform this step manually, doing so is inefficient and introduces a significant risk of a critical error known as data leakage. This occurs when information from the test set inadvertently influences the training process, leading to a model that appears to perform better than it actually would in the real world.</p> <p>The computational solution to this challenge is a layer of abstraction. This comes in the form of a <code>Pipeline</code> meta-estimator that wraps over simple estimator objects such as transformers and models. A <code>Pipeline</code> meta-estimator object lets data flow through arbitrary number of preprocessing transformer steps before the data reaches a model.</p> <p>Using a <code>Pipeline</code> provides following key advantages:</p> <ol> <li>Consistency: It ensures the exact same preprocessing steps are applied to both the training and test data.</li> <li>Prevents Data Leakage: It prevents information from the test set from leaking into the training process during steps like cross-validation.</li> <li>Simplicity: It encapsulates the entire workflow into a single object that can be treated like any other <code>scikit-learn</code> model, with the same <code>.fit()</code> and <code>.predict()</code> methods.</li> <li>Experimentation: Pipelines make it exceptionally easy to experiment with different preprocessing steps or models, compare the results, and arrive at optimal modeling choices.</li> <li>Reproducibility: This is one of the key best practices that helps ensure the model building process is reproducible across time, site, and other factors.</li> </ol>"},{"location":"modeling/robust-modeling-workflow/preprocessing-pipelines/#consistency","title":"Consistency","text":"<p>A <code>Pipeline</code> ensures that your modeling workflow is deterministic and consistently applied. It acts as a locked-down recipe, guaranteeing that the exact same sequence of preprocessing steps\u2014in the same order and with the same configuration\u2014is applied during training, evaluation, and final prediction. This eliminates a common source of error where manual steps are accidentally omitted or misconfigured when working with new data, ensuring that your production predictions are generated in the exact same manner as your training experiments.</p>"},{"location":"modeling/robust-modeling-workflow/preprocessing-pipelines/#prevents-data-leakage","title":"Prevents Data Leakage","text":"<p>This is arguably the most critical operational advantage of using a <code>Pipeline</code>. Data leakage occurs when information from outside the training dataset is used to create the model. A classic example is scaling your data before performing a train-test split. The <code>Pipeline</code> prevents this by intelligently managing the data flow. When used within a process like cross-validation, the <code>Pipeline</code> ensures that the preprocessing steps (e.g., <code>StandardScaler</code>) are fitted only on the training portion of the data for each fold. The learned transformation is then applied to both the training and validation portions, correctly simulating how the model would behave on truly unseen data.</p>"},{"location":"modeling/robust-modeling-workflow/preprocessing-pipelines/#simplicity","title":"Simplicity","text":"<p>Despite encapsulating a potentially complex sequence of operations, a <code>Pipeline</code> object adheres to the same consistent API as any other <code>scikit-learn</code> estimator. It has the familiar <code>.fit()</code>, <code>.predict()</code>, and <code>.score()</code> methods. This elegant abstraction allows you to treat the entire workflow\u2014from data scaling to final prediction\u2014as a single object. This greatly simplifies your code and makes it easier to use your entire workflow within other <code>scikit-learn</code> utilities, such as performing a <code>GridSearchCV</code> over both preprocessing and model hyperparameters simultaneously.</p>"},{"location":"modeling/robust-modeling-workflow/preprocessing-pipelines/#experimentation-and-comparison","title":"Experimentation and Comparison","text":"<p>Pipelines make it exceptionally easy to experiment with different preprocessing steps or models. Because the <code>Pipeline</code> object encapsulates the entire workflow, you can construct multiple pipelines with different components and compare them systematically.</p> <p>For instance, you could create one pipeline with a <code>StandardScaler</code> and another with a <code>MinMaxScaler</code> and then evaluate both using the same cross-validation procedure to see which scaling method yields better results for your specific model and data. This modularity is even more powerful when combined with tools like <code>GridSearchCV</code>, which can treat the preprocessing steps themselves as hyperparameters to be optimized.</p>"},{"location":"modeling/robust-modeling-workflow/preprocessing-pipelines/#reproducibility","title":"Reproducibility","text":"<p>Reproducibility is a cornerstone of reliable data science, and Pipelines are a critical tool for achieving it. By containing all preprocessing and modeling steps in a single object, a <code>Pipeline</code> acts as a complete, self-contained \"recipe\" for your workflow.</p> <p>This single object can be saved (e.g., using <code>joblib</code>) and reloaded later or shared with colleagues. When the saved <code>Pipeline</code> is used to make new predictions, it guarantees that the exact same sequence of transformations with the exact same learned parameters is applied, eliminating the risk of manual errors and ensuring that your results are consistent and reproducible.</p>"},{"location":"modeling/robust-modeling-workflow/robust-modeling-workflow/","title":"Production-grade Workflow","text":"<p>In the previous lesson, we established the core 6-step workflow for a basic classification model. While excellent for initial analysis, moving a model from a notebook to a real business application requires a more robust, automated, and maintainable process. In this lesson, we introduce key upgrades that improve robustness of the 6-step workflow. You might ask, \"Why this extra work and why do we care about robustness of a workflow?\"</p> <p>The answer lies in agility and continuous improvement. Real-world models must consistently deliver business value while navigating changing data, evolving business requirements, and advancing technology. This demanding environment requires that models be continuously developed, integrated, and deployed in short cycles. By transitioning from a manual process to a professional-grade workflow, you lay the foundation for modern MLOps (Machine Learning Operations) and CI/CD (Continuous Integration/Continuous Deployment) practices. The tools in this lesson are specifically designed to help data science teams build this capability, allowing them to stay responsive and maintain high-quality models in production.</p> <p>Let's look at how we'll upgrade our workflow to meet these professional standards:</p> <ul> <li>Step 1: Prepare Data<ul> <li>New Challenge: Manual preprocessing is error-prone and hard to replicate.</li> <li>Our Upgrade: We'll use the <code>Pipeline</code> object to create a single, deployable asset that encapsulates all our steps. This ensures consistency from training to production.</li> </ul> </li> </ul> <ul> <li>Step 2: Split Data<ul> <li>New Challenge: A single train-test split can be misleading if we get an \"unlucky\" sample.</li> <li>Our Upgrade: We'll implement K-Fold Cross-Validation to build confidence and get a more reliable estimate of our model's real-world performance.</li> </ul> </li> </ul> <ul> <li>Step 3: Instantiate Model<ul> <li>New Challenge: How do we efficiently find the best Hyperparameters for our model as new data arrives?</li> <li>Our Upgrade: We will use <code>GridSearchCV</code> to automate the optimization process, ensuring we can quickly and systematically find the best model configuration.</li> </ul> </li> </ul> <ul> <li>Step 6: Evaluate Model Performance<ul> <li>New Challenge: \"Accuracy\" doesn't work for regression. We need metrics that measure the magnitude of our prediction error.</li> <li>Our Upgrade: We will use key regression Metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to quantify business impact.</li> </ul> </li> </ul> <p>By the end of this lesson, you will have moved beyond building a simple model to constructing a sophisticated, automated workflow ready for real-world demands. We will build this workflow by tackling a prediction task using the King County housing dataset, which was introduced in the last module. This larger dataset provides the perfect opportunity to apply familiar supervised learning techniques in a more robust, production-style setting.</p>"},{"location":"modeling/robust-modeling-workflow/tuning-ensemble/","title":"Introduction to Ensemble Models","text":"<p>While tuning a single model can yield significant gains, one of the most powerful techniques in machine learning is to combine multiple models into a single, more powerful ensemble. The core idea is that a \"team\" of models can often achieve better performance, stability, and robustness than any single \"star player\" model.</p> <p>Two of the most common and effective ensemble strategies are Bagging and Boosting.</p>"},{"location":"modeling/robust-modeling-workflow/tuning-ensemble/#bagging-the-power-of-diverse-opinions","title":"Bagging: The Power of Diverse Opinions \ud83c\udf33\ud83c\udf33\ud83c\udf33","text":"<p>Bagging, which stands for Bootstrap Aggregating, works like asking a large group of independent experts for their opinion and averaging their responses. The goal is to reduce variance and prevent a single model from overfitting to quirks in the data.</p> <p>The most popular bagging algorithm is the Random Forest. Here\u2019s how it works:</p> <ol> <li>It creates hundreds of different Decision Trees.</li> <li>Each tree is trained on a random sample (a \"bootstrap\" sample) of the original training data.</li> <li>Each tree also only considers a random subset of features at each split point.</li> <li>To make a final prediction, the Random Forest averages the predictions from all the individual trees.</li> </ol> <p>This process results in a model that is typically much more accurate and stable than a single, highly-tuned Decision Tree.</p>"},{"location":"modeling/robust-modeling-workflow/tuning-ensemble/#boosting-the-power-of-iterative-improvement","title":"Boosting: The Power of Iterative Improvement \ud83c\udfaf","text":"<p>Boosting builds a model sequentially, where each new model in the sequence focuses on correcting the errors made by the previous one. Think of it as building a team of specialists, where each new member is trained specifically to fix the mistakes of the team so far.</p> <p>Popular boosting algorithms include Gradient Boosting and XGBoost. The general process is:</p> <ol> <li>A simple initial model is trained on the data.</li> <li>The algorithm identifies the errors (residuals) made by this first model.</li> <li>A second model is trained specifically to predict those errors.</li> <li>This process is repeated, with each subsequent model focusing on the remaining errors, until the model's predictions are highly accurate.</li> </ol> <p>Boosting often leads to state-of-the-art performance but can be more sensitive to noisy data and requires more careful tuning than bagging models. For our demonstration, we will focus on the highly effective and widely used Random Forest.</p> Note <p>The popular <code>xgboost</code> library provides a wrapper that fully adopts <code>scikit-learn</code>'s Estimator API. This means you can plug an <code>XGBRegressor</code> directly into your existing <code>Pipeline</code> and <code>GridSearchCV</code> framework just like any other <code>scikit-learn</code> model, making it an excellent choice for a demonstration.</p>"},{"location":"modeling/robust-modeling-workflow/tuning-ensemble/#tuning-an-ensemble-model","title":"Tuning an Ensemble Model","text":"<p>Let's see the power of ensembles in action. We will replace the single <code>DecisionTreeRegressor</code> in our workflow with a powerful boosting model, <code>XGBRegressor</code>, and run our hyperparameter search again to find the best configuration for this new model.</p>"},{"location":"modeling/robust-modeling-workflow/tuning-ensemble/#setup-importing-xgboost","title":"Setup: Importing XGBoost","text":"<p>First, you may need to install the <code>xgboost</code> library. You can do this in a notebook cell by running <code>!pip install xgboost</code>. Then, we import the model.</p> <pre><code>from xgboost import XGBRegressor\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/tuning-ensemble/#step-1-create-a-new-pipeline-with-xgboost","title":"Step 1: Create a New Pipeline with XGBoost","text":"<p>We define a new pipeline, this time incorporating the <code>XGBRegressor</code>. Notice how seamlessly the new model fits into the existing structure.</p> <pre><code># Create the new pipeline with the XGBoost Regressor\nxgb_pipe = make_pipeline(\n    StandardScaler(),\n    XGBRegressor(random_state=42)\n)\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/tuning-ensemble/#step-2-define-a-new-parameter-grid","title":"Step 2: Define a New Parameter Grid","text":"<p>Next, we create a new parameter grid with hyperparameters specific to <code>XGBRegressor</code>. Note the naming convention <code>xgbregressor__</code> to target the correct step in the pipeline.</p> <pre><code># Define the grid of hyperparameters for XGBoost\nxgb_param_grid = {\n    'xgbregressor__n_estimators': [100, 200],\n    'xgbregressor__max_depth': [3, 5, 7],\n    'xgbregressor__learning_rate': [0.1, 0.05]\n}\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/tuning-ensemble/#step-3-run-the-grid-search","title":"Step 3: Run the Grid Search","text":"<p>We set up and run a new <code>GridSearchCV</code> with our XGBoost pipeline and its corresponding parameter grid.</p> <pre><code># Set up and run the new grid search\nxgb_grid_search = GridSearchCV(\n    estimator=xgb_pipe,\n    param_grid=xgb_param_grid,\n    cv=5,\n    scoring='neg_mean_absolute_error',\n    verbose=1\n)\n\n# Fit the grid search on the training data\nxgb_grid_search.fit(X_train, y_train)\n</code></pre>"},{"location":"modeling/robust-modeling-workflow/tuning-ensemble/#step-4-evaluate-the-tuned-ensemble-model","title":"Step 4: Evaluate the Tuned Ensemble Model","text":"<p>Finally, we inspect the results and evaluate our best-performing XGBoost model on the held-out test set. You can compare this final Mean Absolute Error to the one achieved by the single Decision Tree to quantify the performance gain from using a more powerful ensemble model.</p> <pre><code># Print the best hyperparameters\nprint(\"Best XGBoost Hyperparameters:\")\nprint(xgb_grid_search.best_params_)\n\n# Print the best cross-validated score\nbest_mae_xgb = -xgb_grid_search.best_score_\nprint(f\"\\nBest Cross-Validated MAE (XGBoost): ${best_mae_xgb:,.2f}\")\n\n# Evaluate the final model on the test set\nfinal_predictions_xgb = xgb_grid_search.predict(X_test)\nfinal_mae_xgb = mean_absolute_error(y_test, final_predictions_xgb)\n\nprint(f\"\\nFinal Model MAE on Test Set (XGBoost): ${final_mae_xgb:,.2f}\")\n</code></pre>"},{"location":"resources/","title":"Resources","text":"<ul> <li> <p> Courses from LinkedIn Learning</p> <p> Coming soon</p> </li> <li> <p> Courses and Books from O\u2019Reilly Learning</p> <p> Coming soon</p> </li> <li> <p> Kaggle Resources</p> <p> Coming soon</p> </li> <li> <p> Open Datasets</p> </li> </ul>"},{"location":"resources/#_1","title":"Resources","text":""},{"location":"resources/open-data/","title":"Data Sources","text":""},{"location":"resources/open-data/#course-project-data-sources","title":"Course Project Data Sources","text":"<p>Below is a list of data sources that you may consider for your semester-long project. When choosing a dataset, remember to consider the project requirements: you'll need to be able to wrangle it with Polars, visualize it with Altair, and apply scikit-learn models. Also, think about whether your chosen dataset could support the optional extra credit in time-series or text analysis. </p> <p>Not an Exhaustive/Authoritative List</p> <p>This list is intended to help you get started; it is neither exhaustive nor authoritative. Please feel free to explore sources beyond those listed here. Choosing data that you find personally meaningful can often lead to a more engaging and well-executed project</p> <p>For each data source, you'll find a brief introduction and some tips to help you get started. Ensure your chosen dataset is readily accessible and that you can create a comprehensive data dictionary for it.</p>"},{"location":"resources/open-data/#general-dataset-search-engines","title":"General Dataset Search Engines","text":"<p>These are great starting points if you have a topic in mind but don't know where to find relevant data.</p>"},{"location":"resources/open-data/#1-google-dataset-search","title":"1. Google Dataset Search","text":"<p>Link: https://datasetsearch.research.google.com/</p> <p>Short Description: A search engine specifically for datasets. It indexes datasets from various sources across the web.</p> <p>Tips:</p> <ul> <li>Use keywords related to your interests (e.g., \"climate change,\" \"mental health,\" \"housing prices\").</li> <li>Check the source of the dataset to ensure reliability and accessibility.</li> <li>Look for common file formats like CSV, JSON, or Excel.</li> <li>Project Fit: Can lead to virtually any type of data, so evaluate carefully against project requirements.</li> </ul>"},{"location":"resources/open-data/#2-datagov","title":"2. Data.gov","text":"<p>Link: https://catalog.data.gov/dataset</p> <p>Short Description: The central repository for U.S. government open data. Provides access to a vast collection of datasets from federal agencies.</p> <p>Tips:</p> <ul> <li>The sheer volume can be overwhelming; use filters for topics, formats (CSV, Excel, XML, JSON), and agencies.</li> <li>Look for datasets with good metadata or accompanying documentation to help create your data dictionary.</li> <li>Data quality and cleanliness can vary, offering good wrangling practice.</li> <li>Project Fit: Suitable for a wide array of topics including health, environment, public safety, and economics. Many datasets are suitable for regression/classification and some for time-series analysis.</li> </ul>"},{"location":"resources/open-data/#3-awesome-public-datasets-github","title":"3. Awesome Public Datasets (GitHub)","text":"<p>Link: https://github.com/awesomedata/awesome-public-datasets</p> <p>Short Description: A community-curated list of high-quality public datasets, often categorized by topic (e.g., agriculture, education, finance).</p> <p>Tips:</p> <ul> <li>Browse by category to find datasets matching your interests.</li> <li>Links usually go directly to the dataset or a page describing it.</li> <li>Great for finding datasets that are already somewhat vetted by the community.</li> <li>Project Fit: Diverse topics; check individual datasets for suitability with Polars, Altair, and scikit-learn.</li> </ul>"},{"location":"resources/open-data/#4-dataisplural","title":"4. DataIsPlural","text":"<p>Link: https://www.data-is-plural.com/</p> <p>Short Description: A weekly newsletter (with a searchable archive) that highlights interesting and often unique datasets.</p> <p>Tips:</p> <ul> <li>Explore the archive for past datasets.</li> <li>Good for finding timely or niche datasets you might not discover elsewhere.</li> <li>Pay attention to the original source and data format.</li> <li>Project Fit: Excellent for unique project ideas. Data will vary widely.</li> </ul>"},{"location":"resources/open-data/#machine-learning-general-purpose-repositories","title":"Machine Learning &amp; General Purpose Repositories","text":"<p>These sites host datasets often used in machine learning and data science.</p>"},{"location":"resources/open-data/#5-kaggle","title":"5. Kaggle","text":"<p>Link: https://www.kaggle.com/datasets</p> <p>Short Description: A popular platform for data science competitions, datasets, and notebooks. Offers a vast range of datasets on diverse topics.</p> <p>Tips:</p> <ul> <li>Many datasets are in CSV format and are relatively clean, but always perform your own EDA.</li> <li>Explore existing notebooks for inspiration on data wrangling and analysis (but ensure your work is original).</li> <li>Check dataset licenses for any usage restrictions.</li> <li>Good source for text data (e.g., product reviews, tweets) and time-series data.</li> <li>Project Fit: Excellent for all project components. Many datasets are well-suited for classification, regression, and the extra credit components.</li> </ul>"},{"location":"resources/open-data/#6-openml","title":"6. OpenML","text":"<p>Link: https://www.openml.org/search?type=data</p> <p>Short Description: An open science platform for machine learning that allows sharing of datasets, algorithms, and experimental results.</p> <p>Tips:</p> <ul> <li>Datasets are often well-documented and structured for machine learning tasks.</li> <li>You can filter datasets by task type (e.g., classification, regression).</li> <li>Good for finding benchmark datasets to test your modeling skills.</li> <li>Project Fit: Strong for the modeling component. Provides clear features and targets.</li> </ul>"},{"location":"resources/open-data/#7-uci-machine-learning-repository","title":"7. UCI Machine Learning Repository","text":"<p>Link: https://archive.ics.uci.edu/ml/index.php</p> <p>Short Description: One of the oldest and most well-known repositories of datasets used for machine learning research.</p> <p>Tips:</p> <ul> <li>Contains a wide variety of datasets, often classic ones used in many studies.</li> <li>Data formats can sometimes be older or require more parsing (e.g., .data files without headers). Check for accompanying <code>.names</code> files or descriptions.</li> <li>Many datasets have clear descriptions of variables and potential ML tasks.</li> <li>Project Fit: Excellent for practicing data wrangling and modeling. Good for both supervised and unsupervised learning tasks.</li> </ul>"},{"location":"resources/open-data/#government-public-sector-data","title":"Government &amp; Public Sector Data","text":"<p>These sources provide official data from governmental and international organizations.</p>"},{"location":"resources/open-data/#8-us-census-bureau","title":"8. U.S. Census Bureau","text":"<p>Link: https://data.census.gov/</p> <p>Short Description: Provides a wealth of data about the U.S. population and economy.</p> <p>Tips:</p> <ul> <li><code>data.census.gov</code> is the primary data portal. It can be complex to navigate; use the advanced search and filtering options.</li> <li>American Community Survey (ACS) data is very detailed.</li> <li>Data often comes in large tables; you'll likely need to select specific variables and geographies.</li> <li>Excellent for demographic analysis and creating rich features for other projects.</li> <li>Project Fit: Great for projects involving demographics, economics, and social trends. Data often requires significant wrangling.</li> </ul>"},{"location":"resources/open-data/#9-cdc-data-centers-for-disease-control-and-prevention","title":"9. CDC Data (Centers for Disease Control and Prevention)","text":"<p>Link: https://open.cdc.gov/data.html</p> <p>Short Description: Offers a wide range of public health data, including surveillance data, statistics, and information on various health topics.</p> <p>Tips:</p> <ul> <li>The \"CDC WONDER\" (Wide-ranging Online Data for Epidemiologic Research) system is a key resource.</li> <li>Data is often available in user-friendly formats (CSV, Excel).</li> <li>Explore topics like chronic diseases, infectious diseases, behavioral risk factors, and environmental health.</li> <li>Project Fit: Ideal for public health projects. Datasets can support regression, classification, and time-series analysis (e.g., disease trends).</li> </ul>"},{"location":"resources/open-data/#10-usafacts","title":"10. USAFacts","text":"<p>Link: https://usafacts.org/</p> <p>Short Description: A non-profit organization that compiles and visualizes data from U.S. government sources. Aims to provide a data-driven portrait of the nation.</p> <p>Tips:</p> <ul> <li>Presents data in an accessible way, often with charts and reports. Look for links to download the underlying data (usually CSV).</li> <li>Covers topics like population, economy, health, and education.</li> <li>Useful for projects focusing on U.S. trends and government metrics.</li> <li>Project Fit: Good for projects requiring aggregated US data. Data is typically clean.</li> </ul>"},{"location":"resources/open-data/#11-who-data-world-health-organization","title":"11. WHO Data (World Health Organization)","text":"<p>Link: https://www.who.int/data/collections</p> <p>Short Description: The primary source for global health statistics and information.</p> <p>Tips:</p> <ul> <li>Explore their \"Data Collections\" and the Global Health Observatory (GHO) data repository.</li> <li>Data covers global health priorities, diseases, health systems, and mortality.</li> <li>Pay attention to indicator definitions and data collection methodologies.</li> <li>Project Fit: Suitable for projects with a global health focus. Many datasets lend themselves to time-series analysis and cross-country comparisons.</li> </ul>"},{"location":"resources/open-data/#12-nhtsa-datasets-national-highway-traffic-safety-administration","title":"12. NHTSA Datasets (National Highway Traffic Safety Administration)","text":"<p>Link: https://www.nhtsa.gov/nhtsa-datasets-and-apis</p> <p>Short Description: Provides data related to traffic safety, including crash statistics, vehicle safety ratings, and recall information.</p> <p>Tips:</p> <ul> <li>The Fatality Analysis Reporting System (FARS) is a rich dataset for detailed crash analysis.</li> <li>Data can be very detailed and may require careful study of documentation and coding manuals.</li> <li>Files can be large, offering good practice with Polars for efficient data handling.</li> <li>Project Fit: Excellent for projects on transportation safety, risk analysis. Good for classification/regression (e.g., predicting crash severity).</li> </ul>"},{"location":"resources/open-data/#13-world-bank-open-data","title":"13. World Bank Open Data","text":"<p>Link: https://databank.worldbank.org/source/world-development-indicators</p> <p>Short Description: Provides free and open access to global development data.</p> <p>Tips:</p> <ul> <li>Includes a vast collection of time-series data on indicators like GDP, poverty, education, and environment from countries around the world.</li> <li>The World Development Indicators (WDI) is a flagship collection.</li> <li>Data can be downloaded in various formats, including CSV and Excel.</li> <li>Project Fit: Excellent for projects on international development, economics, and socio-economic trends. Very suitable for time-series analysis and cross-sectional studies.</li> </ul>"},{"location":"resources/open-data/#city-specific-open-data-portals","title":"City-Specific Open Data Portals","text":"<p>Many cities now have open data portals, which are fantastic for local or urban-focused projects.</p>"},{"location":"resources/open-data/#14-nyc-opendata","title":"14. NYC OpenData","text":"<p>Link: https://opendata.cityofnewyork.us/data/</p> <p>Short Description: Comprehensive portal for data from New York City agencies.</p> <p>Tips:</p> <ul> <li>Huge variety of datasets: public safety, transportation (taxi trips are famous), environment, health, education, etc.</li> <li>Often have APIs and various download formats (CSV, JSON).</li> <li>Many datasets have temporal components, good for time-series.</li> <li>Project Fit: Great for urban analytics projects. Data quality varies.</li> </ul>"},{"location":"resources/open-data/#15-data-wprdc-western-pennsylvania-regional-data-center","title":"15. Data WPRDC (Western Pennsylvania Regional Data Center)","text":"<p>Link: https://data.wprdc.org/dataset/?organization=city-of-pittsburgh</p> <p>Short Description: A regional open data portal with a strong focus on the City of Pittsburgh and Allegheny County.</p> <p>Tips:</p> <ul> <li>Covers topics like public safety, housing, transportation, and environment.</li> <li>Good for projects with local relevance (if you're in the area or interested in Pittsburgh).</li> <li>Check dataset update frequencies and metadata.</li> <li>Project Fit: Similar to NYC OpenData, good for urban analytics.</li> </ul>"},{"location":"resources/open-data/#16-city-of-chicago-data-portal","title":"16. City of Chicago Data Portal","text":"<p>Link: https://data.cityofchicago.org/</p> <p>Short Description: One of the most extensive city data portals, with a wide array of datasets about Chicago.</p> <p>Tips:</p> <ul> <li>Topics include crime, transportation (Divvy bike share, taxi trips), health, permits, etc.</li> <li>Look for datasets with historical data for time-series analysis.</li> <li>Many datasets are updated regularly.</li> <li>Project Fit: Excellent for diverse urban projects, from analyzing crime patterns to public transit usage.</li> </ul>"},{"location":"resources/open-data/#other-interesting-sources","title":"Other Interesting Sources","text":""},{"location":"resources/open-data/#17-fivethirtyeight","title":"17. FiveThirtyEight","text":"<p>Link: https://data.fivethirtyeight.com/</p> <p>Short Description: A site known for data-driven journalism (politics, sports, science, economics). They often share the data and code behind their articles.</p> <p>Tips:</p> <ul> <li>Datasets are usually well-cleaned, in CSV format, and topical.</li> <li>Great for projects that aim to replicate or extend existing analyses, or to explore current events.</li> <li>Often comes with context and methodology explanations.</li> <li>Project Fit: Excellent for projects where storytelling with data is key. Data can be suitable for various modeling tasks and EDA.</li> </ul>"},{"location":"resources/open-data/#18-our-world-in-data","title":"18. Our World in Data","text":"<p>Link: https://ourworldindata.org/</p> <p>Short Description: An online publication that presents data and research on global living conditions and changes (e.g., health, poverty, environment, war).</p> <p>Tips:</p> <ul> <li>Provides clean, well-documented datasets, often in CSV or Excel format (look for \"Download\" buttons on charts or at the bottom of articles).</li> <li>Visualizations on the site can inspire your own EDA with Altair.</li> <li>Focuses on long-term trends, making many datasets suitable for time-series analysis.</li> <li>Project Fit: Excellent for projects addressing global issues and trends. Strong on data suitable for EDA and time-series.</li> </ul>"},{"location":"resources/open-data/#19-spotify-charts","title":"19. Spotify Charts","text":"<p>Link: https://charts.spotify.com/</p> <p>Short Description: Provides data on top and viral songs globally and by country.</p> <p>Tips:</p> <ul> <li>Data is typically available as CSVs for current and past charts (e.g., daily, weekly) via a download button on the chart pages.</li> <li>Can be used for analyzing music trends, popularity, and potentially for time-series forecasting.</li> <li>You might need to combine this with other music metadata sources (e.g., Spotify API for audio features, if you're ambitious and it fits project scope).</li> <li>Project Fit: Good for projects on cultural trends, time-series analysis.</li> </ul>"},{"location":"resources/open-data/#20-imdb-datasets-internet-movie-database","title":"20. IMDb Datasets (Internet Movie Database)","text":"<p>Link: https://developer.imdb.com/non-commercial-datasets/</p> <p>Short Description: IMDb provides subsets of their data for non-commercial use.</p> <p>Tips:</p> <ul> <li>Data includes movie titles, ratings, genres, cast/crew, etc., in TSV (tab-separated values) format.</li> <li>Requires some wrangling to link different files (e.g., titles to ratings).</li> <li>If you can find associated review text (e.g., from other public academic datasets of IMDb reviews, being mindful of terms of service), it could be used for text analysis.</li> <li>Project Fit: Good for projects on the film industry. Structured data is good for general analysis; combining with review text (if sourced appropriately) can enable the text analysis extra credit.</li> </ul> <p>Remember to thoroughly explore any dataset you choose. Read its documentation, understand its variables, and consider its limitations before committing to it for your project. Good luck!</p>"},{"location":"syllabus/","title":"Syllabus","text":"<p>Course Information</p> <ul> <li>Course Title: Data Programming Essentials with Python</li> <li>Class Number: 16908</li> <li>Term &amp; Credits: Summer 2024-2015 / 3 Credits</li> <li>Pre-requisites/Co-requisites: Graduate-level Statistics (BQOM 2401or equivalent)</li> </ul> <p>Schedule &amp; Instructor</p> <ul> <li>Meeting Times: Thursdays 6:20-9:20pm </li> <li>Meeting Location: Zoom Meeting Room. See Canvas for passcode.</li> <li>Instructor: Midhu Balan</li> <li>Virtual Office Hours: Th 3:00-5:00pm @ Zoom.</li> </ul> <ul> <li> Course Description</li> <li> Schedule</li> <li> Course Material</li> <li> Grading &amp; Evaluation</li> <li> Project Guide</li> <li> Course Policies</li> </ul>"},{"location":"syllabus/course-description/","title":"Course Description","text":""},{"location":"syllabus/course-description/#course-description","title":"Course Description","text":"<p>In today's data-driven business landscape, the ability to programmatically access, manipulate, visualize, and model data is no longer a niche skill but a core competency for effective decision-making. Data Programming Essentials with Python is designed specifically for individuals, particularly those in business roles with no prior programming experience, aiming to demystify the world of data science tooling. This course provides a hands-on, practical introduction to the essential concepts and Python libraries needed to turn raw data into actionable insights.</p> <p>We move beyond just syntax, focusing on building intuition for how modern data science libraries like Polars (for high-performance data manipulation), Altair (for declarative statistical visualization), and scikit-learn (for machine learning) are designed. By understanding the choices behind their APIs and domain models \u2013 grounded in practical object-oriented concepts \u2013 you'll gain a deeper, more adaptable understanding that transcends rote memorization.</p> <p>Starting with Python fundamentals within the accessible Google Colab environment, we'll progress through the core data science workflow: acquiring and exploring data, generating insightful visualizations, building and evaluating predictive models (regression and classification), and uncovering structure through unsupervised techniques (clustering and dimensionality reduction). We will also introduce approaches for handling increasingly common specialized data types like time series and text data. Throughout the course, you will be encouraged to leverage documentation and generative AI tools strategically to accelerate your learning and problem-solving.</p> <p>The ultimate goal is to equip you with the foundational knowledge and practical confidence to apply these powerful data programming techniques to your own professional challenges and personal projects, enabling you to ask better questions and derive more value from data, irrespective of your starting point.</p>"},{"location":"syllabus/course-description/#learning-objectives","title":"Learning Objectives","text":"<p>Upon successful completion of this course, you will be able to:</p> <ol> <li> <p>Master Core Python Fundamentals for Data Science:</p> <ul> <li>Write functional Python code utilizing essential data structures (lists, dictionaries), control flow (loops, conditionals), and functions.</li> <li>Set up and manage a data science programming environment using Google Colab notebooks.</li> <li>Explain fundamental object-oriented concepts (objects, classes, methods, attributes) practically as they relate to using and understanding Python library APIs.</li> </ul> </li> <li> <p>Perform Effective Exploratory Data Analysis (EDA):</p> <ul> <li>Manipulate and transform data efficiently using the Polars DataFrame library and its expression API.</li> <li>Create informative and interpretable data visualizations using the Altair declarative charting library.</li> <li>Identify patterns, trends, anomalies, and relationships within datasets to guide further analysis.</li> </ul> </li> <li> <p>Implement Foundational Machine Learning Workflows:</p> <ul> <li>Understand and apply the standard machine learning workflow using scikit-learn, including data preprocessing and model evaluation.</li> <li>Build, train, and interpret results from common supervised learning models for regression and classification tasks.</li> <li>Apply basic unsupervised learning techniques, including clustering (K-Means) and dimensionality reduction (PCA), to uncover structure in data.</li> </ul> </li> <li> <p>Gain Exposure to Specialized Data Types:</p> <ul> <li>Describe the unique characteristics and analytical approaches relevant to time series data and perform basic time series analysis.</li> <li>Understand common techniques for representing and analyzing textual data at an introductory level.</li> </ul> </li> <li> <p>Develop Practical Data Problem-Solving Skills:</p> <ul> <li>Interpret the API design choices of major data science libraries (Polars, Altair, scikit-learn) to use them more effectively.</li> <li>Leverage library documentation, online resources, and generative AI tools as aids for independent learning and troubleshooting.</li> <li>Translate loosely defined problems into concrete data analysis steps and apply appropriate programming techniques within a course project.</li> <li>Communicate findings from data analysis clearly through visualizations and summaries.</li> </ul> </li> </ol>"},{"location":"syllabus/course-materials/","title":"Course Materials","text":"<p>All required course materials, including books and software tools, are  available to students at no cost through the Pitt Library System or as  open-source content. No purchases of books or software are necessary for this course.</p>"},{"location":"syllabus/course-materials/#course-website-and-communication","title":"Course website and communication","text":"<p>We will use Pitt Canvas as the primary place to distribute other course-related  content and announcements. All learning materials including video recordings  and assignments will be posted on the course Canvas page. </p> <p>Info</p> <p>Weekly sessions and office hours will be through Zoom.</p>"},{"location":"syllabus/course-materials/#books","title":"Books","text":"<ul> <li> <p> Exploratory Data Analysis</p> <p> Grammar of Graphics</p> <p> Data Storytelling with Altair and AI</p> </li> <li> <p> Data Wrangling</p> <p> Python Polars: The Definitive Guide</p> <p> Polars Cookbook</p> </li> <li> <p> Data Science</p> <p> Data Mining for Business Analytics</p> <p> Python data science handbook</p> </li> <li> <p> Machine Learning Algorithms</p> <p> Elements of Statistical Learning</p> <p> An Introduction to Statistical Learning</p> </li> <li> <p> Advanced Data Types</p> <p> NLP with Python</p> <p> Practical Time Series Analysis</p> </li> <li> <p> Programming</p> <p> Think Python</p> <p> Automate the Boring Stuff with Python</p> </li> </ul>"},{"location":"syllabus/course-materials/#core-software-tools","title":"Core Software Tools","text":"<ul> <li> <p> Google Colab</p> <p> Colab Overview</p> </li> <li> <p> Python</p> <p> The Python Tutorial</p> </li> <li> <p> Polars</p> <p> Polars User Guide</p> </li> <li> <p> Altair</p> <p> Altair User Guide</p> </li> <li> <p> scikit-learn</p> <p> scikit-learn User Guide</p> </li> </ul>"},{"location":"syllabus/course-policies/","title":"Course Policies","text":""},{"location":"syllabus/course-policies/#your-well-being-matters","title":"Your Well-being Matters","text":"<p>College/Graduate school can be an exciting and challenging time for students. Taking time to maintain your well-being  and seek appropriate support can help you achieve your goals and lead a fulfilling life. It can be helpful to remember  that we all benefit from assistance and guidance at times, and there are many resources available to support your  well-being while you are at Pitt. You are encouraged to visit Thrive@Pitt to learn more  about well-being and the many campus resources available to help you thrive.</p> <p>If you or anyone you know experiences overwhelming academic stress, persistent difficult feelings and/or challenging  life events, you are strongly encouraged to seek support. In addition to reaching out to friends and loved ones,  consider connecting with a faculty member you trust for assistance connecting to helpful resources.</p> <p>The University Counseling Center is also here for you. You can call 412-648-7930  at any time to connect with a clinician. If you or someone you know is feeling suicidal, please call the University  Counseling Center at any time at 412-648-7930. You can also contact Resolve Crisis Network at 888-796-8226.  If the situation is life threatening, call Pitt Police at 412-624-2121 or dial 911.</p>"},{"location":"syllabus/course-policies/#equity-diversity-and-inclusion","title":"Equity, Diversity, and Inclusion","text":"<p>The University of Pittsburgh does not tolerate any form of discrimination, harassment, or retaliation based on  disability, race, color, religion, national origin, ancestry, genetic information, marital status, familial status,  sex, age, sexual orientation, veteran status or gender identity or other factors as stated in the University\u2019s Title IX  policy. The University is committed to taking prompt action to end a hostile environment that interferes with the  University\u2019s mission. For more information about policies, procedures, and practices, visit the  Civil Rights &amp; Title IX Compliance web page.</p> <p>I ask that everyone in the class strive to help ensure that other members of this class can learn in a supportive  and respectful environment. If there are instances of the aforementioned issues, please contact the Title IX Coordinator , by calling 412-648-7860, or e-mailing titleixcoordinator@pitt.edu. Reports can  also be filed online. You may also choose to  report this to a faculty/staff member; they are required to communicate this to the University\u2019s Office of Diversity  and Inclusion. If you wish to maintain complete confidentiality, you may also contact the University Counseling Center  (412-648-7930).</p>"},{"location":"syllabus/course-policies/#disability-services","title":"Disability Services","text":"<p>If you have a disability for which you are or may be requesting an accommodation, you are encouraged to contact both  your instructor and Disability Resources and Services (DRS),  140 William Pitt Union, (412) 648-7890, drsrecep@pitt.edu,  (412) 228-5347 for P3 ASL users, as early as possible in the term. DRS will verify your disability and determine  reasonable accommodations for this course.</p>"},{"location":"syllabus/course-policies/#academic-integrity","title":"Academic Integrity","text":"<p>Students in this course will be expected to comply with the  University of Pittsburgh\u2019s Policy on Academic Integrity.  Any student suspected of violating this obligation for any reason during the semester will be required to participate  in the procedural process, initiated at the instructor level, as outlined in the University Guidelines on Academic  Integrity. This may include, but is not limited to, the confiscation of the examination of any individual suspected of  violating University Policy. Furthermore, no student may bring any unauthorized materials to an exam, including  dictionaries and programmable calculators.</p> <p>To learn more about Academic Integrity, visit the  Academic Integrity Guide  for an overview of the topic. For hands-on practice, complete the  Academic Integrity Modules.</p>"},{"location":"syllabus/grading/","title":"Course Grading","text":"<p>Your performance in this course will be evaluated based on your active engagement with the learning materials and a comprehensive semester-long project, contributing to a total of 85 base points. An optional extra credit component offers the opportunity to earn an additional 15 points, making a maximum achievable score of 100 points.</p>"},{"location":"syllabus/grading/#grade-components","title":"Grade Components","text":"<p>The grading breakdown is as follows:</p> <ul> <li>Active Learning Demonstration: 15 Points</li> <li>Semester Long Project: 70 Points Total (distributed across components below)<ul> <li>Component 1: Project Plan - 10 Points</li> <li>Component 2: Data Wrangling &amp; EDA - 30 Points</li> <li>Component 3: Modeling - 30 Points</li> </ul> </li> <li>Optional Extra Credit (Advanced Data Types): 15 Points</li> </ul>"},{"location":"syllabus/grading/#active-learning","title":"Active Learning","text":"<p>1. Active Learning Demonstration (15 Points)</p> <ul> <li>Objective: To demonstrate consistent engagement with the course's learning materials throughout the semester.</li> <li>Requirement: You are expected to actively work through the scaffolding materials (provided as Colab/Jupyter notebooks) for each session. Submit your completed practice notebooks, showing your work and exploration of the concepts covered.</li> <li>Deliverable: Submitted Colab/Jupyter notebooks containing your practice work for course sessions.</li> </ul> <p>Grading Rubric: Active Learning Demonstration (15 Points Total)</p> <p>Overall Goal: To reward consistent and active engagement with the course's hands-on learning materials throughout the semester.</p> <p>Weekly Grading Method: * For each session where practice work (Colab/Jupyter notebook based on provided scaffolding material) is assigned, your submission will be graded on a Complete / Incomplete basis. * Grading focuses on evidence of effort and engagement, not perfect correctness.</p> <p>Final Score Calculation: * Your final score out of 15 points for this component will be determined by the proportion of weekly submissions marked as 'Complete'. * Formula: <code>Score = (Number of 'Complete' Submissions / Total Number of Required Submissions) * 15</code>     * (Example: If there are 12 sessions with required submissions and a student receives 'Complete' on 10 of them, their score is (10 / 12) * 15 = 12.5 points)</p> <p>Criteria for Weekly Submissions:</p> <p>\u2705 Complete: A submission will be marked as 'Complete' if it meets the following criteria:</p> <ul> <li>Timely Submission: The notebook is submitted by the specified deadline (or within any permitted grace period).</li> <li>Evidence of Engagement: The notebook clearly shows that the student has actively worked through the provided material. This includes:<ul> <li>Most code cells have been executed (outputs are visible).</li> <li>Exercises or sections requiring student input show a genuine attempt (code written, questions answered, or analysis performed, as applicable).</li> <li>The work directly relates to the specific session's scaffolding notebook and objectives.</li> </ul> </li> <li>Good-Faith Effort: The submission demonstrates a reasonable effort to engage with the concepts and tools presented in the notebook. It's understood that solutions may not be perfect, and minor errors or incomplete sections (if the student shows they attempted them) are acceptable for a 'Complete' mark.</li> </ul> <p>\u274c Incomplete: A submission will be marked as 'Incomplete' if it meets one or more of the following criteria:</p> <ul> <li>Not Submitted: No notebook is submitted for the session.</li> <li>Minimal Effort / Blank: The submitted notebook is essentially unchanged from the provided template, or only a negligible amount of interaction is evident (e.g., only the first one or two cells executed, no attempts at exercises).</li> <li>Lack of Engagement: The submission clearly indicates that a good-faith effort was not made to work through the material or complete the assigned tasks.</li> <li>Irrelevant Content: The submitted notebook contains work that is unrelated to the specific session's assignment.</li> </ul>"},{"location":"syllabus/grading/#course-project","title":"Course Project","text":"<p>2. Semester Long Project (70 Points Total)</p> <ul> <li>Objective: To apply the data science concepts and tools learned in the course to a real-world problem. This project requires you to demonstrate proficiency in data wrangling, exploratory data analysis (EDA), and machine learning modeling.</li> <li>Core Tools: Python Polars (for data wrangling), Altair (for EDA and visualization), and scikit-learn (for modeling).</li> <li>Deliverables: The project is broken down into three graded components, submitted sequentially throughout the semester. All submissions must be via Colab Notebooks and include clear markdown annotations explaining your analytical goals, workflow steps, and interpretation of results.</li> </ul>"},{"location":"syllabus/grading/#project-plan","title":"Project Plan","text":"<ul> <li>Component 1: Project Plan (10 Points)<ul> <li>Goal: Define the scope, data, objectives, and plan for your project.</li> <li>Tasks:<ol> <li>Data Identification: Select the dataset you will use. Provide verifiable links or clear sourcing information for instructor review.</li> <li>Data Dictionary: Create a detailed data dictionary in markdown format, describing the variables, data types, and meanings within your chosen dataset.</li> <li>Analytical Objectives &amp; Techniques: Clearly state the business or research questions you aim to answer. Provide a high-level description of the primary analytical technique you plan to employ (e.g., prediction, classification, clustering, anomaly detection).</li> <li>Execution Plan: Outline the major phases of your project and propose a timeline for completion.</li> </ol> </li> </ul> </li> </ul> <p>Grading Rubric: Project Component 1 - Project Plan (10 Points Total)</p> <p>Overall Goal: To assess the clarity, feasibility, and completeness of your initial project proposal, focusing on the chosen data and the planned analysis. </p> <p>Note: While explicit points for overall presentation are removed, clear documentation and organization within the Colab notebook remain crucial for demonstrating your work effectively.</p> <p>Submission Format: Google Colab Notebook (<code>.ipynb</code>) with clear Markdown annotations.</p> <p>Criteria for Evaluation:</p> Criterion Excellent Good Fair Poor / Missing 1. Data Identification, Accessibility &amp; Dictionary (5 pts) Dataset clearly identified, highly relevant, readily accessible via verifiable link. Comprehensive, clear, well-formatted Markdown dictionary provided for key variables (name, type, description). Dataset identified &amp; generally relevant; minor issues with accessibility or link OR dictionary covers most key variables but has minor omissions/formatting issues. Dataset relevance is questionable OR link broken/accessibility issues OR dictionary is significantly incomplete, inaccurate, or poorly formatted. Dataset poorly identified, irrelevant, inaccessible, or missing OR dictionary is missing or completely inadequate. 2. Analytical Objectives, Technique &amp; Plan (5 pts) Objectives are specific, relevant (SMART-like), strongly linked to data. Appropriate high-level technique clearly stated. Execution plan has clear, logical phases and realistic timeline/milestones showing forethought. Objectives stated adequately but could be clearer/more specific. Link to data is present. Technique appropriate. Plan identifies major phases but lacks some detail or realism in timeline. Objectives are vague or weakly linked to data. Technique unclear or questionable. Plan/phases are unclear, illogical, or lack realism. Objectives missing, nonsensical, or disconnected from data. Technique missing/inappropriate. Plan is missing or completely inadequate."},{"location":"syllabus/grading/#data-wrangling-eda","title":"Data Wrangling &amp; EDA","text":"<ul> <li>Component 2: Data Wrangling and Exploratory Data Analysis (EDA) (30 Points)<ul> <li>Goal: Ingest, clean, transform, and explore your data to prepare it for modeling and gain initial insights.</li> <li>Tasks:<ol> <li>Data Ingestion &amp; Transformation (using Polars): Load the raw data identified in Component 1. Perform necessary data wrangling tasks, demonstrating proficiency in handling data type conversions, managing missing values, creating new features, filtering, aggregating, and reshaping data.</li> <li>Exploratory Data Analysis (using Altair): Conduct thorough EDA using visualizations created with Altair. Your exploration should be guided by your analytical objectives (from Component 1) and should uncover patterns, trends, relationships, and potential issues in the data. Demonstrate data storytelling by explaining what your visualizations reveal.</li> </ol> </li> </ul> </li> </ul> <p>Grading Rubric: Project Component 2 - Data Wrangling &amp; EDA (30 Points Total)</p> <p>Overall Goal: To evaluate your ability to effectively ingest, clean, transform, explore, and derive meaningful insights from your chosen dataset using Polars and Altair. </p> <p>Note: While explicit points for overall presentation are removed, clear documentation and organization within the Colab notebook remain crucial for demonstrating your work effectively across all criteria.</p> <p>Submission Format: Google Colab Notebook (<code>.ipynb</code>) with clear Markdown annotations detailing workflow, rationale, and interpretation.</p> <p>Criteria for Evaluation:</p> Criterion Excellent Good Fair Poor / Missing 1. Data Ingestion, Wrangling &amp; Cleaning (Polars) (10 pts) Data correctly ingested with thorough initial checks. Demonstrates strong Polars proficiency. Comprehensively &amp; effectively addresses data quality issues (types, missing, outliers) using logical transformations (joins, groupbys, feature eng.). Clean dataset ready for EDA/modeling. Data correctly ingested with basic checks. Adequately performs necessary cleaning/transformations using Polars, handling common issues correctly. Polars usage is functional. Dataset is reasonably prepared. Data ingestion may have minor errors or checks are minimal. Basic cleaning/transformations attempted, but significant issues may remain or handled suboptimally/inefficiently. Polars usage shows minor inaccuracies. Dataset may not be fully prepared. Unable to load data correctly OR minimal/ineffective cleaning/transformation. Major data quality issues ignored or handled incorrectly. Little evidence of Polars proficiency. Dataset not ready for analysis. 2. Exploratory Data Analysis (Altair) (10 pts) Creates a wide variety of relevant, well-chosen, clear, and correctly labeled Altair visualizations (uni/bivariate, maybe multivariate) appropriate for data types &amp; goals. Effective use of Altair features. Thorough exploration demonstrated. Creates relevant Altair charts covering key data aspects. Generally appropriate chart types &amp; labeling. Covers essential exploration adequately. Limited variety or relevance of visualizations. Chart types sometimes inappropriate, lack clarity/labels. Basic exploration only. Some reliance on non-Altair plots. Very few, irrelevant, or poorly executed visualizations. Missing charts for key aspects. Little evidence of Altair usage. Exploration is insufficient. 3. Insight Generation &amp; Data Storytelling (10 pts) Interprets EDA findings deeply and critically. Clearly and explicitly links insights back to project objectives, strongly supporting interpretations with specific chart references. Tells a compelling and coherent story with the data, effectively motivating the modeling phase. Provides valid interpretation for most visualizations. Makes clear connections to project objectives. A logical data narrative is present. Interpretation is minimal, superficial, or occasionally incorrect. Weak or unclear link between EDA &amp; objectives. Narrative is fragmented or difficult to follow. No meaningful interpretation provided. No connection to objectives. Just code/charts without explanation."},{"location":"syllabus/grading/#modeling","title":"Modeling","text":"<ul> <li>Component 3: Modeling (30 Points)<ul> <li>Goal: Apply appropriate machine learning models to address your analytical objectives.</li> <li>Tasks:<ol> <li>Model Selection &amp; Implementation (using scikit-learn): Based on your objectives and EDA findings, select and implement relevant supervised (e.g., regression, classification) and/or unsupervised (e.g., clustering) learning algorithms using scikit-learn.</li> <li>Model Evaluation &amp; Interpretation: Evaluate the performance of your models using appropriate metrics. Interpret the results in the context of your original objectives. Discuss the limitations and implications of your findings.</li> </ol> </li> </ul> </li> </ul> <p>Grading Rubric: Project Component 3 - Modeling (30 Points Total)</p> <p>Overall Goal: To evaluate your ability to select, implement, evaluate, and interpret appropriate machine learning models using scikit-learn to address your project objectives defined in Component 1 and informed by Component 2. </p> <p>Note: Clear documentation and organization within the Colab notebook remain crucial for demonstrating your work effectively.</p> <p>Submission Format: Google Colab Notebook (<code>.ipynb</code>) with clear Markdown annotations explaining the rationale, process, and findings.</p> <p>Criteria for Evaluation:</p> Criterion Excellent Good Fair Poor / Missing 1. Model Selection &amp; Implementation (sklearn) (10 pts) Choice of algorithm(s) is highly relevant to objectives &amp; data type. Strong justification provided. Correct &amp; effective use of scikit-learn (e.g., pipelines, <code>fit</code>/<code>predict</code>/<code>transform</code>, train/test split). Appropriate feature handling (scaling, encoding) implemented logically. Algorithm choice is relevant to objectives/data. Implementation uses scikit-learn correctly for basic steps. Necessary feature handling and train/test split performed adequately. Algorithm choice may be suboptimal or justification weak. Minor errors in scikit-learn usage or pipeline setup. Feature handling or data splitting might be incomplete or slightly incorrect. Algorithm choice is inappropriate or missing. Major errors in scikit-learn implementation or logic. Fundamental steps like train/test split or essential feature handling missing or incorrect. 2. Model Evaluation (10 pts) Selects highly appropriate evaluation metric(s) with clear justification relevant to project objectives. Correctly calculates/reports metrics on the test set. Compares models effectively (if applicable). Demonstrates clear understanding &amp; checks for overfitting/underfitting. Selects appropriate metric(s) for the task. Correctly calculates/reports metrics on test set. Basic comparison performed if needed. Addresses overfitting/underfitting at a basic level. Metric choice may be less appropriate or lack clear justification. Minor errors in calculation or reporting. Evaluation might be incomplete (e.g., only on training data, weak comparison). Overfitting/underfitting poorly addressed or identified. Inappropriate or missing metrics. Incorrect calculation/reporting. No comparison or testing on appropriate data split. No consideration or check for overfitting/underfitting. 3. Interpretation, Conclusion &amp; Limitations (10 pts) Provides deep, insightful interpretation of model results, explicitly linking them back to project objectives. Draws clear, well-supported conclusions answering the original question(s). Thoroughly discusses model limitations, assumptions, potential biases, and implications. Provides correct interpretation of results and relates them back to objectives. Draws reasonable conclusions based on the evaluation. Mentions some relevant limitations of the model or analysis. Interpretation is superficial, partially incorrect, or weakly linked to objectives. Conclusions are unclear or weakly supported by the evaluation results. Discussion of limitations is minimal, generic, or missing key aspects. Interpretation is missing, incorrect, or irrelevant. No connection made to objectives. Conclusions are missing or unsupported by evidence. No discussion of limitations or implications."},{"location":"syllabus/grading/#extra-credit","title":"Extra Credit","text":"<p>3. Optional Extra Credit Component: Advanced Data Types (15 Points)</p> <ul> <li>Objective: To demonstrate proficiency in analyzing specialized data types beyond the core project requirements.</li> <li>Requirement: Students choosing this option must select either time-series analysis or text analysis. This requires selecting a dataset suitable for the chosen advanced technique during the Project Plan phase (Component 1).</li> <li>Tasks: Apply relevant libraries and techniques (e.g., libraries like <code>statsmodels</code>, <code>prophet</code> for time-series; <code>nltk</code>, <code>spaCy</code>, or <code>scikit-learn</code>'s text features for text analysis) to your chosen dataset. Perform analysis appropriate to the data type and your project goals.</li> <li>Deliverable: Integrated analysis within your final project submission (Colab Notebook), clearly marked as the extra credit component, with relevant markdown explanations.</li> </ul> <p>Grading Rubric: Optional Extra Credit - Advanced Data Types (15 Points Total)</p> <p>Overall Goal: To evaluate your ability to apply techniques appropriate for either time-series or text data, interpret the results meaningfully, and integrate them with your main project analysis. Success requires choosing an appropriate dataset early in the project.</p> <p>Submission Format: Clearly marked section within the final project Colab Notebook submission, including Markdown annotations explaining the process, rationale, and findings specific to this extra credit work.</p> <p>Criteria for Evaluation:</p> Criterion Excellent Good Fair Poor / Missing 1. Data Prep &amp; Technique Selection (5 pts) Clear choice of relevant advanced track (time-series/text). Data appropriately preprocessed using techniques specific to the chosen type (e.g., time indexing/resampling; tokenization/vectorization). Appropriate libraries used effectively. Setup is clear within the notebook. Choice of track is clear &amp; appropriate. Basic necessary preprocessing performed correctly using relevant libraries. Setup is adequate. Choice of track appropriate, but preprocessing is incomplete, contains minor errors, or uses less suitable techniques/libraries. Setup might be unclear. Track choice unclear or inappropriate for data OR preprocessing is missing or contains major errors OR relevant libraries not used correctly. Setup inadequate or missing. 2. Analysis &amp; Implementation (5 pts) Correctly applies relevant &amp; reasonably sophisticated analysis techniques (e.g., forecasting model; topic model, sentiment analysis). Implementation uses library functions accurately and effectively. Analysis directly addresses relevant goal (either main project goal or specific extra credit goal). Correctly applies relevant basic analysis techniques. Implementation uses library functions correctly for the chosen method. Analysis is relevant. Attempts relevant techniques but contains minor errors in application or implementation. Analysis may be only tangentially relevant or incomplete. Inappropriate techniques applied OR major errors in implementation OR analysis is missing or irrelevant. 3. Interpretation &amp; Integration (5 pts) Provides insightful interpretation of advanced analysis results. Clearly explains findings and their significance. Effectively integrates/connects these findings with the main project's objectives or conclusions. Communication (markdown) within the extra credit section is clear and comprehensive. Provides correct interpretation of results. Explains findings adequately. Makes a reasonable attempt to connect findings to the main project. Communication is clear. Interpretation is superficial or partially incorrect. Explanation is unclear. Connection to main project is weak or missing. Communication lacks detail. Interpretation missing or incorrect. No explanation of findings. No connection made to the main project. Poor or missing communication within the section."},{"location":"syllabus/grading/#grading-scheme","title":"Grading Scheme","text":"<p>Pitt's Default Canvas Grading Scheme:</p> Letter Grade Range A 100% to 94% A- &lt; 94% to 90% B+ &lt; 90% to 87% B &lt; 87% to 84% B- &lt; 84% to 80% C+ &lt; 80% to 77% C &lt; 77% to 74% C- &lt; 74% to 70% D+ &lt; 70% to 67% D &lt; 67% to 64% D- &lt; 64% to 61% F &lt; 61% to 0%"},{"location":"syllabus/project-guide/","title":"Project Guide","text":""},{"location":"syllabus/project-guide/#introduction","title":"Introduction","text":"<p>Welcome to the semester project for our MBA Data Science course. This project provides a significant opportunity for you to synthesize and apply the core concepts learned throughout the semester. You'll engage in data wrangling using Polars, conduct exploratory data analysis (EDA) with Altair, and build predictive or descriptive models using scikit-learn, all centered around a topic of your choosing.</p> <p>The objective extends beyond simply writing functional code. We aim for you to demonstrate a methodical and thoughtful analytical process. This involves clearly defining a question or objective, sourcing and meticulously preparing relevant data, exploring that data to uncover insights, constructing and evaluating appropriate models, and, critically, communicating your entire process, findings, limitations, and interpretations clearly. Consider this a simulation of a real-world data science task where clear communication and a documented process are paramount.</p>"},{"location":"syllabus/project-guide/#ai-for-coursework","title":"AI for Coursework","text":"<p>Generative AI (GenAI) tools (like ChatGPT, Google Gemini, Microsoft Copilot, Claude) can be valuable learning aids, but their misuse undermines learning and constitutes academic dishonesty.</p> <p>Guiding Principle: Employ GenAI as a tutor, brainstorming partner, or debugging assistant. It should augment your learning, not replace your critical thinking, analytical work, or original writing. The project you submit must fundamentally represent your own effort and understanding.</p>"},{"location":"syllabus/project-guide/#acceptable-uses","title":"Acceptable Uses","text":"<ul> <li>Brainstorming: Generating initial ideas for project topics or analytical questions. (e.g., \"Suggest data science project ideas relevant to healthcare administration.\")</li> <li>Conceptual Clarification: Asking for explanations of terms, algorithms, or methodologies. (e.g., \"Explain the concept of 'overfitting' in machine learning.\")</li> <li>Code Explanation &amp; Debugging: Getting explanations for specific functions or error messages using small snippets of your code. (e.g., \"What does this Polars error mean: <code>[error message]</code>?\")</li> <li>Syntax Examples: Requesting basic syntax for specific library functions (which you must then adapt, implement, and understand). (e.g., \"Show a basic example of reading a parquet file in Polars.\")</li> <li>Exploring Alternatives: Asking about different methods or approaches to consider for a task. (e.g., \"What are common alternatives to k-means clustering?\")</li> <li>Writing Refinement: Improving the grammar and clarity of your own written explanations in markdown cells.</li> </ul>"},{"location":"syllabus/project-guide/#unacceptable-uses","title":"Unacceptable Uses","text":"<ul> <li>Generating Core Analysis Code: Asking the AI to write substantial portions of your data cleaning, EDA, or modeling code.</li> <li>Generating Interpretations/Conclusions: Requesting the AI to interpret your charts, model results, or write concluding paragraphs.</li> <li>Direct Plagiarism: Submitting AI-generated code or text as your own without significant modification, understanding, and integration into your work.</li> <li>Bypassing Understanding: Relying on AI for answers without trying to grasp the underlying concepts yourself.</li> </ul>"},{"location":"syllabus/project-guide/#your-responsibility","title":"Your Responsibility","text":"<ul> <li>Verify AI Output: GenAI can be inaccurate. Always cross-reference its suggestions with course materials, documentation, or further investigation. Code provided by AI should be carefully tested and understood.</li> <li>Understand Your Work: Be prepared to explain the logic behind any code or analysis step you submit.</li> <li>Own Your Insights: All interpretations, conclusions, and discussions of limitations must stem from your own critical thinking.</li> <li>Ask if Unsure: Please consult the instructor if you have questions about appropriate AI use.</li> </ul> <p>Self-Check: Could you confidently explain your project's methodology and findings without the AI tool? If so, you are likely using it appropriately.</p>"},{"location":"syllabus/project-guide/#component-1-project-plan","title":"Component 1: Project Plan","text":"<p>Goal: Define the project's scope, data, objectives, and initial roadmap.</p> <p>Step 1: Identify a Topic and Research Question/Objective. Start by choosing a domain that interests you, perhaps related to your career aspirations or other MBA coursework. Formulate an initial question you'd like to explore with data. Examples include: \"Can we predict employee attrition based on HR data?\" or \"What customer characteristics are associated with higher lifetime value?\"</p> <p>GenAI Tip: Use AI for brainstorming topics or refining your initial question for better focus.</p> <p>Step 2: Find Your Data. Search for relevant public datasets (Kaggle, data.gov, UCI ML Repository, etc.). Assess if the data is suitable (has relevant variables), accessible (downloadable), and credible. It's crucial to provide a verifiable link and note the source clearly in your plan.</p> <p>GenAI Tip: Ask AI about types of data used for certain analyses or for search keywords. Warning: Always verify dataset existence and links yourself, as AI suggestions can be unreliable here.</p> <p>Step 3: Define Analytical Objectives &amp; High-Level Technique. Refine your research question into 1-3 specific analytical objectives. Based on these objectives, identify the primary type of analysis you anticipate needing, such as prediction, classification, clustering, or descriptive analysis/EDA.</p> <p>GenAI Tip: Ask AI to help phrase objectives clearly or explain the differences between high-level techniques. Verify suggestions against course content.</p> <p>Step 4: Create Data Dictionary. After briefly inspecting your data or its documentation, create a Markdown table. List the key variables you intend to use, their data types (e.g., integer, float, string, boolean, date), and a concise description of what each represents.</p> <p>GenAI Tip: Request Markdown table templates or explanations of data types. Creating the descriptions accurately usually requires manual review of the data/documentation.</p> <p>Step 5: Outline Execution Plan. List the major phases for your project, roughly aligning with Components 2 and 3, and potentially including sub-steps like 'Data Cleaning' or 'Feature Engineering'. Propose a realistic timeline or milestones for completing these phases.</p> <p>GenAI Tip: Ask for typical phases in a data science project to ensure your plan is comprehensive.</p> <p>Deliverable: A Colab notebook containing these five elements, clearly documented using Markdown.</p>"},{"location":"syllabus/project-guide/#component-2-data-wrangling-eda","title":"Component 2: Data Wrangling &amp; EDA","text":"<p>Goal: Prepare your data for analysis, explore it visually, and derive initial insights.</p> <p>Step 1: Ingest &amp; Initial Inspection. Load your dataset into Colab using Polars commands appropriate for your file type (e.g., <code>pl.read_csv</code>). Perform initial checks like examining the <code>shape</code>, <code>head()</code>, <code>dtypes</code>, and <code>describe()</code> outputs. Document your first impressions.</p> <p>GenAI Tip: Get syntax examples for loading data or performing checks. Debug loading errors. Ask about interpreting initial summary statistics.</p> <p>Step 2: Data Cleaning &amp; Transformation (Polars). This is often the most time-consuming phase. Address missing values using an appropriate strategy (e.g., imputation, deletion) and justify your choice. Correct data types using Polars expressions (e.g., <code>.cast()</code>). Filter rows or select columns as needed (<code>.filter()</code>, <code>.select()</code>). If necessary, create new features (<code>.with_columns()</code>) or reshape data (<code>.pivot()</code>). Remember to clearly document the rationale for each significant transformation in Markdown.</p> <p>GenAI Tip: Explore strategies for handling missing data via AI prompts. Get Polars syntax examples for specific tasks (e.g., type conversion, conditional feature creation). Debug Polars code snippets. Always verify the logic.</p> <p>Step 3: Exploratory Data Analysis (Altair). Use Altair to create visualizations that help you understand your data and address your objectives. Explore individual variables (univariate analysis, e.g., histograms) and relationships between variables (bivariate analysis, e.g., scatter plots, box plots). Ensure your charts are clear and well-labeled.</p> <p>GenAI Tip: Ask about appropriate chart types for different kinds of data or relationships. Get basic Altair syntax examples. Debug plotting code.</p> <p>Step 4: Document Insights &amp; Storytelling. For each key visualization, add a Markdown cell explaining what it reveals. What patterns or insights emerge? How do they relate to your project objectives? Structure your EDA section to tell a coherent story about your data exploration journey.</p> <p>GenAI Tip: Use AI to help structure your interpretation notes or refine the clarity of your writing. The insights must be your own derivation from the analysis.</p> <p>Deliverable: An updated Colab notebook showing data ingestion, the cleaning/transformation process (with justifications), and EDA using Altair charts accompanied by clear interpretations.</p>"},{"location":"syllabus/project-guide/#component-3-modeling","title":"Component 3: Modeling","text":"<p>Goal: Apply machine learning models using scikit-learn to address your objectives, evaluate their performance, and interpret the results.</p> <p>Step 1: Feature Preparation for Modeling. Select the features you'll use based on your EDA and objectives. Prepare them for modeling by encoding categorical features (e.g., One-Hot Encoding) and scaling numerical features if necessary (e.g., StandardScaler). Split your data into training and testing sets (<code>train_test_split</code>). Using a scikit-learn <code>Pipeline</code> to combine preprocessing and model estimation is highly recommended for efficiency and rigor.</p> <p>GenAI Tip: Ask about encoding/scaling strategies or the rationale for train/test splits. Get example <code>Pipeline</code> structures. Debug preprocessing code.</p> <p>Step 2: Model Selection and Training. Choose appropriate scikit-learn models based on your task (regression, classification, clustering) and justify your selection. Instantiate your chosen model(s) or pipeline(s). Train the model(s) using the <code>.fit()</code> method exclusively on your training data.</p> <p>GenAI Tip: Ask for explanations of algorithms suitable for your task. Get basic syntax examples for fitting models. Rely on course knowledge and your analysis, not just AI suggestions, for model selection.</p> <p>Step 3: Model Evaluation. Use the trained model to make predictions on the testing data. Select appropriate evaluation metrics for your task (e.g., accuracy, precision, recall, F1, confusion matrix for classification; R2, MAE, RMSE for regression; silhouette score for clustering) and justify your choices. Calculate these metrics using <code>sklearn.metrics</code>. Analyze the results, checking for signs of overfitting (much better performance on training vs. test data) and comparing models if multiple were trained.</p> <p>GenAI Tip: Ask about appropriate metrics for your goal. Get syntax examples for metric calculation. Ask how to interpret evaluation outputs like confusion matrices.</p> <p>Step 4: Interpretation, Conclusion &amp; Limitations. Interpret what your model's performance means in the context of your project goals. Draw clear conclusions based on the evidence. Did you answer your original question? Critically discuss the limitations of your analysis \u2013 consider data limitations, model assumptions, potential biases, and practical implications. Suggest potential next steps or areas for improvement.</p> <p>GenAI Tip: Ask for help structuring your conclusion or brainstorming potential limitations common to your chosen model type. Use AI to refine the clarity of your writing. The core interpretation and critical assessment must be your own.</p> <p>Deliverable: The final Colab notebook incorporating feature preparation, model training/evaluation (sklearn), and detailed interpretation, conclusions, and discussion of limitations, all clearly documented.</p>"},{"location":"syllabus/project-guide/#optional-extra-credit-component","title":"Optional Extra Credit Component","text":"<p>Goal: Explore time-series or text analysis techniques.</p> <p>If you chose a suitable dataset, you can pursue this option. This involves specific preprocessing relevant to time-series (e.g., time indexing, stationarity checks) or text (e.g., tokenization, vectorization). You'll then apply appropriate analysis techniques using relevant libraries (e.g., <code>statsmodels</code>, <code>prophet</code> for time-series; <code>nltk</code>, <code>spaCy</code>, <code>sklearn.feature_extraction</code> for text). Finally, interpret and integrate these findings with your main project. Clearly mark this section in your notebook and document your process.</p> <p>GenAI Tip: Use AI similarly to other steps \u2013 for conceptual explanations, basic syntax examples specific to these advanced libraries, debugging, and writing refinement.</p> <p>Deliverable: A clearly marked section within your final submitted Colab notebook.</p> <p>7. General Tips for Success</p> <ul> <li>Start Early: Allocate sufficient time for each project phase.</li> <li>Iterate: Data analysis is often cyclical. Be prepared to revisit earlier steps.</li> <li>Document Thoroughly: Use Markdown cells extensively to explain your choices and interpretations. Your notebook should be a self-contained report.</li> <li>Justify Choices: The reasoning behind your decisions is a key part of the evaluation.</li> <li>Seek Help When Stuck: Leverage office hours, TA sessions, and course forums.</li> <li>Scope Realistically: A well-executed analysis on a focused problem is better than a superficial one on an overly broad topic.</li> </ul> <p>8. Submission</p> <p>Ensure your Colab notebook (<code>.ipynb</code> file) is well-organized, all code cells have been run with visible outputs, and submitted by the specified deadlines for each component. The final submission should clearly delineate all project components.</p>"},{"location":"syllabus/schedule/","title":"Schedule","text":"Module Date, Topic, &amp;  Project Deliverables Getting Started 05/15: Course Overview &amp; Introduction to Google Colab 05/22: Introduction to Python Exploratory Data Analysis 05/29: Data Wrangling with Polars Project Plan 06/05: Data Visualization with Altair 06/12: Data Storytelling Introduction to Modeling 06/19: Summer-1 Finals Week Juneteenth - No Classes  06/26: Introduction to Modeling Concepts &amp; Workflow Data Wrangling &amp; EDA  07/03: Robust Modeling Workflow 07/10: Modeling Without a Target Advanced Data Types 07/17: Time Series Data Analysis Modeling 07/24: Text Analysis 07/31: Finals week - No Classes Optional Extra Credit"}]}